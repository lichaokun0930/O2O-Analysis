# -*- coding: utf-8 -*-
"""
ä»Šæ—¥å¿…åš - å•†å“ä¾§åˆ†ææ¨¡å— (V2.3 è¿è¥è§†è§’é‡æ„)

ä¸¥æ ¼æŒ‰ç…§ã€Œå•†å“ä¾§è¿è¥é‡æ„æ–¹æ¡ˆã€å®ç°:

æ ¸å¿ƒç†å¿µ: ä»"æ•°æ®åˆ†æå¸ˆè§†è§’"è½¬å˜ä¸º"é—¨åº—è¿è¥è§†è§’"
å››å¤§åœºæ™¯:
1. ğŸ† é«˜åˆ©æ¶¦å•†å“æ¦œ (Top Profit): è°åœ¨èµšé’±?
2. ğŸ“‰ æµé‡ä¸‹è·Œ/å¼‚å¸¸æ¦œ (Traffic Drop): è°çªç„¶å–ä¸åŠ¨äº†?
3. ğŸŒ æ–°å¢æ»é”€é¢„è­¦ (New Slow-Moving): è°åˆšå¼€å§‹ç§¯å‹?
4. ğŸš€ æ½œåŠ›æ–°å“æ¦œ (New Potential): è°æ˜¯æ˜æ—¥ä¹‹æ˜Ÿ?

âš ï¸ æ—¶é—´åŸºå‡†: æ•°æ®æœ€åä¸€å¤© = "æ˜¨æ—¥"
"""

import pandas as pd
import numpy as np
from datetime import timedelta
from typing import Dict, Tuple, Optional, Any, List


def get_base_dates(df: pd.DataFrame) -> Tuple[Optional[pd.Timestamp], Optional[pd.Timestamp]]:
    """
    è·å–åŸºå‡†æ—¥æœŸ
    
    Returns:
        (æ˜¨æ—¥, å‰æ—¥) - æ˜¨æ—¥æ˜¯æ•°æ®æœ€åä¸€å¤©
    """
    date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in df.columns else 'ä¸‹å•æ—¶é—´'
    if date_col not in df.columns:
        return None, None
    
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col])
    
    # æ˜¨æ—¥ = æ•°æ®æœ€åä¸€å¤©
    yesterday = df[date_col].max().normalize()
    # å‰æ—¥ = æ˜¨æ—¥ - 1å¤©
    day_before = yesterday - timedelta(days=1)
    
    return yesterday, day_before


def get_product_daily_metrics(
    df: pd.DataFrame, 
    target_date: pd.Timestamp
) -> pd.DataFrame:
    """
    è·å–æŒ‡å®šæ—¥æœŸçš„å•†å“çº§æŒ‡æ ‡æ±‡æ€»
    
    Args:
        df: åŸå§‹æ•°æ®ï¼ˆå•†å“çº§æ˜ç»†ï¼‰
        target_date: ç›®æ ‡æ—¥æœŸ
    
    Returns:
        DataFrame: åº—å†…ç  | å•†å“åç§° | é”€é‡ | é”€å”®é¢ | åˆ©æ¶¦é¢ | æ¯›åˆ©ç‡
    
    Note:
        âš ï¸ ä½¿ç”¨åº—å†…ç ï¼ˆè€Œéå•†å“åç§°ï¼‰åŒºåˆ†å•†å“ï¼Œé¿å…åŒåä¸åŒè§„æ ¼æ··æ·†
    """
    date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in df.columns else 'ä¸‹å•æ—¶é—´'
    sales_col = 'æœˆå”®' if 'æœˆå”®' in df.columns else 'é”€é‡'
    
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col])
    
    # ç­›é€‰æŒ‡å®šæ—¥æœŸ
    day_data = df[df[date_col].dt.normalize() == target_date]
    
    if len(day_data) == 0:
        return pd.DataFrame()
    
    # âš ï¸ ä¼˜å…ˆä½¿ç”¨åº—å†…ç èšåˆï¼Œé¿å…åŒåä¸åŒè§„æ ¼å•†å“æ··æ·†
    group_key = 'åº—å†…ç ' if 'åº—å†…ç ' in day_data.columns else 'å•†å“åç§°'
    
    # æŒ‰å•†å“èšåˆ
    agg_dict = {}
    
    # ä¿ç•™å•†å“åç§°
    if group_key == 'åº—å†…ç ':
        agg_dict['å•†å“åç§°'] = ('å•†å“åç§°', 'first')
    
    # é”€é‡
    if sales_col in day_data.columns:
        agg_dict['é”€é‡'] = (sales_col, 'sum')
    
    # é”€å”®é¢ = å®æ”¶ä»·æ ¼ Ã— é”€é‡
    if 'å®æ”¶ä»·æ ¼' in day_data.columns and sales_col in day_data.columns:
        day_data['_å®æ”¶ä»·æ ¼_é”€é‡'] = day_data['å®æ”¶ä»·æ ¼'].fillna(0) * day_data[sales_col].fillna(1)
        agg_dict['é”€å”®é¢'] = ('_å®æ”¶ä»·æ ¼_é”€é‡', 'sum')
    elif 'å•†å“å®å”®ä»·' in day_data.columns:
        # å¤‡é€‰ï¼šå•†å“å®å”®ä»·å·²ç»æ˜¯æ€»ä»·
        agg_dict['é”€å”®é¢'] = ('å•†å“å®å”®ä»·', 'sum')
    
    # åˆ©æ¶¦é¢
    if 'åˆ©æ¶¦é¢' in day_data.columns:
        agg_dict['åˆ©æ¶¦é¢'] = ('åˆ©æ¶¦é¢', 'sum')
    
    # æˆæœ¬
    cost_col = 'å•†å“é‡‡è´­æˆæœ¬' if 'å•†å“é‡‡è´­æˆæœ¬' in day_data.columns else 'æˆæœ¬'
    if cost_col in day_data.columns:
        agg_dict['æˆæœ¬'] = (cost_col, 'sum')
    
    if not agg_dict:
        return pd.DataFrame()
    
    # æ‰§è¡Œèšåˆ
    result = day_data.groupby(group_key).agg(**agg_dict).reset_index()
    
    # è®¡ç®—æ¯›åˆ©ç‡
    # ä¿®æ­£é€»è¾‘: é”€å”®é¢ä¸º0æ—¶ï¼Œæ¯›åˆ©ç‡åº”è§†ä¸ºæ— æ•ˆ(NaN)ï¼Œä¸å‚ä¸"æ¯›åˆ©ç‡ä¸‹æ»‘"è®¡ç®—
    # é¿å…å‡ºç° 0(æ— é”€é‡) vs 40%(æœ‰é”€é‡) è¢«è¯¯åˆ¤ä¸º"æ¯›åˆ©ç‡æš´è·Œ"
    if 'é”€å”®é¢' in result.columns:
        # ä½¿ç”¨ä¸´æ—¶å˜é‡é¿å…é™¤ä»¥0
        sales_safe = result['é”€å”®é¢'].replace(0, np.nan)
        
        if 'æˆæœ¬' in result.columns:
            result['æ¯›åˆ©ç‡'] = ((result['é”€å”®é¢'] - result['æˆæœ¬']) / sales_safe * 100).round(2)
        elif 'åˆ©æ¶¦é¢' in result.columns:
            result['æ¯›åˆ©ç‡'] = (result['åˆ©æ¶¦é¢'] / sales_safe * 100).round(2)
        else:
            result['æ¯›åˆ©ç‡'] = np.nan
            
        # æ¸…ç† inf/-inf
        result['æ¯›åˆ©ç‡'] = result['æ¯›åˆ©ç‡'].replace([np.inf, -np.inf], np.nan)
    else:
        result['æ¯›åˆ©ç‡'] = np.nan
    
    return result


def analyze_top_profit_products(df: pd.DataFrame, top_n: int = 20) -> Dict[str, Any]:
    """
    åœºæ™¯A: ğŸ† é«˜åˆ©æ¶¦å•†å“æ¦œ (Top Profit)
    å®šä¹‰: æ˜¨æ—¥ç»™é—¨åº—èµšé’±æœ€å¤šçš„å•†å“ï¼ˆç°é‡‘ç‰›ï¼‰ã€‚
    """
    result = {'summary': {}, 'data': pd.DataFrame(), 'error': None}
    
    try:
        yesterday, day_before = get_base_dates(df)
        if yesterday is None:
            result['error'] = 'æ— æ³•è·å–æ—¥æœŸä¿¡æ¯'
            return result
            
        # è·å–æ˜¨æ—¥æ•°æ®
        metrics = get_product_daily_metrics(df, yesterday)
        if metrics.empty:
            result['error'] = 'æ˜¨æ—¥æ— é”€å”®æ•°æ®'
            return result
            
        # ç­›é€‰åˆ©æ¶¦>0å¹¶æ’åº
        if 'åˆ©æ¶¦é¢' not in metrics.columns:
            result['error'] = 'ç¼ºå°‘åˆ©æ¶¦é¢å­—æ®µ'
            return result
            
        top_profit = metrics[metrics['åˆ©æ¶¦é¢'] > 0].sort_values('åˆ©æ¶¦é¢', ascending=False).head(top_n).copy()
        
        # è®¡ç®—å•å‡æŒ‡æ ‡
        top_profit['å•å‡åˆ©æ¶¦é¢'] = (top_profit['åˆ©æ¶¦é¢'] / top_profit['é”€é‡']).round(2)
        
        # æ£€æŸ¥å‰æ—¥é”€é‡ä»¥æ ‡è®°"æ˜¨æ—¥é¦–é”€"
        day_before_metrics = get_product_daily_metrics(df, day_before)
        if not day_before_metrics.empty:
            day_before_sales = day_before_metrics[['å•†å“åç§°', 'é”€é‡']].rename(columns={'é”€é‡': 'å‰æ—¥é”€é‡'})
            top_profit = top_profit.merge(day_before_sales, on='å•†å“åç§°', how='left')
            top_profit['å‰æ—¥é”€é‡'] = top_profit['å‰æ—¥é”€é‡'].fillna(0)
        else:
            top_profit['å‰æ—¥é”€é‡'] = 0
            
        result['data'] = top_profit
        result['summary'] = {
            'total_profit': top_profit['åˆ©æ¶¦é¢'].sum(),
            'count': len(top_profit)
        }
        return result
        
    except Exception as e:
        result['error'] = f'åˆ†æé«˜åˆ©æ¶¦å•†å“æ—¶å‡ºé”™: {str(e)}'
        return result


def analyze_traffic_drop_products(df: pd.DataFrame, top_n: int = 20) -> Dict[str, Any]:
    """
    åœºæ™¯B: ğŸ“‰ æµé‡ä¸‹è·Œ/å¼‚å¸¸æ¦œ (Traffic Drop)
    å®šä¹‰: ä»¥å‰å–å¾—å¥½ï¼Œæ˜¨å¤©çªç„¶å–ä¸åŠ¨äº†ã€‚
    ç­›é€‰: å‰æ—¥é”€é‡ >= 3 ä¸” æ˜¨æ—¥é”€é‡ç¯æ¯”ä¸‹è·Œ > 50%ï¼ˆæˆ–è€…ç›´æ¥ä¸º0ï¼‰ã€‚
    """
    result = {'summary': {}, 'data': pd.DataFrame(), 'error': None}
    
    try:
        yesterday, day_before = get_base_dates(df)
        if yesterday is None:
            result['error'] = 'æ— æ³•è·å–æ—¥æœŸä¿¡æ¯'
            return result
            
        date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in df.columns else 'ä¸‹å•æ—¶é—´'
        
        # è·å–ä¸¤æ—¥æ•°æ®
        yesterday_metrics = get_product_daily_metrics(df, yesterday)
        day_before_metrics = get_product_daily_metrics(df, day_before)
        
        if day_before_metrics.empty:
            result['error'] = 'å‰æ—¥æ— é”€å”®æ•°æ®ï¼Œæ— æ³•è®¡ç®—ä¸‹è·Œ'
            return result
            
        # åˆå¹¶æ•°æ®
        comparison = day_before_metrics[['å•†å“åç§°', 'é”€é‡', 'åˆ©æ¶¦é¢']].merge(
            yesterday_metrics[['å•†å“åç§°', 'é”€é‡']], 
            on='å•†å“åç§°', 
            how='left',
            suffixes=('_å‰æ—¥', '_æ˜¨æ—¥')
        )
        comparison['é”€é‡_æ˜¨æ—¥'] = comparison['é”€é‡_æ˜¨æ—¥'].fillna(0)
        
        # è®¡ç®—å•å‡åˆ©æ¶¦ï¼ˆç”¨å‰æ—¥æ•°æ®ä¼°ç®—ï¼‰
        comparison['å•å‡åˆ©æ¶¦'] = (comparison['åˆ©æ¶¦é¢'] / comparison['é”€é‡_å‰æ—¥']).replace([np.inf, -np.inf], 0).fillna(0)
        
        # ç­›é€‰æ¡ä»¶: å‰æ—¥é”€é‡>=3 ä¸” (æ˜¨æ—¥é”€é‡=0 æˆ– è·Œå¹…>50%)
        # è·Œå¹… = (å‰æ—¥-æ˜¨æ—¥)/å‰æ—¥
        comparison['è·Œå¹…'] = (comparison['é”€é‡_å‰æ—¥'] - comparison['é”€é‡_æ˜¨æ—¥']) / comparison['é”€é‡_å‰æ—¥']
        
        mask = (comparison['é”€é‡_å‰æ—¥'] >= 3) & (comparison['è·Œå¹…'] > 0.5)
        drops = comparison[mask].copy()
        
        # è·å–æ˜¨æ—¥åº“å­˜ä¿¡æ¯ (ç”¨äºåˆ¤æ–­æ˜¯å¦å”®ç½„)
        stock_col = next((c for c in ['å‰©ä½™åº“å­˜', 'åº“å­˜', 'stock'] if c in df.columns), None)
        if stock_col:
            # åˆ›å»ºå‰¯æœ¬å¹¶è½¬æ¢æ—¥æœŸï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            df_stock = df[[date_col, 'å•†å“åç§°', stock_col]].copy()
            df_stock[date_col] = pd.to_datetime(df_stock[date_col])
            
            # è·å–æ˜¨æ—¥æœ€åä¸€æ¡è®°å½•çš„åº“å­˜
            yesterday_data = df_stock[df_stock[date_col].dt.normalize() == yesterday]
            # æŒ‰æ—¶é—´æ’åºå–æœ€åä¸€æ¡
            latest_stock = yesterday_data.sort_values(date_col).groupby('å•†å“åç§°')[stock_col].last().reset_index()
            latest_stock.rename(columns={stock_col: 'æ˜¨æ—¥åº“å­˜'}, inplace=True)
            
            drops = drops.merge(latest_stock, on='å•†å“åç§°', how='left')
            drops['æ˜¨æ—¥åº“å­˜'] = drops['æ˜¨æ—¥åº“å­˜'].fillna(0)
        else:
            drops['æ˜¨æ—¥åº“å­˜'] = 0
            
        # æ ‡è®°åŸå› : å”®ç½„ vs æµé‡ä¸‹æ»‘
        # å”®ç½„é€»è¾‘: æ˜¨æ—¥åº“å­˜ <= 0
        drops['åŸå› '] = drops.apply(
            lambda x: 'ğŸš« å”®ç½„ç¼ºè´§' if x['æ˜¨æ—¥åº“å­˜'] <= 0 else 'ğŸ“‰ æµé‡ä¸‹æ»‘', 
            axis=1
        )
        
        # è®¡ç®—é¢„ä¼°æŸå¤±
        drops['æµå¤±åˆ©æ¶¦ä¼°ç®—'] = ((drops['é”€é‡_å‰æ—¥'] - drops['é”€é‡_æ˜¨æ—¥']) * drops['å•å‡åˆ©æ¶¦']).round(2)
        
        # æ’åº: æŒ‰æµå¤±åˆ©æ¶¦å€’åº
        drops = drops.sort_values('æµå¤±åˆ©æ¶¦ä¼°ç®—', ascending=False).head(top_n)
        
        result['data'] = drops
        result['summary'] = {
            'count': len(drops),
            'total_loss': drops['æµå¤±åˆ©æ¶¦ä¼°ç®—'].sum(),
            'stockout_count': (drops['æ˜¨æ—¥åº“å­˜'] <= 0).sum()
        }
        return result
        
    except Exception as e:
        result['error'] = f'åˆ†ææµé‡ä¸‹è·Œå•†å“æ—¶å‡ºé”™: {str(e)}'
        return result


def analyze_new_slow_moving_products(df: pd.DataFrame) -> Dict[str, Any]:
    """
    åœºæ™¯C: ğŸŒ æ–°å¢æ»é”€é¢„è­¦ (New Slow-Moving)
    å®šä¹‰: åˆšåˆšæ‰å…¥æ»é”€å‘ä½çš„å•†å“ã€‚
    ç­›é€‰: æœ€åé”€å”®æ—¥æœŸ è·ä»Š = 7å¤© (è½»åº¦) æˆ– 30å¤© (é‡åº¦)ã€‚
    """
    result = {'summary': {}, 'data': pd.DataFrame(), 'error': None}
    
    try:
        date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in df.columns else 'ä¸‹å•æ—¶é—´'
        if date_col not in df.columns:
            result['error'] = 'ç¼ºå°‘æ—¥æœŸå­—æ®µ'
            return result
            
        df = df.copy()
        df[date_col] = pd.to_datetime(df[date_col])
        last_date = df[date_col].max().normalize()
        
        # è®¡ç®—æ¯ä¸ªå•†å“çš„æœ€åé”€å”®æ—¥æœŸ
        last_sales = df.groupby('å•†å“åç§°')[date_col].max().reset_index()
        last_sales['days_since'] = (last_date - last_sales[date_col].dt.normalize()).dt.days
        
        # ç­›é€‰åˆšå¥½æ»¡7å¤©æˆ–30å¤©çš„
        # è€ƒè™‘åˆ°æ•°æ®å¯èƒ½ä¸æ˜¯æ¯å¤©éƒ½æœ‰ï¼Œæ”¾å®½ä¸€ç‚¹èŒƒå›´: 7-8å¤©, 30-31å¤©
        new_light = last_sales[last_sales['days_since'].between(7, 8)].copy()
        new_severe = last_sales[last_sales['days_since'].between(30, 31)].copy()
        
        new_light['æ»é”€ç­‰çº§'] = 'ğŸŸ¡ è½»åº¦(7å¤©)'
        new_severe['æ»é”€ç­‰çº§'] = 'ğŸ”´ é‡åº¦(30å¤©)'
        
        combined = pd.concat([new_severe, new_light])
        
        if combined.empty:
            return result
            
        # è·å–åº“å­˜å’Œæˆæœ¬ä¿¡æ¯
        # ä¼˜å…ˆä½¿ç”¨æœ€åä¸€å¤©çš„åº“å­˜å¿«ç…§ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨æœ€åä¸€æ¬¡é”€å”®æ—¶çš„è®°å½•
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå°è¯•è·å–æœ€æ–°çš„åº“å­˜è®°å½•
        stock_col = next((c for c in ['å‰©ä½™åº“å­˜', 'åº“å­˜', 'stock'] if c in df.columns), None)
        cost_col = next((c for c in ['å•†å“é‡‡è´­æˆæœ¬', 'æˆæœ¬', 'cost'] if c in df.columns), None)
        
        if stock_col and cost_col:
            # è·å–æ¯ä¸ªå•†å“çš„æœ€æ–°åº“å­˜å’Œæˆæœ¬
            # æ³¨æ„: è¿™é‡Œçš„æˆæœ¬åº”è¯¥æ˜¯å•ä»·
            latest_info = df.sort_values(date_col).groupby('å•†å“åç§°')[[stock_col, cost_col]].last().reset_index()
            combined = combined.merge(latest_info, on='å•†å“åç§°', how='left')
            combined['ç§¯å‹æˆæœ¬'] = (combined[stock_col] * combined[cost_col]).fillna(0).round(2)
        else:
            combined['åº“å­˜'] = 0
            combined['ç§¯å‹æˆæœ¬'] = 0
            
        result['data'] = combined
        result['summary'] = {
            'count': len(combined),
            'total_cost': combined['ç§¯å‹æˆæœ¬'].sum()
        }
        return result
        
    except Exception as e:
        result['error'] = f'åˆ†ææ–°å¢æ»é”€å•†å“æ—¶å‡ºé”™: {str(e)}'
        return result


def analyze_potential_new_products(df: pd.DataFrame, top_n: int = 20) -> Dict[str, Any]:
    """
    åœºæ™¯D: ğŸš€ æ½œåŠ›æ–°å“æ¦œ (New Potential)
    å®šä¹‰: è¿‘æœŸé¦–æ¬¡åŠ¨é”€ä¸”è¡¨ç°ä¸é”™çš„å•†å“ã€‚
    ç­›é€‰: è¿‡å»7å¤©æ— é”€é‡(é™¤æ˜¨æ—¥å¤–) ä¸” æ˜¨æ—¥é”€é‡>0ã€‚
    """
    result = {'summary': {}, 'data': pd.DataFrame(), 'error': None}
    
    try:
        yesterday, day_before = get_base_dates(df)
        if yesterday is None:
            result['error'] = 'æ— æ³•è·å–æ—¥æœŸä¿¡æ¯'
            return result
            
        date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in df.columns else 'ä¸‹å•æ—¶é—´'
        df[date_col] = pd.to_datetime(df[date_col])
        
        # è·å–æ˜¨æ—¥æœ‰é”€é‡çš„å•†å“
        yesterday_sales = df[df[date_col].dt.normalize() == yesterday]['å•†å“åç§°'].unique()
        
        # æ£€æŸ¥è¿™äº›å•†å“åœ¨è¿‡å»7å¤©(ä¸å«æ˜¨æ—¥)æ˜¯å¦æœ‰é”€é‡
        start_check_date = yesterday - timedelta(days=7)
        past_sales = df[
            (df[date_col].dt.normalize() >= start_check_date) & 
            (df[date_col].dt.normalize() < yesterday)
        ]
        past_sold_products = set(past_sales['å•†å“åç§°'].unique())
        
        # ç­›é€‰å‡º"æ˜¨æ—¥æ–°å¢åŠ¨é”€" (æ˜¨æ—¥å–äº†ï¼Œä½†è¿‡å»7å¤©æ²¡å–)
        new_movers = [p for p in yesterday_sales if p not in past_sold_products]
        
        if not new_movers:
            return result
            
        # è·å–è¿™äº›å•†å“çš„æ˜¨æ—¥æŒ‡æ ‡
        metrics = get_product_daily_metrics(df, yesterday)
        potential = metrics[metrics['å•†å“åç§°'].isin(new_movers)].copy()
        
        # æ’åº: æŒ‰é”€å”®é¢å€’åº
        potential = potential.sort_values('é”€å”®é¢', ascending=False).head(top_n)
        
        result['data'] = potential
        result['summary'] = {
            'count': len(potential),
            'total_sales': potential['é”€å”®é¢'].sum()
        }
        return result
        
    except Exception as e:
        result['error'] = f'åˆ†ææ½œåŠ›æ–°å“æ—¶å‡ºé”™: {str(e)}'
        return result


# ä¿ç•™æ—§å‡½æ•°ä»¥å…¼å®¹å…¶ä»–æ¨¡å—è°ƒç”¨ï¼Œä½†æ ‡è®°ä¸ºDeprecated
def analyze_product_fluctuation(df: pd.DataFrame, top_n: int = 10) -> Dict[str, Any]:
    """
    [Deprecated] æ—§ç‰ˆæ³¢åŠ¨åˆ†æï¼Œä¿ç•™å…¼å®¹æ€§
    """
    # ...existing code...
    result = {
        'summary': {},
        'top_declining': pd.DataFrame(),
        'all_declining': pd.DataFrame(),
        'error': None
    }
    
    try:
        # è·å–åŸºå‡†æ—¥æœŸ
        yesterday, day_before = get_base_dates(df)
        if yesterday is None:
            result['error'] = 'æ— æ³•è·å–æ—¥æœŸä¿¡æ¯'
            return result
        
        # è·å–æ˜¨æ—¥å’Œå‰æ—¥çš„å•†å“æŒ‡æ ‡
        yesterday_metrics = get_product_daily_metrics(df, yesterday)
        day_before_metrics = get_product_daily_metrics(df, day_before)
        
        if len(yesterday_metrics) == 0:
            result['error'] = 'æ˜¨æ—¥æ— é”€å”®æ•°æ®'
            return result
        
        if len(day_before_metrics) == 0:
            result['error'] = 'å‰æ—¥æ— é”€å”®æ•°æ®ï¼Œæ— æ³•è®¡ç®—ç¯æ¯”'
            return result
        
        # åˆå¹¶å¯¹æ¯”
        comparison = yesterday_metrics.merge(
            day_before_metrics,
            on='å•†å“åç§°',
            how='outer',
            suffixes=('_æ˜¨æ—¥', '_å‰æ—¥')
        ).fillna(0)
        
        # è®¡ç®—ç¯æ¯”å˜åŒ–
        if 'åˆ©æ¶¦é¢_æ˜¨æ—¥' in comparison.columns and 'åˆ©æ¶¦é¢_å‰æ—¥' in comparison.columns:
            comparison['åˆ©æ¶¦é¢å˜åŒ–'] = comparison['åˆ©æ¶¦é¢_æ˜¨æ—¥'] - comparison['åˆ©æ¶¦é¢_å‰æ—¥']
            comparison['åˆ©æ¶¦é¢ç¯æ¯”'] = comparison.apply(
                lambda r: (r['åˆ©æ¶¦é¢å˜åŒ–'] / r['åˆ©æ¶¦é¢_å‰æ—¥'] * 100) 
                          if r['åˆ©æ¶¦é¢_å‰æ—¥'] != 0 else (0 if r['åˆ©æ¶¦é¢_æ˜¨æ—¥'] == 0 else -100),
                axis=1
            ).round(2)
        
        if 'æ¯›åˆ©ç‡_æ˜¨æ—¥' in comparison.columns and 'æ¯›åˆ©ç‡_å‰æ—¥' in comparison.columns:
            # ä¿®æ­£: å¦‚æœæ˜¨æ—¥é”€é‡ä¸º0 (æ¯›åˆ©ç‡ä¸ºNaN)ï¼Œåˆ™æ¯›åˆ©ç‡å˜åŒ–ä¹Ÿåº”ä¸ºNaNï¼Œä¸å‚ä¸"æ¯›åˆ©ç‡ä¸‹æ»‘"ç»Ÿè®¡
            comparison['æ¯›åˆ©ç‡å˜åŒ–'] = comparison['æ¯›åˆ©ç‡_æ˜¨æ—¥'] - comparison['æ¯›åˆ©ç‡_å‰æ—¥']
        
        if 'é”€é‡_æ˜¨æ—¥' in comparison.columns and 'é”€é‡_å‰æ—¥' in comparison.columns:
            comparison['é”€é‡å˜åŒ–'] = comparison['é”€é‡_æ˜¨æ—¥'] - comparison['é”€é‡_å‰æ—¥']
            comparison['é”€é‡ç¯æ¯”'] = comparison.apply(
                lambda r: (r['é”€é‡å˜åŒ–'] / r['é”€é‡_å‰æ—¥'] * 100)
                          if r['é”€é‡_å‰æ—¥'] != 0 else (0 if r['é”€é‡_æ˜¨æ—¥'] == 0 else -100),
                axis=1
            ).round(2)
            
        # æ·»åŠ ä¸‹æ»‘åŸå› åˆ†æ
        def analyze_decline_reason(row):
            reasons = []
            # 1. çªå‘åœå”®: å‰æ—¥æœ‰é”€é‡ï¼Œæ˜¨æ—¥æ— é”€é‡
            if row.get('é”€é‡_å‰æ—¥', 0) > 0 and row.get('é”€é‡_æ˜¨æ—¥', 0) == 0:
                return 'ğŸ›‘ çªå‘åœå”®'
            
            # 2. é”€é‡è·³æ°´: é”€é‡ä¸‹æ»‘è¶…è¿‡30%
            if row.get('é”€é‡ç¯æ¯”', 0) < -30:
                reasons.append('ğŸ“‰ é”€é‡è·³æ°´')
            elif row.get('é”€é‡å˜åŒ–', 0) < 0:
                reasons.append('ğŸ“‰ é”€é‡å¾®è·Œ')
                
            # 3. æ¯›åˆ©æ¶åŒ–: æ¯›åˆ©ç‡ä¸‹æ»‘è¶…è¿‡5ä¸ªç™¾åˆ†ç‚¹ (ä¸”æ˜¨æ—¥æœ‰é”€é‡)
            if pd.notna(row.get('æ¯›åˆ©ç‡å˜åŒ–')) and row.get('æ¯›åˆ©ç‡å˜åŒ–') < -5:
                reasons.append('ğŸ’¸ æ¯›åˆ©æ¶åŒ–')
            elif pd.notna(row.get('æ¯›åˆ©ç‡å˜åŒ–')) and row.get('æ¯›åˆ©ç‡å˜åŒ–') < 0:
                reasons.append('ğŸ’¸ æ¯›åˆ©å¾®è·Œ')
                
            if not reasons:
                return 'âš ï¸ åˆ©æ¶¦ä¸‹æ»‘' # å…œåº•: é”€é‡/æ¯›åˆ©æ²¡å¤§è·Œï¼Œä½†åˆ©æ¶¦è·Œäº†
                
            return ' + '.join(reasons)

        comparison['ä¸‹æ»‘åŸå› '] = comparison.apply(analyze_decline_reason, axis=1)
        
        # åˆ¤æ–­æ˜¯å¦ä¸‹æ»‘
        decline_conditions = []
        if 'åˆ©æ¶¦é¢å˜åŒ–' in comparison.columns:
            decline_conditions.append(comparison['åˆ©æ¶¦é¢å˜åŒ–'] < 0)
        # æ³¨æ„: æ¯›åˆ©ç‡å˜åŒ–ä¸ºNaNæ—¶ (å³æ˜¨æ—¥æ— é”€é‡)ï¼Œä¸åº”è¢«è§†ä¸º"æ¯›åˆ©ç‡ä¸‹æ»‘"
        if 'æ¯›åˆ©ç‡å˜åŒ–' in comparison.columns:
            decline_conditions.append(comparison['æ¯›åˆ©ç‡å˜åŒ–'] < 0)
        if 'é”€é‡å˜åŒ–' in comparison.columns:
            decline_conditions.append(comparison['é”€é‡å˜åŒ–'] < 0)
        
        if not decline_conditions:
            result['error'] = 'æ— æ³•è®¡ç®—å˜åŒ–æŒ‡æ ‡'
            return result
        
        is_declining = pd.concat(decline_conditions, axis=1).any(axis=1)
        declining_products = comparison[is_declining].copy()
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        summary = {
            'declining_count': len(declining_products),
            'yesterday': yesterday.strftime('%Y-%m-%d'),
            'day_before': day_before.strftime('%Y-%m-%d')
        }
        
        if 'åˆ©æ¶¦é¢å˜åŒ–' in declining_products.columns:
            profit_declining = declining_products[declining_products['åˆ©æ¶¦é¢å˜åŒ–'] < 0]
            summary['profit_declining_count'] = len(profit_declining)
            summary['profit_loss_total'] = round(abs(profit_declining['åˆ©æ¶¦é¢å˜åŒ–'].sum()), 2)
        
        if 'æ¯›åˆ©ç‡å˜åŒ–' in declining_products.columns:
            # ä¿®æ­£: æ’é™¤NaNå€¼
            margin_declining = declining_products[declining_products['æ¯›åˆ©ç‡å˜åŒ–'] < 0]
            summary['margin_declining_count'] = len(margin_declining)
            summary['margin_avg_drop'] = round(margin_declining['æ¯›åˆ©ç‡å˜åŒ–'].mean(), 2) if len(margin_declining) > 0 else 0
        
        if 'é”€é‡å˜åŒ–' in declining_products.columns:
            sales_declining = declining_products[declining_products['é”€é‡å˜åŒ–'] < 0]
            summary['sales_declining_count'] = len(sales_declining)
        
        if 'åˆ©æ¶¦é¢å˜åŒ–' in declining_products.columns:
            declining_products = declining_products.sort_values('åˆ©æ¶¦é¢å˜åŒ–', ascending=True)
        
        result['summary'] = summary
        result['all_declining'] = declining_products
        result['top_declining'] = declining_products.head(top_n)
        
        return result
        
    except Exception as e:
        result['error'] = f'åˆ†æå•†å“æ³¢åŠ¨æ—¶å‡ºé”™: {str(e)}'
        return result


def analyze_slow_moving_products(df: pd.DataFrame) -> Dict[str, Any]:
    """
    [Deprecated] æ—§ç‰ˆæ»é”€åˆ†æï¼Œä¿ç•™å…¼å®¹æ€§
    """
    # ...existing code...
    result = {
        'summary': {},
        'severe': pd.DataFrame(),
        'medium': pd.DataFrame(),
        'light': pd.DataFrame(),
        'error': None
    }
    
    try:
        date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in df.columns else 'ä¸‹å•æ—¶é—´'
        sales_col = 'æœˆå”®' if 'æœˆå”®' in df.columns else 'é”€é‡'
        
        if date_col not in df.columns:
            result['error'] = 'ç¼ºå°‘æ—¥æœŸå­—æ®µ'
            return result
        
        if 'å•†å“åç§°' not in df.columns:
            result['error'] = 'ç¼ºå°‘å•†å“åç§°å­—æ®µ'
            return result
        
        df = df.copy()
        df[date_col] = pd.to_datetime(df[date_col])
        
        max_date = df[date_col].max().normalize()
        min_date = df[date_col].min().normalize()
        data_range_days = (max_date - min_date).days + 1
        
        if sales_col in df.columns:
            sales_df = df[df[sales_col] > 0]
        else:
            sales_df = df
        
        category_col = 'ä¸€çº§åˆ†ç±»å' if 'ä¸€çº§åˆ†ç±»å' in df.columns else 'ä¸€çº§åˆ†ç±»'
        
        if len(sales_df) == 0:
            result['error'] = 'æ— é”€å”®è®°å½•'
            return result
        
        last_sale = sales_df.groupby('å•†å“åç§°').agg({date_col: 'max'}).reset_index()
        last_sale.columns = ['å•†å“åç§°', 'æœ€åé”€å”®æ—¥']
        
        if category_col in df.columns:
            product_category = df.groupby('å•†å“åç§°')[category_col].first().reset_index()
            last_sale = last_sale.merge(product_category, on='å•†å“åç§°', how='left')
        
        last_sale['æ— é”€é‡å¤©æ•°'] = (max_date - pd.to_datetime(last_sale['æœ€åé”€å”®æ—¥'])).dt.days
        
        severe = last_sale[last_sale['æ— é”€é‡å¤©æ•°'] >= 30].copy()
        medium = last_sale[(last_sale['æ— é”€é‡å¤©æ•°'] >= 15) & (last_sale['æ— é”€é‡å¤©æ•°'] < 30)].copy()
        light = last_sale[(last_sale['æ— é”€é‡å¤©æ•°'] >= 7) & (last_sale['æ— é”€é‡å¤©æ•°'] < 15)].copy()
        
        for df_temp in [severe, medium, light]:
            if len(df_temp) > 0:
                df_temp['æœ€åé”€å”®æ—¥'] = pd.to_datetime(df_temp['æœ€åé”€å”®æ—¥']).dt.strftime('%mæœˆ%dæ—¥')
        
        severe = severe.sort_values('æ— é”€é‡å¤©æ•°', ascending=False)
        medium = medium.sort_values('æ— é”€é‡å¤©æ•°', ascending=False)
        light = light.sort_values('æ— é”€é‡å¤©æ•°', ascending=False)
        
        result['summary'] = {
            'severe_count': len(severe),
            'medium_count': len(medium),
            'light_count': len(light),
            'data_range_days': data_range_days,
            'as_of_date': max_date.strftime('%Y-%m-%d')
        }
        result['severe'] = severe
        result['medium'] = medium
        result['light'] = light
        
        return result
        
    except Exception as e:
        result['error'] = f'åˆ†ææ»é”€å•†å“æ—¶å‡ºé”™: {str(e)}'
        return result


def get_product_insight(df: pd.DataFrame, product_name: str) -> Dict[str, Any]:
    """è·å–å•å“æ´å¯Ÿï¼ˆä¸‹é’»åˆ†æï¼‰"""
    result = {
        'product_name': product_name,
        'trend_data': pd.DataFrame(),
        'price_change': {},
        'cost_change': {},
        'activity_change': {},
        'insight': '',
        'error': None
    }
    
    try:
        date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in df.columns else 'ä¸‹å•æ—¶é—´'
        sales_col = 'æœˆå”®' if 'æœˆå”®' in df.columns else 'é”€é‡'
        
        product_df = df[df['å•†å“åç§°'] == product_name].copy()
        
        if len(product_df) == 0:
            result['error'] = f'æœªæ‰¾åˆ°å•†å“: {product_name}'
            return result
        
        product_df[date_col] = pd.to_datetime(product_df[date_col])
        yesterday, day_before = get_base_dates(df)
        
        # å†å²è¶‹åŠ¿
        agg_cols = {}
        if sales_col in product_df.columns:
            agg_cols[sales_col] = 'sum'
        if 'åˆ©æ¶¦é¢' in product_df.columns:
            agg_cols['åˆ©æ¶¦é¢'] = 'sum'
        if 'å•†å“å®å”®ä»·' in product_df.columns:
            agg_cols['å•†å“å®å”®ä»·'] = 'sum'
        
        if agg_cols:
            daily_agg = product_df.groupby(product_df[date_col].dt.date).agg(agg_cols).reset_index()
            # ç»Ÿä¸€æ—¥æœŸåˆ—åä¸º'æ—¥æœŸ'ï¼Œæ–¹ä¾¿å‰ç«¯ç»˜å›¾
            if date_col in daily_agg.columns and date_col != 'æ—¥æœŸ':
                daily_agg.rename(columns={date_col: 'æ—¥æœŸ'}, inplace=True)
            # é˜²æ­¢groupbyååˆ—åä¸¢å¤±
            if 'æ—¥æœŸ' not in daily_agg.columns:
                daily_agg.rename(columns={daily_agg.columns[0]: 'æ—¥æœŸ'}, inplace=True)
            
            # ç»Ÿä¸€é”€é‡åˆ—åä¸º'é”€é‡'
            if sales_col != 'é”€é‡' and sales_col in daily_agg.columns:
                daily_agg.rename(columns={sales_col: 'é”€é‡'}, inplace=True)
                
            result['trend_data'] = daily_agg
        
        # æ˜¨æ—¥vså‰æ—¥å¯¹æ¯”
        if yesterday and day_before:
            yesterday_data = product_df[product_df[date_col].dt.normalize() == yesterday]
            day_before_data = product_df[product_df[date_col].dt.normalize() == day_before]
            
            if 'å®æ”¶ä»·æ ¼' in product_df.columns and sales_col in product_df.columns:
                y_sales = yesterday_data[sales_col].sum()
                d_sales = day_before_data[sales_col].sum()
                y_avg_price = (yesterday_data['å®æ”¶ä»·æ ¼'] * yesterday_data[sales_col]).sum() / y_sales if y_sales > 0 else 0
                d_avg_price = (day_before_data['å®æ”¶ä»·æ ¼'] * day_before_data[sales_col]).sum() / d_sales if d_sales > 0 else 0
                result['price_change'] = {
                    'æ˜¨æ—¥å‡ä»·': round(y_avg_price, 2),
                    'å‰æ—¥å‡ä»·': round(d_avg_price, 2),
                    'å˜åŒ–ç‡': round((y_avg_price - d_avg_price) / d_avg_price * 100, 2) if d_avg_price > 0 else 0
                }
            
            if 'æ»¡å‡é‡‘é¢' in product_df.columns:
                y_activity = (yesterday_data['æ»¡å‡é‡‘é¢'] > 0).sum() / len(yesterday_data) * 100 if len(yesterday_data) > 0 else 0
                d_activity = (day_before_data['æ»¡å‡é‡‘é¢'] > 0).sum() / len(day_before_data) * 100 if len(day_before_data) > 0 else 0
                result['activity_change'] = {
                    'æ˜¨æ—¥æ»¡å‡å æ¯”': round(y_activity, 1),
                    'å‰æ—¥æ»¡å‡å æ¯”': round(d_activity, 1),
                    'å˜åŒ–': round(y_activity - d_activity, 1)
                }
        
        insights = []
        if result['price_change'] and result['price_change'].get('å˜åŒ–ç‡', 0) < -5:
            insights.append(f"å”®ä»·ä¸‹é™{abs(result['price_change']['å˜åŒ–ç‡'])}%")
        if result['activity_change'] and result['activity_change'].get('å˜åŒ–', 0) > 10:
            insights.append(f"æ»¡å‡æ´»åŠ¨å‚ä¸å¢åŠ {result['activity_change']['å˜åŒ–']}ä¸ªç™¾åˆ†ç‚¹")
        
        result['insight'] = 'åˆæ­¥åˆ¤æ–­: ' + 'ï¼Œ'.join(insights) if insights else 'æš‚æ— æ˜æ˜¾å¼‚å¸¸åŸå› '
        return result
        
    except Exception as e:
        result['error'] = f'è·å–å•†å“æ´å¯Ÿæ—¶å‡ºé”™: {str(e)}'
        return result


# ============== å…¼å®¹æ—§API ==============

def get_declining_products(df: pd.DataFrame, top_n: int = 10) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
    """å…¼å®¹æ—§APIçš„åŒ…è£…å‡½æ•°"""
    result = analyze_product_fluctuation(df, top_n)
    if result['error']:
        return None, result['error']
    return result['top_declining'], None


def identify_slow_moving_products(df: pd.DataFrame, as_of_date: Optional[pd.Timestamp] = None) -> Dict[str, pd.Series]:
    """å…¼å®¹æ—§APIçš„åŒ…è£…å‡½æ•°"""
    result = analyze_slow_moving_products(df)
    if result['error']:
        return {
            'light': pd.Series(dtype=float),
            'medium': pd.Series(dtype=float),
            'severe': pd.Series(dtype=float),
            'error': result['error']
        }
    
    def df_to_series(df_temp):
        if len(df_temp) == 0:
            return pd.Series(dtype=int)
        return df_temp.set_index('å•†å“åç§°')['æ— é”€é‡å¤©æ•°']
    
    return {
        'light': df_to_series(result['light']),
        'medium': df_to_series(result['medium']),
        'severe': df_to_series(result['severe']),
        'error': None
    }


# ============== å¢å¼ºç‰ˆå•å“æ´å¯Ÿ (V2.0) ==============

def get_product_insight_enhanced(df: pd.DataFrame, product_name: str) -> Dict[str, Any]:
    """
    å¢å¼ºç‰ˆå•å“æ´å¯Ÿ - å…¨é¢åˆ†æå•ä¸ªå•†å“
    
    è¿”å›ç»“æ„:
    {
        'product_name': str,           # å•†å“åç§°
        'summary': {                   # æ±‡æ€»æŒ‡æ ‡
            'total_sales': float,      # æ€»é”€å”®é¢
            'total_profit': float,     # æ€»åˆ©æ¶¦
            'total_quantity': int,     # æ€»é”€é‡ï¼ˆè®¢å•æ•°ï¼‰
            'avg_price': float,        # å¹³å‡å•ä»·
            'avg_margin': float,       # å¹³å‡æ¯›åˆ©ç‡(%)
            'avg_profit_per_order': float,  # å¹³å‡è®¢å•åˆ©æ¶¦
        },
        'daily_trend': pd.DataFrame,   # æŒ‰æ—¥è¶‹åŠ¿: æ—¥æœŸ, é”€é‡, é”€å”®é¢, åˆ©æ¶¦, å®æ”¶åˆ©æ¶¦ç‡, å®šä»·åˆ©æ¶¦ç‡
        'hourly_trend': pd.DataFrame,  # æŒ‰æ—¶æ®µè¶‹åŠ¿: å°æ—¶, é”€é‡, é”€å”®é¢, åˆ©æ¶¦, å®æ”¶åˆ©æ¶¦ç‡, å®šä»·åˆ©æ¶¦ç‡
        'partners': pd.DataFrame,      # æœ€ä½³æ‹æ¡£: å•†å“åç§°, é¢‘æ¬¡, ä¸€çº§åˆ†ç±»
        'role_daily': pd.DataFrame,    # è´­ä¹°è§’è‰²æŒ‰æ—¥åˆ†å¸ƒ: æ—¥æœŸ, è§’è‰², é”€é‡
        'price_sensitivity': {         # ä»·æ ¼æ•æ„Ÿåº¦åˆ†æ
            'correlation': float,      # ä»·æ ¼-é”€é‡ç›¸å…³ç³»æ•°
            'level': str,              # æ•æ„Ÿåº¦ç­‰çº§
            'color': str,              # å¯¹åº”é¢œè‰²
        },
        'recommendations': list,       # æ¨èè¡ŒåŠ¨åˆ—è¡¨
        'error': str or None
    }
    """
    result = {
        'product_name': product_name,
        'summary': {},
        'daily_trend': pd.DataFrame(),
        'hourly_trend': pd.DataFrame(),
        'partners': pd.DataFrame(),
        'role_daily': pd.DataFrame(),
        'price_sensitivity': {},
        'recommendations': [],
        'error': None
    }
    
    try:
        # ========== 1. åŸºç¡€æ•°æ®å‡†å¤‡ ==========
        date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in df.columns else 'ä¸‹å•æ—¶é—´'
        sales_col = 'æœˆå”®' if 'æœˆå”®' in df.columns else 'é”€é‡'
        
        # æ£€æµ‹ä¸€çº§åˆ†ç±»åˆ—
        category_col = None
        for col_name in ['ä¸€çº§åˆ†ç±»å', 'ç¾å›¢ä¸€çº§åˆ†ç±»', 'ä¸€çº§åˆ†ç±»']:
            if col_name in df.columns:
                category_col = col_name
                break
        
        product_df = df[df['å•†å“åç§°'] == product_name].copy()
        
        if len(product_df) == 0:
            result['error'] = f'æœªæ‰¾åˆ°å•†å“: {product_name}'
            return result
        
        # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
        product_df[date_col] = pd.to_datetime(product_df[date_col])
        product_df['_date'] = product_df[date_col].dt.date
        product_df['_hour'] = product_df[date_col].dt.hour
        
        # ç¡®ä¿æ•°å€¼åˆ—å­˜åœ¨ä¸”æ­£ç¡®
        numeric_cols = ['å®æ”¶ä»·æ ¼', 'åˆ©æ¶¦é¢', 'å•†å“é‡‡è´­æˆæœ¬', 'æ»¡å‡é‡‘é¢', 'å•†å“åŸä»·']
        for col in numeric_cols:
            if col in product_df.columns:
                product_df[col] = pd.to_numeric(product_df[col], errors='coerce').fillna(0)
        
        # é”€é‡å­—æ®µ
        sales_col = 'æœˆå”®' if 'æœˆå”®' in product_df.columns else 'é”€é‡'
        if sales_col in product_df.columns:
            product_df[sales_col] = pd.to_numeric(product_df[sales_col], errors='coerce').fillna(1)
        else:
            product_df[sales_col] = 1
        
        # å¦‚æœæ²¡æœ‰åˆ©æ¶¦é¢ï¼Œè®¡ç®—å®ƒ
        if 'åˆ©æ¶¦é¢' not in product_df.columns:
            if 'å•†å“é‡‡è´­æˆæœ¬' in product_df.columns:
                # æ³¨æ„ï¼šå•†å“é‡‡è´­æˆæœ¬å·²ç»æ˜¯æ€»æˆæœ¬(å•å“æˆæœ¬Ã—æœˆå”®)ï¼Œåˆ©æ¶¦é¢ä¹Ÿåº”è¯¥æ˜¯æ€»åˆ©æ¶¦
                product_df['åˆ©æ¶¦é¢'] = product_df['å®æ”¶ä»·æ ¼'] * product_df[sales_col] - product_df['å•†å“é‡‡è´­æˆæœ¬']
            else:
                product_df['åˆ©æ¶¦é¢'] = 0
        
        # âš ï¸ è®¡ç®—å®æ”¶é‡‘é¢ = å®æ”¶ä»·æ ¼(å•ä»·) Ã— æœˆå”®(é”€é‡)
        product_df['_å®æ”¶é‡‘é¢'] = product_df['å®æ”¶ä»·æ ¼'] * product_df[sales_col]
        
        # ========== 2. æ±‡æ€»æŒ‡æ ‡ ==========
        # é”€å”®é¢ = å®æ”¶ä»·æ ¼ Ã— æœˆå”® (å®æ”¶ä»·æ ¼æ˜¯å•ä»·ï¼Œéœ€ä¹˜ä»¥é”€é‡)
        total_sales = product_df['_å®æ”¶é‡‘é¢'].sum()
        total_profit = product_df['åˆ©æ¶¦é¢'].sum()
        total_quantity = product_df['è®¢å•ID'].nunique()
        avg_price = total_sales / total_quantity if total_quantity > 0 else 0
        avg_margin = (total_profit / total_sales * 100) if total_sales > 0 else 0
        avg_profit_per_order = total_profit / total_quantity if total_quantity > 0 else 0
        
        result['summary'] = {
            'total_sales': round(total_sales, 2),
            'total_profit': round(total_profit, 2),
            'total_quantity': total_quantity,
            'avg_price': round(avg_price, 2),
            'avg_margin': round(avg_margin, 1),
            'avg_profit_per_order': round(avg_profit_per_order, 2),
        }
        
        # ========== 3. æŒ‰æ—¥è¶‹åŠ¿ï¼ˆä»·æ ¼æ•æ„Ÿåº¦ç»´åº¦ï¼‰ ==========
        # ä½¿ç”¨_å®æ”¶é‡‘é¢(å·²ä¹˜ä»¥é”€é‡)ä½œä¸ºé”€å”®é¢
        daily_agg = product_df.groupby('_date').agg({
            'è®¢å•ID': 'nunique',
            '_å®æ”¶é‡‘é¢': 'sum',  # å®æ”¶ä»·æ ¼Ã—æœˆå”®
            'åˆ©æ¶¦é¢': 'sum',
        }).reset_index()
        daily_agg.columns = ['æ—¥æœŸ', 'é”€é‡', 'é”€å”®é¢', 'åˆ©æ¶¦é¢']
        
        # è®¡ç®—å¹³å‡å•ä»·
        daily_agg['å¹³å‡å•ä»·'] = np.where(
            daily_agg['é”€é‡'] > 0,
            (daily_agg['é”€å”®é¢'] / daily_agg['é”€é‡']).round(2),
            0
        )
        
        # è®¡ç®—å®æ”¶åˆ©æ¶¦ç‡ = åˆ©æ¶¦é¢ / é”€å”®é¢ * 100
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†… (-100% ~ 100%)ï¼Œé¿å…æç«¯å€¼
        raw_margin = np.where(
            daily_agg['é”€å”®é¢'] > 0,
            (daily_agg['åˆ©æ¶¦é¢'] / daily_agg['é”€å”®é¢']) * 100,
            0
        )
        daily_agg['å®æ”¶åˆ©æ¶¦ç‡'] = np.clip(raw_margin, -100, 100).round(1)
        
        # è®¡ç®—å®šä»·åˆ©æ¶¦ç‡ï¼ˆéœ€è¦å•†å“åŸä»·å’Œæˆæœ¬ï¼‰
        # å®šä»·åˆ©æ¶¦ç‡ = (å•†å“åŸä»· - å•†å“é‡‡è´­æˆæœ¬) / å•†å“åŸä»· Ã— 100
        # æ³¨æ„ï¼šå•†å“åŸä»·ä¸º0æˆ–æ— æ•ˆæ—¶ï¼Œä½¿ç”¨å®æ”¶åˆ©æ¶¦ç‡ä»£æ›¿
        if 'å•†å“åŸä»·' in product_df.columns and 'å•†å“é‡‡è´­æˆæœ¬' in product_df.columns:
            # åªç»Ÿè®¡æœ‰æ•ˆçš„å•†å“åŸä»·ï¼ˆ>0ï¼‰
            valid_pricing_data = product_df[product_df['å•†å“åŸä»·'] > 0].copy()
            if not valid_pricing_data.empty:
                daily_cost = valid_pricing_data.groupby('_date').agg({
                    'å•†å“åŸä»·': 'sum',
                    'å•†å“é‡‡è´­æˆæœ¬': 'sum'
                }).reset_index()
                daily_cost.columns = ['æ—¥æœŸ', '_åŸä»·æ€»é¢', '_æˆæœ¬æ€»é¢']
                daily_agg = daily_agg.merge(daily_cost, on='æ—¥æœŸ', how='left')
                daily_agg['_åŸä»·æ€»é¢'] = daily_agg['_åŸä»·æ€»é¢'].fillna(0)
                daily_agg['_æˆæœ¬æ€»é¢'] = daily_agg['_æˆæœ¬æ€»é¢'].fillna(0)
                
                # å½“åŸä»·æ€»é¢æœ‰æ•ˆæ—¶è®¡ç®—å®šä»·åˆ©æ¶¦ç‡ï¼Œå¦åˆ™ä½¿ç”¨å®æ”¶åˆ©æ¶¦ç‡
                raw_pricing_margin = np.where(
                    daily_agg['_åŸä»·æ€»é¢'] > 0,
                    ((daily_agg['_åŸä»·æ€»é¢'] - daily_agg['_æˆæœ¬æ€»é¢']) / daily_agg['_åŸä»·æ€»é¢']) * 100,
                    daily_agg['å®æ”¶åˆ©æ¶¦ç‡']  # æ— æœ‰æ•ˆåŸä»·æ—¶ç”¨å®æ”¶åˆ©æ¶¦ç‡ä»£æ›¿
                )
                daily_agg['å®šä»·åˆ©æ¶¦ç‡'] = np.clip(raw_pricing_margin, -100, 100).round(1)
                daily_agg = daily_agg.drop(columns=['_åŸä»·æ€»é¢', '_æˆæœ¬æ€»é¢'])
            else:
                # æ‰€æœ‰å•†å“åŸä»·éƒ½ä¸º0æˆ–æ— æ•ˆ
                daily_agg['å®šä»·åˆ©æ¶¦ç‡'] = daily_agg['å®æ”¶åˆ©æ¶¦ç‡']
        else:
            daily_agg['å®šä»·åˆ©æ¶¦ç‡'] = daily_agg['å®æ”¶åˆ©æ¶¦ç‡']  # æ— åŸä»·æ—¶ç­‰åŒå®æ”¶åˆ©æ¶¦ç‡
        
        daily_agg = daily_agg.sort_values('æ—¥æœŸ')
        daily_agg['æ—¥æœŸ'] = pd.to_datetime(daily_agg['æ—¥æœŸ'])
        result['daily_trend'] = daily_agg
        
        # ========== 4. æŒ‰æ—¶æ®µè¶‹åŠ¿ï¼ˆæ—¶æ®µç”»åƒç»´åº¦ï¼‰ ==========
        # ä½¿ç”¨_å®æ”¶é‡‘é¢(å·²ä¹˜ä»¥é”€é‡)ä½œä¸ºé”€å”®é¢
        hourly_agg = product_df.groupby('_hour').agg({
            'è®¢å•ID': 'nunique',
            '_å®æ”¶é‡‘é¢': 'sum',  # å®æ”¶ä»·æ ¼Ã—æœˆå”®
            'åˆ©æ¶¦é¢': 'sum',
        }).reset_index()
        hourly_agg.columns = ['å°æ—¶', 'é”€é‡', 'é”€å”®é¢', 'åˆ©æ¶¦é¢']
        
        # è¡¥å…¨24å°æ—¶
        full_hours = pd.DataFrame({'å°æ—¶': range(24)})
        hourly_agg = full_hours.merge(hourly_agg, on='å°æ—¶', how='left').fillna(0)
        
        # è®¡ç®—æ—¶æ®µæŒ‡æ ‡
        hourly_agg['å¹³å‡å•ä»·'] = np.where(
            hourly_agg['é”€é‡'] > 0,
            (hourly_agg['é”€å”®é¢'] / hourly_agg['é”€é‡']).round(2),
            0
        )
        # åˆ©æ¶¦ç‡é™åˆ¶åœ¨åˆç†èŒƒå›´å†… (-100% ~ 100%)
        raw_hourly_margin = np.where(
            hourly_agg['é”€å”®é¢'] > 0,
            (hourly_agg['åˆ©æ¶¦é¢'] / hourly_agg['é”€å”®é¢']) * 100,
            0
        )
        hourly_agg['å®æ”¶åˆ©æ¶¦ç‡'] = np.clip(raw_hourly_margin, -100, 100).round(1)
        
        # è®¡ç®—å®šä»·åˆ©æ¶¦ç‡ï¼ˆæŒ‰æ—¶æ®µï¼‰
        # å®šä»·åˆ©æ¶¦ç‡ = (å•†å“åŸä»· - å•†å“é‡‡è´­æˆæœ¬) / å•†å“åŸä»· Ã— 100
        # æ³¨æ„ï¼šå•†å“åŸä»·ä¸º0æˆ–æ— æ•ˆæ—¶ï¼Œä½¿ç”¨å®æ”¶åˆ©æ¶¦ç‡ä»£æ›¿
        if 'å•†å“åŸä»·' in product_df.columns and 'å•†å“é‡‡è´­æˆæœ¬' in product_df.columns:
            # åªç»Ÿè®¡æœ‰æ•ˆçš„å•†å“åŸä»·ï¼ˆ>0ï¼‰
            valid_pricing_data = product_df[product_df['å•†å“åŸä»·'] > 0].copy()
            if not valid_pricing_data.empty:
                hourly_cost = valid_pricing_data.groupby('_hour').agg({
                    'å•†å“åŸä»·': 'sum',
                    'å•†å“é‡‡è´­æˆæœ¬': 'sum'
                }).reset_index()
                hourly_cost.columns = ['å°æ—¶', '_åŸä»·æ€»é¢', '_æˆæœ¬æ€»é¢']
                hourly_agg = hourly_agg.merge(hourly_cost, on='å°æ—¶', how='left')
                hourly_agg['_åŸä»·æ€»é¢'] = hourly_agg['_åŸä»·æ€»é¢'].fillna(0)
                hourly_agg['_æˆæœ¬æ€»é¢'] = hourly_agg['_æˆæœ¬æ€»é¢'].fillna(0)
                
                # å½“åŸä»·æ€»é¢æœ‰æ•ˆæ—¶è®¡ç®—å®šä»·åˆ©æ¶¦ç‡ï¼Œå¦åˆ™ä½¿ç”¨å®æ”¶åˆ©æ¶¦ç‡
                raw_hourly_pricing = np.where(
                    hourly_agg['_åŸä»·æ€»é¢'] > 0,
                    ((hourly_agg['_åŸä»·æ€»é¢'] - hourly_agg['_æˆæœ¬æ€»é¢']) / hourly_agg['_åŸä»·æ€»é¢']) * 100,
                    hourly_agg['å®æ”¶åˆ©æ¶¦ç‡']  # æ— æœ‰æ•ˆåŸä»·æ—¶ç”¨å®æ”¶åˆ©æ¶¦ç‡ä»£æ›¿
                )
                hourly_agg['å®šä»·åˆ©æ¶¦ç‡'] = np.clip(raw_hourly_pricing, -100, 100).round(1)
                hourly_agg = hourly_agg.drop(columns=['_åŸä»·æ€»é¢', '_æˆæœ¬æ€»é¢'])
            else:
                # æ‰€æœ‰å•†å“åŸä»·éƒ½ä¸º0æˆ–æ— æ•ˆ
                hourly_agg['å®šä»·åˆ©æ¶¦ç‡'] = hourly_agg['å®æ”¶åˆ©æ¶¦ç‡']
        else:
            hourly_agg['å®šä»·åˆ©æ¶¦ç‡'] = hourly_agg['å®æ”¶åˆ©æ¶¦ç‡']
        
        result['hourly_trend'] = hourly_agg
        
        # ========== 5. æœ€ä½³æ‹æ¡£ï¼ˆå‰”é™¤è€—æï¼‰ ==========
        order_ids = product_df['è®¢å•ID'].unique()
        
        # è·å–åŒå•å•†å“
        related_orders = df[df['è®¢å•ID'].isin(order_ids)].copy()
        partners = related_orders[related_orders['å•†å“åç§°'] != product_name].copy()
        
        if not partners.empty and category_col:
            # å‰”é™¤è€—æåˆ†ç±»
            partners = partners[partners[category_col] != 'è€—æ'].copy()
        
        if not partners.empty:
            # ç»Ÿè®¡é¢‘æ¬¡å¹¶ä¿ç•™åˆ†ç±»ä¿¡æ¯
            if category_col:
                partner_stats = partners.groupby('å•†å“åç§°').agg({
                    'è®¢å•ID': 'nunique',
                    category_col: 'first'
                }).reset_index()
                partner_stats.columns = ['å•†å“åç§°', 'é¢‘æ¬¡', 'ä¸€çº§åˆ†ç±»']
            else:
                partner_stats = partners.groupby('å•†å“åç§°')['è®¢å•ID'].nunique().reset_index()
                partner_stats.columns = ['å•†å“åç§°', 'é¢‘æ¬¡']
                partner_stats['ä¸€çº§åˆ†ç±»'] = '-'
            
            partner_stats = partner_stats.sort_values('é¢‘æ¬¡', ascending=False).head(10)
            result['partners'] = partner_stats
        
        # ========== 6. è´­ä¹°è§’è‰²åˆ†æï¼ˆå•å“æ—¥è®°ï¼‰ ==========
        # è®¡ç®—æ¯ä¸ªè®¢å•çš„æ€»é‡‘é¢
        order_totals = related_orders.groupby('è®¢å•ID')['å®æ”¶ä»·æ ¼'].sum().to_dict()
        
        def get_role(row):
            if row['åˆ©æ¶¦é¢'] < 0:
                return 'äºæŸå¼•æµ'
            total = order_totals.get(row['è®¢å•ID'], 0)
            if total == 0:
                return 'æ ¸å¿ƒéœ€æ±‚'
            ratio = row['å®æ”¶ä»·æ ¼'] / total
            if ratio > 0.6:
                return 'æ ¸å¿ƒéœ€æ±‚'  # ä¸»ä¹°
            elif ratio < 0.3:
                return 'å‡‘å•é…è§’'  # é¡ºæ‰‹ä¹°
            else:
                return 'æ ¸å¿ƒéœ€æ±‚'
        
        product_df['_role'] = product_df.apply(get_role, axis=1)
        
        role_daily = product_df.groupby(['_date', '_role'])['è®¢å•ID'].nunique().reset_index()
        role_daily.columns = ['æ—¥æœŸ', 'è§’è‰²', 'é”€é‡']
        role_daily['æ—¥æœŸ'] = pd.to_datetime(role_daily['æ—¥æœŸ'])
        result['role_daily'] = role_daily
        
        # ========== 7. ä»·æ ¼æ•æ„Ÿåº¦åˆ†æ ==========
        correlation = 0
        sensitivity_level = 'æ•°æ®ä¸è¶³'
        sensitivity_color = 'gray'
        
        if len(daily_agg) > 3:
            correlation = daily_agg['å¹³å‡å•ä»·'].corr(daily_agg['é”€é‡'])
            
            if pd.isna(correlation):
                correlation = 0
            
            if correlation < -0.6:
                sensitivity_level = 'é«˜æ•æ„Ÿ'
                sensitivity_color = 'red'
            elif correlation < -0.3:
                sensitivity_level = 'ä¸­ç­‰æ•æ„Ÿ'
                sensitivity_color = 'orange'
            elif correlation < 0:
                sensitivity_level = 'ä½æ•æ„Ÿ'
                sensitivity_color = 'green'
            else:
                sensitivity_level = 'ä¸æ•æ„Ÿ'
                sensitivity_color = 'blue'
        
        result['price_sensitivity'] = {
            'correlation': round(correlation, 3),
            'level': sensitivity_level,
            'color': sensitivity_color,
        }
        
        # ========== 8. æ™ºèƒ½å»ºè®® ==========
        recommendations = []
        avg_margin_val = result['summary']['avg_margin']
        
        # è§„åˆ™1: è´Ÿæ¯›åˆ©é¢„è­¦
        if avg_margin_val < 0:
            recommendations.append({
                'title': 'ğŸ›‘ æ­¢æŸå»ºè®®',
                'desc': 'å½“å‰å•†å“å¤„äºäºæŸçŠ¶æ€ï¼Œå»ºè®®ç«‹å³æ£€æŸ¥æˆæœ¬é…ç½®æˆ–æé«˜å”®ä»·ã€‚',
                'type': 'danger'
            })
        
        # è§„åˆ™2: ä½æ¯›åˆ© + ä½æ•æ„Ÿ -> æ¶¨ä»·
        elif avg_margin_val < 15 and correlation > -0.3:
            recommendations.append({
                'title': 'ğŸ’° æ¶¨ä»·æœºä¼š',
                'desc': 'ç”¨æˆ·å¯¹ä»·æ ¼ä¸æ•æ„Ÿä¸”å½“å‰æ¯›åˆ©è¾ƒä½ï¼Œå»ºè®®å°è¯•æä»·ä»¥æå‡åˆ©æ¶¦ã€‚',
                'type': 'success'
            })
        
        # è§„åˆ™3: é«˜æ¯›åˆ© + é«˜æ•æ„Ÿ -> ä¿ƒé”€
        elif avg_margin_val > 40 and correlation < -0.6:
            recommendations.append({
                'title': 'ğŸ“¢ ä»¥ä»·æ¢é‡',
                'desc': 'ç”¨æˆ·å¯¹ä»·æ ¼é«˜åº¦æ•æ„Ÿä¸”æ¯›åˆ©ç©ºé—´å……è¶³ï¼Œå¯å°è¯•çŸ­æœŸä¿ƒé”€æ‹‰åŠ¨é”€é‡ã€‚',
                'type': 'info'
            })
        
        # è§„åˆ™4: æ ¸å¿ƒå•†å“è¯†åˆ«
        if len(result['role_daily']) > 0:
            core_ratio = result['role_daily'][result['role_daily']['è§’è‰²'] == 'æ ¸å¿ƒéœ€æ±‚']['é”€é‡'].sum()
            total_role = result['role_daily']['é”€é‡'].sum()
            if total_role > 0 and core_ratio / total_role > 0.7:
                recommendations.append({
                    'title': 'â­ æ ¸å¿ƒå•†å“',
                    'desc': 'è¯¥å•†å“ä¸»è¦ä½œä¸ºé¡¾å®¢çš„æ ¸å¿ƒéœ€æ±‚è´­ä¹°ï¼Œæ˜¯é—¨åº—çš„å¼•æµå•†å“ã€‚',
                    'type': 'primary'
                })
        
        if not recommendations:
            recommendations.append({
                'title': 'âœ… ç»´æŒç°çŠ¶',
                'desc': 'å½“å‰å•†å“è¡¨ç°å¹³ç¨³ï¼Œå»ºè®®ç»§ç»­ä¿æŒå½“å‰ç­–ç•¥ã€‚',
                'type': 'secondary'
            })
        
        result['recommendations'] = recommendations
        
        return result
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        result['error'] = f'è·å–å•†å“æ´å¯Ÿæ—¶å‡ºé”™: {str(e)}'
        return result
