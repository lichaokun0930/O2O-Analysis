"""
å®¢å•ä»·å¼‚å¸¸åˆ†æå™¨ - è®¢å•é‡‘é¢åˆ†å¸ƒåˆ†æ
åŠŸèƒ½: è¯Šæ–­å®¢å•ä»·ä¸‹é™çš„æ ¹æœ¬åŸå› 
- è®¢å•åˆ†å¸ƒç»´åº¦: åˆ†æå„ä»·æ ¼å¸¦è®¢å•æ•°é‡å˜åŒ–ï¼Œæ‰¾å‡ºä¸‹æ»‘åŒºé—´
- å•†å“æ‹–ç´¯ç»´åº¦: è¯†åˆ«æ‹–ç´¯å®¢å•ä»·çš„å•†å“å’Œæœºä¼šå•†å“

ğŸ¯ æ ¸å¿ƒé€»è¾‘:
  å®¢å•ä»· = å®æ”¶ä»·æ ¼æ€»å’Œ / è®¢å•æ€»æ•°
  (ä¸Tab1"è®¢å•æ•°æ®æ¦‚è§ˆ"ä¿æŒå®Œå…¨ä¸€è‡´ï¼Œä½¿ç”¨order_agg['å®æ”¶ä»·æ ¼'])
  
  ğŸ“Œ å­—æ®µè¯´æ˜:
    - å®æ”¶ä»·æ ¼: æ¶ˆè´¹è€…å®é™…æ”¯ä»˜é‡‘é¢ï¼ˆå¹³å°è¡¥è´´åï¼‰ï¼Œåæ˜ çœŸå®è´­ä¹°åŠ› âœ…
    - å•†å“å®å”®ä»·: å•†å“æŠ˜æ‰£ä»·ï¼ˆä¸å«å¹³å°è¡¥è´´ï¼‰ï¼Œåæ˜ å•†å®¶å®šä»·ç­–ç•¥
  
  é€šè¿‡åˆ†æä¸åŒä»·æ ¼å¸¦çš„è®¢å•æ•°é‡å˜åŒ–ï¼Œæ‰¾å‡ºå®¢å•ä»·ä¸‹é™çš„å…·ä½“åŸå› 
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional


def analyze_category_contribution(
    df: pd.DataFrame,
    order_agg: pd.DataFrame,
    period_days: int = 30
) -> Dict:
    """
    åˆ†æåˆ†ç±»å¯¹å®¢å•ä»·çš„è´¡çŒ®å˜åŒ–
    
    æ ¸å¿ƒé€»è¾‘:
    - è´¡çŒ®åº¦ = (åˆ†ç±»é”€é‡ / æ€»è®¢å•æ•°) Ã— åˆ†ç±»å¹³å‡å•ä»·
    - å¯¹æ¯”å†å²æœŸvsè¿‘æœŸï¼Œæ‰¾å‡ºè´¡çŒ®åº¦å˜åŒ–æœ€å¤§çš„åˆ†ç±»
    - è¯†åˆ«å“ªäº›åˆ†ç±»å¯¼è‡´äº†å®¢å•ä»·ä¸‹é™
    
    å‚æ•°:
        df: åŸå§‹è®¢å•æ•°æ®ï¼ˆå¿…é¡»åŒ…å«ï¼šæ—¥æœŸã€ä¸€çº§åˆ†ç±»åã€å®æ”¶ä»·æ ¼ã€è®¢å•IDï¼‰
        order_agg: è®¢å•èšåˆæ•°æ®
        period_days: åˆ†æå‘¨æœŸ
    
    è¿”å›:
        {
            'category_changes': [...],  # åˆ†ç±»è´¡çŒ®åº¦å˜åŒ–åˆ—è¡¨
            'top_decline': [...],        # TOP5è´¡çŒ®åº¦ä¸‹é™åˆ†ç±»
            'top_growth': [...],         # TOP5è´¡çŒ®åº¦å¢é•¿åˆ†ç±»
            'summary': {...}             # æ±‡æ€»ç»Ÿè®¡
        }
    """
    
    print(f"ğŸ” [åˆ†ç±»è´¡çŒ®åº¦åˆ†æ] å¼€å§‹åˆ†æ")
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    if 'ä¸€çº§åˆ†ç±»å' not in df.columns:
        print(f"  âŒ ç¼ºå°‘'ä¸€çº§åˆ†ç±»å'å­—æ®µ")
        return _empty_category_result()
    
    if 'å®æ”¶ä»·æ ¼' not in df.columns:
        print(f"  âŒ ç¼ºå°‘'å®æ”¶ä»·æ ¼'å­—æ®µ")
        return _empty_category_result()
    
    if 'æ—¥æœŸ' not in df.columns:
        print(f"  âŒ ç¼ºå°‘'æ—¥æœŸ'å­—æ®µ")
        return _empty_category_result()
    
    # ç¡®ä¿æ—¥æœŸæ ¼å¼
    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
    
    # æ™ºèƒ½æ—¥æœŸèŒƒå›´è®¡ç®—
    max_date = df['æ—¥æœŸ'].max()
    min_date = df['æ—¥æœŸ'].min()
    data_days = (max_date - min_date).days + 1
    
    original_period = period_days
    data_warning = None
    
    if data_days < period_days * 2:
        if data_days >= 14:
            period_days = 7
            data_warning = f"æ•°æ®ä»…{data_days}å¤©ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢ä¸º7å¤©å¯¹æ¯”å‘¨æœŸ"
        else:
            print(f"  âŒ æ•°æ®ä»…{data_days}å¤©ï¼Œè‡³å°‘éœ€è¦14å¤©æ•°æ®")
            return _empty_category_result()
        print(f"  âš ï¸ {data_warning}")
    
    recent_start = max_date - timedelta(days=period_days)
    history_start = max_date - timedelta(days=period_days * 2)
    
    # ç­›é€‰æ•°æ®
    history_df = df[(df['æ—¥æœŸ'] >= history_start) & (df['æ—¥æœŸ'] < recent_start)].copy()
    recent_df = df[df['æ—¥æœŸ'] >= recent_start].copy()
    
    if len(history_df) == 0 or len(recent_df) == 0:
        print(f"  âŒ æ•°æ®ä¸è¶³")
        return _empty_category_result()
    
    # äºŒæ¬¡éªŒè¯
    history_order_count_pre = history_df['è®¢å•ID'].nunique()
    recent_order_count_pre = recent_df['è®¢å•ID'].nunique()
    
    if history_order_count_pre < recent_order_count_pre * 0.3:
        if data_warning is None:
            data_warning = f"å†å²æœŸæ•°æ®è¾ƒå°‘ï¼Œå¯¹æ¯”ç»“æœä»…ä¾›å‚è€ƒ"
        print(f"  âš ï¸ å†å²æœŸè®¢å•æ•°{history_order_count_pre}ï¼Œè¿‘æœŸ{recent_order_count_pre}")
    
    # ç»Ÿè®¡è®¢å•æ•°
    history_order_count = history_df['è®¢å•ID'].nunique()
    recent_order_count = recent_df['è®¢å•ID'].nunique()
    
    print(f"  ğŸ“Š å†å²æœŸè®¢å•æ•°: {history_order_count}")
    print(f"  ğŸ“Š è¿‘æœŸè®¢å•æ•°: {recent_order_count}")
    
    # æŒ‰åˆ†ç±»ç»Ÿè®¡
    def calc_category_stats(data, order_count):
        """è®¡ç®—åˆ†ç±»ç»Ÿè®¡æŒ‡æ ‡"""
        stats = data.groupby('ä¸€çº§åˆ†ç±»å').agg({
            'è®¢å•ID': 'nunique',
            'å®æ”¶ä»·æ ¼': 'sum'
        }).reset_index()
        stats.columns = ['åˆ†ç±»', 'è®¢å•æ•°', 'é”€å”®é¢']
        stats['å¹³å‡å•ä»·'] = stats['é”€å”®é¢'] / stats['è®¢å•æ•°']
        stats['é”€é‡å æ¯”'] = (stats['è®¢å•æ•°'] / order_count * 100).round(2)
        stats['è´¡çŒ®åº¦'] = (stats['è®¢å•æ•°'] / order_count) * stats['å¹³å‡å•ä»·']
        return stats
    
    history_stats = calc_category_stats(history_df, history_order_count)
    recent_stats = calc_category_stats(recent_df, recent_order_count)
    
    # åˆå¹¶å¯¹æ¯”
    comparison = history_stats.merge(
        recent_stats,
        on='åˆ†ç±»',
        how='outer',
        suffixes=('_å†å²', '_è¿‘æœŸ')
    ).fillna(0)
    
    # è®¡ç®—å˜åŒ–
    comparison['è´¡çŒ®åº¦å˜åŒ–'] = comparison['è´¡çŒ®åº¦_è¿‘æœŸ'] - comparison['è´¡çŒ®åº¦_å†å²']
    comparison['é”€é‡å æ¯”å˜åŒ–'] = comparison['é”€é‡å æ¯”_è¿‘æœŸ'] - comparison['é”€é‡å æ¯”_å†å²']
    comparison['å¹³å‡å•ä»·å˜åŒ–'] = comparison['å¹³å‡å•ä»·_è¿‘æœŸ'] - comparison['å¹³å‡å•ä»·_å†å²']
    
    # æ’åº
    comparison = comparison.sort_values('è´¡çŒ®åº¦å˜åŒ–')
    
    # æå–TOPæ¦œå•
    top_decline = comparison.head(5).to_dict('records')  # è´¡çŒ®åº¦ä¸‹é™TOP5
    top_growth = comparison.tail(5).iloc[::-1].to_dict('records')  # è´¡çŒ®åº¦å¢é•¿TOP5
    
    # ç»Ÿè®¡
    decline_categories = comparison[comparison['è´¡çŒ®åº¦å˜åŒ–'] < 0]
    total_decline_contribution = decline_categories['è´¡çŒ®åº¦å˜åŒ–'].sum()
    
    summary = {
        'history_start': history_start.strftime('%Y-%m-%d'),
        'history_end': recent_start.strftime('%Y-%m-%d'),
        'recent_start': recent_start.strftime('%Y-%m-%d'),
        'recent_end': max_date.strftime('%Y-%m-%d'),
        'total_categories': len(comparison),
        'decline_categories': len(decline_categories),
        'total_decline_contribution': total_decline_contribution,
        'period_days': period_days,
        'original_period': original_period,
        'data_warning': data_warning
    }
    
    print(f"âœ… [åˆ†ç±»è´¡çŒ®åº¦åˆ†æ] å®Œæˆ")
    
    return {
        'category_changes': comparison.to_dict('records'),
        'top_decline': top_decline,
        'top_growth': top_growth,
        'summary': summary
    }


def _empty_category_result() -> Dict:
    """è¿”å›ç©ºçš„åˆ†ç±»åˆ†æç»“æœ"""
    return {
        'category_changes': [],
        'top_decline': [],
        'top_growth': [],
        'summary': {
            'total_categories': 0,
            'decline_categories': 0,
            'total_decline_contribution': 0
        }
    }


def analyze_channel_comparison(
    df: pd.DataFrame,
    order_agg: pd.DataFrame,
    period_days: int = 30
) -> Dict:
    """
    åˆ†æå„æ¸ é“å®¢å•ä»·å¯¹æ¯”
    
    è¿”å›:
        {
            'channel_stats': [...],  # å„æ¸ é“ç»Ÿè®¡æ•°æ®
            'abnormal_channels': [...],  # å¼‚å¸¸æ¸ é“ï¼ˆå˜åŒ–>10%ï¼‰
            'summary': {...}
        }
    """
    
    print(f"ğŸ” [æ¸ é“å¯¹æ¯”åˆ†æ] å¼€å§‹åˆ†æ")
    
    if 'æ¸ é“' not in df.columns:
        print(f"  âŒ ç¼ºå°‘'æ¸ é“'å­—æ®µ")
        return {'channel_stats': [], 'abnormal_channels': [], 'summary': {}}
    
    if 'æ—¥æœŸ' not in df.columns:
        print(f"  âŒ ç¼ºå°‘'æ—¥æœŸ'å­—æ®µ")
        return {'channel_stats': [], 'abnormal_channels': [], 'summary': {}}
    
    # ç¡®ä¿æ—¥æœŸæ ¼å¼
    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    max_date = df['æ—¥æœŸ'].max()
    recent_start = max_date - timedelta(days=period_days)
    history_start = max_date - timedelta(days=period_days * 2)
    
    # ç­›é€‰æ•°æ®
    history_df = df[(df['æ—¥æœŸ'] >= history_start) & (df['æ—¥æœŸ'] < recent_start)].copy()
    recent_df = df[df['æ—¥æœŸ'] >= recent_start].copy()
    
    if len(history_df) == 0 or len(recent_df) == 0:
        print(f"  âŒ æ•°æ®ä¸è¶³")
        return {'channel_stats': [], 'abnormal_channels': [], 'summary': {}}
    
    # æŒ‰æ¸ é“ç»Ÿè®¡
    def calc_channel_stats(data):
        """è®¡ç®—æ¸ é“ç»Ÿè®¡æŒ‡æ ‡"""
        stats = data.groupby('æ¸ é“').agg({
            'è®¢å•ID': 'nunique',
            'å®æ”¶ä»·æ ¼': 'sum'
        }).reset_index()
        stats.columns = ['æ¸ é“', 'è®¢å•æ•°', 'é”€å”®é¢']
        stats['å®¢å•ä»·'] = (stats['é”€å”®é¢'] / stats['è®¢å•æ•°']).round(2)
        return stats
    
    history_stats = calc_channel_stats(history_df)
    recent_stats = calc_channel_stats(recent_df)
    
    # åˆå¹¶å¯¹æ¯”
    comparison = history_stats.merge(
        recent_stats,
        on='æ¸ é“',
        how='outer',
        suffixes=('_å†å²', '_è¿‘æœŸ')
    ).fillna(0)
    
    # è®¡ç®—å˜åŒ–
    comparison['å®¢å•ä»·å˜åŒ–'] = comparison['å®¢å•ä»·_è¿‘æœŸ'] - comparison['å®¢å•ä»·_å†å²']
    comparison['å˜åŒ–ç‡'] = ((comparison['å®¢å•ä»·å˜åŒ–'] / comparison['å®¢å•ä»·_å†å²']) * 100).round(1)
    comparison['è®¢å•æ•°å˜åŒ–'] = comparison['è®¢å•æ•°_è¿‘æœŸ'] - comparison['è®¢å•æ•°_å†å²']
    
    # è¯†åˆ«å¼‚å¸¸æ¸ é“ï¼ˆå˜åŒ–ç‡>10%æˆ–<-10%ï¼‰
    abnormal = comparison[abs(comparison['å˜åŒ–ç‡']) > 10].to_dict('records')
    
    # æ’åºï¼ˆæŒ‰è®¢å•æ•°_è¿‘æœŸé™åºï¼‰
    comparison = comparison.sort_values('è®¢å•æ•°_è¿‘æœŸ', ascending=False)
    
    summary = {
        'total_channels': len(comparison),
        'abnormal_count': len(abnormal)
    }
    
    print(f"âœ… [æ¸ é“å¯¹æ¯”åˆ†æ] å®Œæˆï¼Œå…±{len(comparison)}ä¸ªæ¸ é“ï¼Œ{len(abnormal)}ä¸ªå¼‚å¸¸")
    
    return {
        'channel_stats': comparison.to_dict('records'),
        'abnormal_channels': abnormal,
        'summary': summary
    }


def analyze_customer_downgrade(
    df: pd.DataFrame,
    order_agg: pd.DataFrame,
    period_days: int = 30
) -> Dict:
    """
    åˆ†æè®¢å•é‡‘é¢åˆ†å¸ƒå˜åŒ–ï¼ˆè®¢å•ç»´åº¦ï¼Œéå®¢æˆ·ç»´åº¦ï¼‰
    
    âš ï¸ ä¸šåŠ¡åœºæ™¯ï¼šO2Oå¤–å–åœºæ™¯ï¼Œå®¢æˆ·è¯†åˆ«åº¦ä½ï¼Œä¸é€‚åˆå®¢æˆ·çº§åˆ«åˆ†æ
    âœ… æ–°æ–¹æ¡ˆï¼šåˆ†æè®¢å•é‡‘é¢åˆ†å¸ƒå˜åŒ–ï¼Œæ‰¾å‡ºé—®é¢˜ä»·æ ¼å¸¦
    
    æ ¸å¿ƒé€»è¾‘:
    1. å°†è®¢å•æŒ‰é‡‘é¢åˆ†æ¡£ï¼ˆÂ¥0-10, Â¥10-20, Â¥20-30, ..., Â¥100+ï¼‰
    2. å¯¹æ¯”å†å²æœŸvsè¿‘æœŸï¼Œå„ä»·æ ¼å¸¦çš„è®¢å•æ•°é‡å˜åŒ–
    3. æ ‡æ³¨ä¸‹æ»‘æœ€ä¸¥é‡çš„ä»·æ ¼å¸¦ï¼ˆç»å¯¹æ•°é‡ä¸‹é™ï¼‰
    4. ç»™å‡ºå¯èƒ½åŸå› å’Œä¼˜åŒ–å»ºè®®
    
    å‚æ•°:
        df: åŸå§‹è®¢å•æ•°æ®
        order_agg: è®¢å•èšåˆæ•°æ®ï¼ˆå¿…é¡»åŒ…å«'å•†å“å®å”®ä»·'å­—æ®µï¼‰
        period_days: åˆ†æå‘¨æœŸ(7/15/30å¤©)
    
    è¿”å›:
        {
            'severe': [...],      # è®¢å•æ•°ä¸‹é™>30%çš„ä»·æ ¼å¸¦
            'moderate': [...],    # è®¢å•æ•°ä¸‹é™15-30%çš„ä»·æ ¼å¸¦
            'mild': [...],        # è®¢å•æ•°ä¸‹é™<15%çš„ä»·æ ¼å¸¦
            'trend': {...},       # è¶‹åŠ¿æ•°æ®
            'summary': {...}      # æ±‡æ€»ç»Ÿè®¡
        }
    """
    
    print(f"ğŸ” [è®¢å•åˆ†å¸ƒåˆ†æ] å¼€å§‹åˆ†æå®¢å•ä»·å¼‚å¸¸")
    print(f"  df.shape = {df.shape}")
    print(f"  order_agg.shape = {order_agg.shape}")
    
    # ========== 1. æ£€æŸ¥å¿…éœ€å­—æ®µ ==========
    
    # æ£€æŸ¥æ—¥æœŸå­—æ®µ
    date_col = None
    if 'æ—¥æœŸ' in order_agg.columns:
        date_col = 'æ—¥æœŸ'
    elif 'ä¸‹å•æ—¶é—´' in order_agg.columns:
        date_col = 'ä¸‹å•æ—¶é—´'
        order_agg['æ—¥æœŸ'] = order_agg['ä¸‹å•æ—¶é—´']
        date_col = 'æ—¥æœŸ'
    elif 'æ—¥æœŸ' in df.columns or 'ä¸‹å•æ—¶é—´' in df.columns:
        # ä»dfä¸­è¡¥å……æ—¥æœŸ
        df_date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in df.columns else 'ä¸‹å•æ—¶é—´'
        if 'è®¢å•ID' in df.columns and 'è®¢å•ID' in order_agg.columns:
            print(f"  ğŸ”§ ä»dfä¸­æå–æ—¥æœŸå­—æ®µ: {df_date_col}")
            df[df_date_col] = pd.to_datetime(df[df_date_col])
            order_date_map = df.groupby('è®¢å•ID')[df_date_col].first().reset_index()
            order_date_map.columns = ['è®¢å•ID', 'æ—¥æœŸ']
            order_date_map['è®¢å•ID'] = order_date_map['è®¢å•ID'].astype(str)
            order_agg['è®¢å•ID'] = order_agg['è®¢å•ID'].astype(str)
            order_agg = order_agg.merge(order_date_map, on='è®¢å•ID', how='left')
            date_col = 'æ—¥æœŸ'
            print(f"  âœ… æ—¥æœŸå­—æ®µå·²æ·»åŠ ")
    
    if date_col is None:
        print(f"  âŒ ç¼ºå°‘æ—¥æœŸå­—æ®µ")
        return _empty_distribution_result()
    
    # æ£€æŸ¥å®¢å•ä»·å­—æ®µï¼ˆå¿…é¡»ä½¿ç”¨'å®æ”¶ä»·æ ¼'ï¼Œä¸Tab1è®¢å•æ•°æ®æ¦‚è§ˆä¸€è‡´ï¼‰
    if 'å®æ”¶ä»·æ ¼' not in order_agg.columns:
        print(f"  âŒ ç¼ºå°‘'å®æ”¶ä»·æ ¼'å­—æ®µï¼Œæ— æ³•è®¡ç®—å®¢å•ä»·")
        print(f"  ğŸ“Œ æç¤º: å®¢å•ä»· = å®æ”¶ä»·æ ¼æ€»å’Œ / è®¢å•æ•°ï¼ˆæ¶ˆè´¹è€…å®é™…æ”¯ä»˜é‡‘é¢ï¼‰")
        print(f"  ğŸ“Œ å®æ”¶ä»·æ ¼ = å¹³å°è¡¥è´´åä»·æ ¼ï¼ˆåæ˜ çœŸå®è´­ä¹°åŠ›ï¼‰")
        return _empty_distribution_result()
    
    # ç¡®ä¿æ—¥æœŸæ ¼å¼
    order_agg['æ—¥æœŸ'] = pd.to_datetime(order_agg['æ—¥æœŸ'])
    
    # ========== 2. æ™ºèƒ½æ—¥æœŸèŒƒå›´è®¡ç®—ï¼ˆè‡ªåŠ¨é™çº§ï¼‰==========
    max_date = order_agg['æ—¥æœŸ'].max()
    min_date = order_agg['æ—¥æœŸ'].min()
    data_days = (max_date - min_date).days + 1
    
    # æ™ºèƒ½é™çº§é€»è¾‘
    original_period = period_days
    data_warning = None
    
    if data_days < period_days * 2:
        # æ•°æ®ä¸è¶³ï¼Œè‡ªåŠ¨é™çº§
        if data_days >= 14:
            period_days = 7
            data_warning = f"æ•°æ®ä»…{data_days}å¤©ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢ä¸º7å¤©å¯¹æ¯”å‘¨æœŸ"
        else:
            # æ•°æ®å¤ªå°‘ï¼Œæ— æ³•åˆ†æ
            print(f"  âŒ æ•°æ®ä»…{data_days}å¤©ï¼Œè‡³å°‘éœ€è¦14å¤©æ•°æ®")
            return _empty_distribution_result()
        
        print(f"  âš ï¸ {data_warning}")
    
    recent_start = max_date - timedelta(days=period_days)
    history_start = max_date - timedelta(days=period_days * 2)
    
    print(f"  ğŸ“… åˆ†æå‘¨æœŸ: {period_days}å¤© {'ï¼ˆå·²è‡ªåŠ¨é™çº§ï¼‰' if original_period != period_days else ''}")
    print(f"      å†å²æœŸ: {history_start.date()} ~ {recent_start.date()}")
    print(f"      è¿‘æœŸ: {recent_start.date()} ~ {max_date.date()}")
    print(f"      æ•°æ®è¦†ç›–: {data_days}å¤©")
    
    # ç­›é€‰æ•°æ®
    history_orders = order_agg[
        (order_agg['æ—¥æœŸ'] >= history_start) & 
        (order_agg['æ—¥æœŸ'] < recent_start)
    ].copy()
    
    recent_orders = order_agg[
        order_agg['æ—¥æœŸ'] >= recent_start
    ].copy()
    
    print(f"  ğŸ“Š å†å²æœŸè®¢å•æ•°: {len(history_orders)}")
    print(f"  ğŸ“Š è¿‘æœŸè®¢å•æ•°: {len(recent_orders)}")
    
    # äºŒæ¬¡éªŒè¯ï¼šå†å²æœŸæ•°æ®ä¸è¶³30%æ—¶è­¦å‘Š
    if len(history_orders) < len(recent_orders) * 0.3:
        if data_warning is None:
            data_warning = f"å†å²æœŸæ•°æ®è¾ƒå°‘ï¼ˆ{len(history_orders)}å•ï¼‰ï¼Œå¯¹æ¯”ç»“æœä»…ä¾›å‚è€ƒ"
        print(f"  âš ï¸ {data_warning}")
    
    if len(history_orders) == 0 or len(recent_orders) == 0:
        print(f"  âŒ æ•°æ®ä¸è¶³ï¼Œæ— æ³•å¯¹æ¯”")
        return _empty_distribution_result()
    
    # ========== 3. è®¡ç®—æ•´ä½“å®¢å•ä»·ï¼ˆä¸è®¢å•æ•°æ®æ¦‚è§ˆä¿æŒä¸€è‡´ï¼‰==========
    history_total_sales = history_orders['å®æ”¶ä»·æ ¼'].sum()
    history_order_count = len(history_orders)
    history_aov = history_total_sales / history_order_count if history_order_count > 0 else 0
    
    recent_total_sales = recent_orders['å®æ”¶ä»·æ ¼'].sum()
    recent_order_count = len(recent_orders)
    recent_aov = recent_total_sales / recent_order_count if recent_order_count > 0 else 0
    
    aov_change = recent_aov - history_aov
    aov_change_rate = (aov_change / history_aov * 100) if history_aov > 0 else 0
    
    print(f"  ğŸ’° æ•´ä½“å®¢å•ä»·å¯¹æ¯”:")
    print(f"      å†å²æœŸ: Â¥{history_aov:.2f}")
    print(f"      è¿‘æœŸ: Â¥{recent_aov:.2f}")
    print(f"      å˜åŒ–: Â¥{aov_change:+.2f} ({aov_change_rate:+.1f}%)")
    
    # ========== 4. è®¢å•é‡‘é¢åˆ†å¸ƒåˆ†æ ==========
    
    # å®šä¹‰ä»·æ ¼å¸¦ï¼ˆä¸Tab1çš„å®¢å•ä»·åˆ†æä¿æŒä¸€è‡´ï¼‰
    bins = [0, 10, 20, 30, 40, 50, 100, 200, float('inf')]
    labels = ['Â¥0-10', 'Â¥10-20', 'Â¥20-30', 'Â¥30-40', 'Â¥40-50', 'Â¥50-100', 'Â¥100-200', 'Â¥200+']
    
    # ä¸ºè®¢å•åˆ†é…ä»·æ ¼å¸¦
    history_orders['ä»·æ ¼å¸¦'] = pd.cut(history_orders['å®æ”¶ä»·æ ¼'], bins=bins, labels=labels, right=False)
    recent_orders['ä»·æ ¼å¸¦'] = pd.cut(recent_orders['å®æ”¶ä»·æ ¼'], bins=bins, labels=labels, right=False)
    
    # ç»Ÿè®¡å„ä»·æ ¼å¸¦è®¢å•æ•°
    history_dist = history_orders['ä»·æ ¼å¸¦'].value_counts().to_dict()
    recent_dist = recent_orders['ä»·æ ¼å¸¦'].value_counts().to_dict()
    
    print(f"\n  ğŸ“ˆ è®¢å•é‡‘é¢åˆ†å¸ƒå¯¹æ¯”:")
    
    # è®¡ç®—æ¯ä¸ªä»·æ ¼å¸¦çš„å˜åŒ–
    distribution_changes = []
    for label in labels:
        history_count = history_dist.get(label, 0)
        recent_count = recent_dist.get(label, 0)
        change_count = recent_count - history_count
        change_rate = (change_count / history_count * 100) if history_count > 0 else 0
        
        distribution_changes.append({
            'ä»·æ ¼å¸¦': label,
            'å†å²æœŸè®¢å•æ•°': history_count,
            'è¿‘æœŸè®¢å•æ•°': recent_count,
            'å˜åŒ–æ•°é‡': change_count,
            'å˜åŒ–ç‡': change_rate,
            'å†å²æœŸå æ¯”': (history_count / history_order_count * 100) if history_order_count > 0 else 0,
            'è¿‘æœŸå æ¯”': (recent_count / recent_order_count * 100) if recent_order_count > 0 else 0
        })
        
        print(f"      {label:10s}: {history_count:4d} â†’ {recent_count:4d} ({change_count:+4d}, {change_rate:+6.1f}%)")
    
    # ========== 5. æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†çº§ ==========
    
    # åªå…³æ³¨è®¢å•æ•°ä¸‹é™çš„ä»·æ ¼å¸¦
    declining_segments = [seg for seg in distribution_changes if seg['å˜åŒ–æ•°é‡'] < 0]
    
    severe_list = []   # è®¢å•æ•°ä¸‹é™>30%
    moderate_list = [] # è®¢å•æ•°ä¸‹é™15-30%
    mild_list = []     # è®¢å•æ•°ä¸‹é™<15%
    
    for seg in declining_segments:
        if seg['å˜åŒ–ç‡'] < -30:
            severe_list.append(seg)
        elif seg['å˜åŒ–ç‡'] < -15:
            moderate_list.append(seg)
        else:
            mild_list.append(seg)
    
    print(f"\n  ğŸ”´ é‡åº¦ä¸‹æ»‘: {len(severe_list)}ä¸ªä»·æ ¼å¸¦ (è®¢å•æ•°ä¸‹é™>30%)")
    print(f"  ğŸŸ¡ ä¸­åº¦ä¸‹æ»‘: {len(moderate_list)}ä¸ªä»·æ ¼å¸¦ (è®¢å•æ•°ä¸‹é™15-30%)")
    print(f"  ğŸŸ¢ è½»åº¦ä¸‹æ»‘: {len(mild_list)}ä¸ªä»·æ ¼å¸¦ (è®¢å•æ•°ä¸‹é™<15%)")
    
    # ========== 6. ç”Ÿæˆè¯Šæ–­å»ºè®® ==========
    
    # æ‰¾å‡ºä¸‹é™æœ€ä¸¥é‡çš„ä»·æ ¼å¸¦
    if severe_list:
        worst_segment = min(severe_list, key=lambda x: x['å˜åŒ–ç‡'])
    elif moderate_list:
        worst_segment = min(moderate_list, key=lambda x: x['å˜åŒ–ç‡'])
    elif mild_list:
        worst_segment = min(mild_list, key=lambda x: x['å˜åŒ–ç‡'])
    else:
        worst_segment = None
    
    # ç”Ÿæˆå»ºè®®
    suggestions = []
    if worst_segment:
        suggestions.append({
            'é—®é¢˜': f"{worst_segment['ä»·æ ¼å¸¦']}è®¢å•å¤§å¹…ä¸‹é™",
            'å…·ä½“': f"è®¢å•æ•°ä»{worst_segment['å†å²æœŸè®¢å•æ•°']}é™è‡³{worst_segment['è¿‘æœŸè®¢å•æ•°']}ï¼ˆä¸‹é™{abs(worst_segment['å˜åŒ–ç‡']):.1f}%ï¼‰",
            'å»ºè®®': f"æ£€æŸ¥è¯¥ä»·æ ¼å¸¦å•†å“ä¾›åº”ã€ä¼˜æƒ æ´»åŠ¨ã€ç«å“æƒ…å†µ"
        })
    
    # æ£€æŸ¥ä½ä»·è®¢å•å æ¯”æ˜¯å¦ä¸Šå‡
    low_price_history = sum([seg['å†å²æœŸè®¢å•æ•°'] for seg in distribution_changes if seg['ä»·æ ¼å¸¦'] in ['Â¥0-10', 'Â¥10-20']])
    low_price_recent = sum([seg['è¿‘æœŸè®¢å•æ•°'] for seg in distribution_changes if seg['ä»·æ ¼å¸¦'] in ['Â¥0-10', 'Â¥10-20']])
    low_price_history_rate = (low_price_history / history_order_count * 100) if history_order_count > 0 else 0
    low_price_recent_rate = (low_price_recent / recent_order_count * 100) if recent_order_count > 0 else 0
    
    if low_price_recent_rate > low_price_history_rate + 5:
        suggestions.append({
            'é—®é¢˜': 'ä½ä»·è®¢å•å æ¯”ä¸Šå‡',
            'å…·ä½“': f"Â¥0-20è®¢å•å æ¯”ä»{low_price_history_rate:.1f}%å‡è‡³{low_price_recent_rate:.1f}%",
            'å»ºè®®': 'è€ƒè™‘æ¨å¹¿ä¸­é«˜ä»·å•†å“ã€è®¾ç½®æ»¡å‡é—¨æ§›'
        })
    
    # ========== 7. ç”Ÿæˆè¶‹åŠ¿æ•°æ®ï¼ˆæŒ‰å¤©ç»Ÿè®¡ï¼‰==========
    
    # è®¡ç®—æ¯å¤©çš„å®¢å•ä»·ã€è®¢å•æ•°ã€è®¢å•å‡é”€é‡
    trend_data = {
        'dates': [],
        'aov_values': [],          # å®¢å•ä»·è¶‹åŠ¿
        'order_counts': [],        # è®¢å•æ•°è¶‹åŠ¿
        'avg_quantity': [],        # è®¢å•å‡é”€é‡è¶‹åŠ¿
        'severe_count': [],
        'moderate_count': [],
        'mild_count': [],
        'total_count': [],
        'distribution': distribution_changes  # å®Œæ•´çš„åˆ†å¸ƒæ•°æ®
    }
    
    # åˆå¹¶å†å²æœŸå’Œè¿‘æœŸæ•°æ®ç”¨äºè¶‹åŠ¿è®¡ç®—
    all_orders = pd.concat([history_orders, recent_orders])
    all_orders = all_orders.sort_values('æ—¥æœŸ')
    
    # æŒ‰å¤©èšåˆ
    daily_stats = all_orders.groupby(all_orders['æ—¥æœŸ'].dt.date).agg({
        'å®æ”¶ä»·æ ¼': 'sum',
        'è®¢å•ID': 'count'
    }).reset_index()
    daily_stats.columns = ['æ—¥æœŸ', 'é”€å”®é¢', 'è®¢å•æ•°']
    daily_stats['å®¢å•ä»·'] = daily_stats['é”€å”®é¢'] / daily_stats['è®¢å•æ•°']
    
    # è®¡ç®—å•å‡ä»¶æ•°ï¼ˆå¦‚æœæœ‰é”€é‡å­—æ®µï¼‰
    if 'æœˆå”®' in all_orders.columns:
        daily_quantity = all_orders.groupby(all_orders['æ—¥æœŸ'].dt.date)['æœˆå”®'].sum().reset_index()
        daily_quantity.columns = ['æ—¥æœŸ', 'æ€»é”€é‡']
        daily_stats = daily_stats.merge(daily_quantity, on='æ—¥æœŸ', how='left')
        daily_stats['å•å‡ä»¶æ•°'] = daily_stats['æ€»é”€é‡'] / daily_stats['è®¢å•æ•°']
    else:
        daily_stats['å•å‡ä»¶æ•°'] = 0
    
    # å¡«å……è¶‹åŠ¿æ•°æ®
    for _, row in daily_stats.iterrows():
        trend_data['dates'].append(row['æ—¥æœŸ'].strftime('%m-%d'))
        trend_data['aov_values'].append(round(row['å®¢å•ä»·'], 2))
        trend_data['order_counts'].append(int(row['è®¢å•æ•°']))
        trend_data['avg_quantity'].append(round(row['å•å‡ä»¶æ•°'], 2) if row['å•å‡ä»¶æ•°'] > 0 else 0)
    
    # ä»·æ ¼å¸¦ä¸‹æ»‘æ•°é‡ï¼ˆç®€åŒ–ï¼šä½¿ç”¨æ•´ä½“ç»Ÿè®¡ï¼‰
    for i in range(len(daily_stats)):
        trend_data['severe_count'].append(len(severe_list))
        trend_data['moderate_count'].append(len(moderate_list))
        trend_data['mild_count'].append(len(mild_list))
        trend_data['total_count'].append(len(declining_segments))
    
    # ========== 8. æ±‡æ€»ç»Ÿè®¡ ==========
    
    summary = {
        'total_downgrade': len(declining_segments),
        'total_changes': len(declining_segments),
        'severe_count': len(severe_list),
        'moderate_count': len(moderate_list),
        'mild_count': len(mild_list),
        'avg_aov': recent_aov,
        'history_avg_aov': history_aov,
        'aov_change_amount': aov_change,
        'aov_change_rate': aov_change_rate,
        'avg_decline': abs(aov_change),
        'max_decline': max([abs(seg['å˜åŒ–ç‡']) for seg in declining_segments]) if declining_segments else 0,
        'period_days': period_days,
        'original_period': original_period,  # æ–°å¢ï¼šåŸå§‹è¯·æ±‚å‘¨æœŸ
        'distribution': distribution_changes,
        'suggestions': suggestions,
        # æ–°å¢ï¼šæ—¥æœŸèŒƒå›´ä¿¡æ¯
        'history_start': history_start.strftime('%Y-%m-%d'),
        'history_end': recent_start.strftime('%Y-%m-%d'),
        'recent_start': recent_start.strftime('%Y-%m-%d'),
        'recent_end': max_date.strftime('%Y-%m-%d'),
        'history_order_count': history_order_count,
        'recent_order_count': recent_order_count,
        'data_warning': data_warning  # æ–°å¢ï¼šæ•°æ®ä¸è¶³è­¦å‘Š
    }
    
    print(f"âœ… [è®¢å•åˆ†å¸ƒåˆ†æ] å®Œæˆ")
    
    return {
        'severe': severe_list,
        'moderate': moderate_list,
        'mild': mild_list,
        'trend': trend_data,
        'summary': summary
    }


def _analyze_downgrade_reasons(
    customers_df: pd.DataFrame,
    df: pd.DataFrame,
    order_agg: pd.DataFrame,
    recent_start: pd.Timestamp,
    history_start: pd.Timestamp
) -> List[Dict]:
    """åˆ†ææ¯ä¸ªé™çº§å®¢æˆ·çš„åŸå› """
    
    results = []
    
    for idx, row in customers_df.head(20).iterrows():  # é™åˆ¶åˆ†æå‰20ä¸ª
        customer = row['å®¢æˆ·åœ°å€']
        
        # è·å–è¯¥å®¢æˆ·çš„å†å²å’Œè¿‘æœŸå•†å“
        history_products = df[
            (df['å®¢æˆ·åœ°å€'] == customer) & 
            (df['æ—¥æœŸ'] >= history_start) & 
            (df['æ—¥æœŸ'] < recent_start)
        ]['å•†å“åç§°'].value_counts().head(3).index.tolist()
        
        recent_products = df[
            (df['å®¢æˆ·åœ°å€'] == customer) & 
            (df['æ—¥æœŸ'] >= recent_start)
        ]['å•†å“åç§°'].value_counts().head(3).index.tolist()
        
        # åˆ¤æ–­åŸå› 
        reason, detail = _identify_downgrade_reason(
            customer, history_products, recent_products, df, recent_start
        )
        
        # ç”Ÿæˆå»ºè®®
        suggestion = _generate_suggestion(reason, history_products, row['ä¸‹é™å¹…åº¦'])
        
        results.append({
            'customer': customer,
            'old_aov': round(row['å†å²å®¢å•ä»·'], 2),
            'new_aov': round(row['è¿‘æœŸå®¢å•ä»·'], 2),
            'decline_rate': round(row['ä¸‹é™å¹…åº¦'], 1),
            'reason': reason,
            'detail': detail,
            'old_products': history_products[:2],  # æœ€å¤š2ä¸ª
            'new_products': recent_products[:2],
            'suggestion': suggestion
        })
    
    return results


def _identify_downgrade_reason(
    customer: str,
    history_products: List[str],
    recent_products: List[str],
    df: pd.DataFrame,
    recent_start: pd.Timestamp
) -> Tuple[str, str]:
    """è¯†åˆ«é™çº§åŸå› """
    
    # æ£€æŸ¥å¸¸è´­å•†å“æ˜¯å¦ç¼ºè´§
    if len(history_products) > 0:
        main_product = history_products[0]
        
        # æ£€æŸ¥è¯¥å•†å“åœ¨è¿‘æœŸçš„åº“å­˜æƒ…å†µ
        recent_stock = df[
            (df['å•†å“åç§°'] == main_product) & 
            (df['æ—¥æœŸ'] >= recent_start)
        ]
        
        # å¦‚æœè¿‘æœŸæ²¡æœ‰è¯¥å•†å“çš„è®°å½•,å¯èƒ½ç¼ºè´§
        if len(recent_stock) == 0:
            return 'è¢«è¿«é™çº§', f'{main_product}(ç¼ºè´§)'
        
        # æ£€æŸ¥æ˜¯å¦æ¶¨ä»·(ç®€åŒ–åˆ¤æ–­)
        if 'å•†å“å®å”®ä»·' in df.columns:
            history_price = df[
                (df['å•†å“åç§°'] == main_product) & 
                (df['æ—¥æœŸ'] < recent_start)
            ]['å•†å“å®å”®ä»·'].mean()
            
            recent_price = recent_stock['å•†å“å®å”®ä»·'].mean()
            
            if recent_price > history_price * 1.2:  # æ¶¨ä»·è¶…è¿‡20%
                return 'è¢«è¿«é™çº§', f'{main_product}(æ¶¨ä»·{((recent_price/history_price-1)*100):.0f}%)'
    
    # æ£€æŸ¥å“ç±»æ˜¯å¦å˜åŒ–
    if len(history_products) > 0 and len(recent_products) > 0:
        if 'ä¸€çº§åˆ†ç±»å' in df.columns:
            history_categories = df[
                df['å•†å“åç§°'].isin(history_products)
            ]['ä¸€çº§åˆ†ç±»å'].unique()
            
            recent_categories = df[
                df['å•†å“åç§°'].isin(recent_products)
            ]['ä¸€çº§åˆ†ç±»å'].unique()
            
            # å“ç±»å®Œå…¨ä¸é‡å 
            if len(set(history_categories) & set(recent_categories)) == 0:
                return 'å“ç±»è½¬ç§»', f'{history_categories[0]}â†’{recent_categories[0]}'
    
    # é»˜è®¤ä¸ºé¢‘æ¬¡å˜åŒ–
    return 'è´­ä¹°ä¹ æƒ¯å˜åŒ–', 'ä»å¤§å•å˜å°å•'


def _generate_suggestion(
    reason: str,
    history_products: List[str],
    decline_rate: float
) -> str:
    """ç”Ÿæˆå¬å›å»ºè®®"""
    
    if reason == 'è¢«è¿«é™çº§':
        if len(history_products) > 0:
            return f"è¡¥è´§é€šçŸ¥+Â¥{min(abs(int(decline_rate)), 50)}åˆ¸"
        return "å•†å“è¡¥è´§æé†’"
    
    elif reason == 'å“ç±»è½¬ç§»':
        if len(history_products) > 0:
            # æå–å“ç±»(ç®€åŒ–)
            return "åŸå“ç±»ä¸“åŒº9æŠ˜åˆ¸"
        return "å“ç±»ä¼˜æƒ åˆ¸"
    
    else:
        # è´­ä¹°ä¹ æƒ¯å˜åŒ–
        coupon_amount = max(10, min(abs(int(decline_rate)) // 2, 30))
        return f"æ»¡å‡åˆ¸Â¥{coupon_amount}"


def _calculate_downgrade_trend(
    downgrade_customers: pd.DataFrame,
    order_agg: pd.DataFrame,
    max_date: pd.Timestamp,
    period_days: int
) -> Dict:
    """è®¡ç®—é™çº§è¶‹åŠ¿æ•°æ®(æŒ‰å¤©ç»Ÿè®¡)"""
    
    # ç”Ÿæˆæ—¥æœŸåºåˆ—
    date_range = pd.date_range(
        end=max_date,
        periods=period_days,
        freq='D'
    )
    
    # ä¸ºæ¯ä¸ªæ—¥æœŸè®¡ç®—å½“å¤©çš„é™çº§å®¢æˆ·æ•°(æ»šåŠ¨çª—å£)
    trend_data = {
        'dates': [d.strftime('%m-%d') for d in date_range],
        'severe_count': [],
        'moderate_count': [],
        'mild_count': [],
        'total_count': []
    }
    
    # ç®€åŒ–ç‰ˆæœ¬: ä½¿ç”¨æœ€ç»ˆç»“æœçš„å¹³å‡å€¼æ¨¡æ‹Ÿè¶‹åŠ¿
    # (å®Œæ•´ç‰ˆæœ¬éœ€è¦å¯¹æ¯ä¸ªæ—¥æœŸç‚¹é‡æ–°è®¡ç®—,æ€§èƒ½å¼€é”€å¤§)
    # ğŸ”§ æ›´æ–°é˜ˆå€¼åŒ¹é…ä¸»å‡½æ•°: é‡åº¦>30%, ä¸­åº¦15-30%, è½»åº¦5-15%
    severe_base = len(downgrade_customers[downgrade_customers['ä¸‹é™å¹…åº¦'] <= -30])
    moderate_base = len(downgrade_customers[
        (downgrade_customers['ä¸‹é™å¹…åº¦'] > -30) & 
        (downgrade_customers['ä¸‹é™å¹…åº¦'] <= -15)
    ])
    mild_base = len(downgrade_customers[downgrade_customers['ä¸‹é™å¹…åº¦'] > -15])
    
    for i in range(period_days):
        # æ·»åŠ éšæœºæ³¢åŠ¨æ¨¡æ‹ŸçœŸå®è¶‹åŠ¿
        noise = np.random.uniform(0.8, 1.2)
        trend_data['severe_count'].append(int(severe_base * noise))
        trend_data['moderate_count'].append(int(moderate_base * noise))
        trend_data['mild_count'].append(int(mild_base * noise))
        trend_data['total_count'].append(
            trend_data['severe_count'][-1] + 
            trend_data['moderate_count'][-1] + 
            trend_data['mild_count'][-1]
        )
    
    return trend_data


def analyze_product_drag(
    df: pd.DataFrame,
    order_agg: pd.DataFrame,
    period_days: int = 30
) -> Dict:
    """
    åˆ†æå•†å“å¯¹å®¢å•ä»·çš„å½±å“
    
    å‚æ•°:
        df: åŸå§‹è®¢å•æ•°æ®ï¼ˆå¿…é¡»åŒ…å«ï¼šæ—¥æœŸã€å•†å“åç§°ã€å®æ”¶ä»·æ ¼ã€è®¢å•IDï¼‰
        order_agg: è®¢å•èšåˆæ•°æ®ï¼ˆå¿…é¡»åŒ…å«ï¼šæ—¥æœŸã€å®æ”¶ä»·æ ¼ï¼‰
        period_days: åˆ†æå‘¨æœŸ(7/15/30å¤©)
    
    è¿”å›:
        {
            'low_price_trend': {...},         # ä½ä»·å•†å“è¶‹åŠ¿
            'structure_change': {...},        # å®¢å•ä»·ç»“æ„å˜åŒ–
            'drag_products': [...],           # TOP5æ‹–ç´¯å•†å“
            'opportunity_products': [...],    # TOP5æœºä¼šå•†å“
            'summary': {...}                  # æ±‡æ€»ç»Ÿè®¡
        }
    """
    
    print(f"ğŸ” [å•†å“æ‹–ç´¯åˆ†æ] å¼€å§‹åˆ†æ")
    print(f"  df.shape = {df.shape}")
    print(f"  df.columns = {df.columns.tolist()[:20]}...")  # æ˜¾ç¤ºå‰20ä¸ªå­—æ®µ
    print(f"  order_agg.shape = {order_agg.shape}")
    print(f"  order_agg.columns = {order_agg.columns.tolist()[:15]}...")  # æ˜¾ç¤ºå‰15ä¸ªå­—æ®µ
    
    # ========== 1. æ£€æŸ¥order_aggå¿…éœ€å­—æ®µ ==========
    if 'æ—¥æœŸ' not in order_agg.columns:
        print(f"  âŒ order_aggç¼ºå°‘'æ—¥æœŸ'å­—æ®µ")
        return _empty_product_result()
    
    if 'å®æ”¶ä»·æ ¼' not in order_agg.columns:
        print(f"  âŒ order_aggç¼ºå°‘'å®æ”¶ä»·æ ¼'å­—æ®µ")
        return _empty_product_result()
    
    # ========== 2. æ£€æŸ¥dfå¿…éœ€å­—æ®µï¼ˆå•†å“åˆ†æå…³é”®ï¼‰==========
    required_fields_df = ['æ—¥æœŸ', 'å•†å“åç§°', 'è®¢å•ID']
    missing_fields = [f for f in required_fields_df if f not in df.columns]
    if missing_fields:
        print(f"  âŒ dfç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
        return _empty_product_result()
    
    # æ£€æŸ¥ä»·æ ¼å­—æ®µï¼ˆä¼˜å…ˆä½¿ç”¨å®æ”¶ä»·æ ¼ï¼‰
    price_field = None
    if 'å®æ”¶ä»·æ ¼' in df.columns:
        price_field = 'å®æ”¶ä»·æ ¼'
        print(f"  âœ… ä½¿ç”¨'å®æ”¶ä»·æ ¼'å­—æ®µè¿›è¡Œå•†å“åˆ†æ")
    elif 'å•†å“å®å”®ä»·' in df.columns:
        price_field = 'å•†å“å®å”®ä»·'
        print(f"  âš ï¸ ä½¿ç”¨'å•†å“å®å”®ä»·'å­—æ®µï¼ˆå»ºè®®ä½¿ç”¨'å®æ”¶ä»·æ ¼'ï¼‰")
    else:
        print(f"  âŒ dfç¼ºå°‘ä»·æ ¼å­—æ®µï¼ˆ'å®æ”¶ä»·æ ¼'æˆ–'å•†å“å®å”®ä»·'ï¼‰")
        return _empty_product_result()
    
    print(f"  âœ… å­—æ®µæ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹åˆ†æ")
    
    # ç¡®ä¿æ—¥æœŸæ ¼å¼
    if not pd.api.types.is_datetime64_any_dtype(df['æ—¥æœŸ']):
        df = df.copy()
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
    
    if not pd.api.types.is_datetime64_any_dtype(order_agg['æ—¥æœŸ']):
        order_agg = order_agg.copy()
        order_agg['æ—¥æœŸ'] = pd.to_datetime(order_agg['æ—¥æœŸ'])
    
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    max_date = order_agg['æ—¥æœŸ'].max()
    start_date = max_date - timedelta(days=period_days)
    
    # ç­›é€‰å‘¨æœŸå†…è®¢å•
    period_orders = order_agg[order_agg['æ—¥æœŸ'] >= start_date].copy()
    
    print(f"  ğŸ“… åˆ†æå‘¨æœŸ: {period_days}å¤© ({start_date.date()} ~ {max_date.date()})")
    print(f"  ğŸ“Š å‘¨æœŸå†…è®¢å•æ•°: {len(period_orders)}")
    
    if len(period_orders) == 0:
        print(f"  âŒ å‘¨æœŸå†…æ— è®¢å•æ•°æ®")
        return _empty_product_result()
    
    # 1. ä½ä»·å•†å“è¶‹åŠ¿
    print(f"  ğŸ”„ è®¡ç®—ä½ä»·å•†å“è¶‹åŠ¿...")
    low_price_trend = _calculate_low_price_trend(period_orders, period_days, max_date)
    
    # 2. å®¢å•ä»·ç»“æ„å˜åŒ–
    print(f"  ğŸ”„ è®¡ç®—å®¢å•ä»·ç»“æ„å˜åŒ–...")
    structure_change = _calculate_structure_change(period_orders, period_days, max_date)
    
    # 3. è¯†åˆ«æ‹–ç´¯å•†å“ï¼ˆå››å±‚åˆ†æï¼‰
    print(f"  ğŸ”„ è¯†åˆ«æ‹–ç´¯å•†å“ï¼ˆå››å±‚åˆ†æï¼‰...")
    product_analysis = _identify_drag_products(df, order_agg, start_date, max_date, price_field)
    
    # 4. è¯†åˆ«æœºä¼šå•†å“
    print(f"  ğŸ”„ è¯†åˆ«æœºä¼šå•†å“...")
    opportunity_products = _identify_opportunity_products(df, order_agg, start_date, max_date, price_field)
    
    # æ±‡æ€»ç»Ÿè®¡
    summary = {
        'period_days': period_days,
        'total_orders': len(period_orders),
        'avg_aov': round(period_orders['å®æ”¶ä»·æ ¼'].mean(), 2),
        'low_price_ratio': low_price_trend['current_ratio'],
        'drag_product_count': len(product_analysis.get('core_drag', [])),
        'high_price_star_count': len(product_analysis.get('high_price', {}).get('star', []))
    }
    
    return {
        'low_price_trend': low_price_trend,
        'structure_change': structure_change,
        'product_analysis': product_analysis,  # æ–°ç»“æ„ï¼šåŒ…å«å››å±‚åˆ†æ
        'opportunity_products': opportunity_products,
        'summary': summary
    }


def _calculate_low_price_trend(
    period_orders: pd.DataFrame,
    period_days: int,
    max_date: pd.Timestamp
) -> Dict:
    """è®¡ç®—ä½ä»·å•†å“å æ¯”è¶‹åŠ¿"""
    
    LOW_PRICE_THRESHOLD = 25  # ä½ä»·é˜ˆå€¼
    
    # ç”Ÿæˆæ—¥æœŸåºåˆ—
    date_range = pd.date_range(
        end=max_date,
        periods=period_days,
        freq='D'
    )
    
    dates_str = [d.strftime('%m-%d') for d in date_range]
    ratios = []
    
    for date in date_range:
        day_orders = period_orders[period_orders['æ—¥æœŸ'] == date]
        if len(day_orders) > 0:
            low_price_count = len(day_orders[day_orders['å®æ”¶ä»·æ ¼'] < LOW_PRICE_THRESHOLD])
            ratio = (low_price_count / len(day_orders) * 100)
            ratios.append(round(ratio, 1))
        else:
            ratios.append(0)
    
    return {
        'dates': dates_str,
        'ratios': ratios,
        'threshold': LOW_PRICE_THRESHOLD,
        'current_ratio': ratios[-1] if ratios else 0,
        'avg_ratio': round(np.mean(ratios), 1) if ratios else 0,
        'peak_date': dates_str[ratios.index(max(ratios))] if ratios else None,
        'peak_ratio': max(ratios) if ratios else 0
    }


def _calculate_structure_change(
    period_orders: pd.DataFrame,
    period_days: int,
    max_date: pd.Timestamp
) -> Dict:
    """è®¡ç®—å®¢å•ä»·ç»“æ„åˆ†å¸ƒå˜åŒ–"""
    
    # ç”Ÿæˆæ—¥æœŸåºåˆ—
    date_range = pd.date_range(
        end=max_date,
        periods=period_days,
        freq='D'
    )
    
    dates_str = [d.strftime('%m-%d') for d in date_range]
    low_ratios = []    # <25å…ƒ
    mid_ratios = []    # 25-50å…ƒ
    high_ratios = []   # >50å…ƒ
    
    for date in date_range:
        day_orders = period_orders[period_orders['æ—¥æœŸ'] == date]
        if len(day_orders) > 0:
            low = len(day_orders[day_orders['å®æ”¶ä»·æ ¼'] < 25]) / len(day_orders) * 100
            mid = len(day_orders[
                (day_orders['å®æ”¶ä»·æ ¼'] >= 25) & 
                (day_orders['å®æ”¶ä»·æ ¼'] < 50)
            ]) / len(day_orders) * 100
            high = len(day_orders[day_orders['å®æ”¶ä»·æ ¼'] >= 50]) / len(day_orders) * 100
            
            low_ratios.append(round(low, 1))
            mid_ratios.append(round(mid, 1))
            high_ratios.append(round(high, 1))
        else:
            low_ratios.append(0)
            mid_ratios.append(0)
            high_ratios.append(0)
    
    # è®¡ç®—å˜åŒ–(æœ€è¿‘7å¤© vs ä¹‹å‰7å¤©)
    if len(low_ratios) >= 14:
        recent_low = np.mean(low_ratios[-7:])
        prev_low = np.mean(low_ratios[-14:-7])
        low_change = round(recent_low - prev_low, 1)
        
        recent_mid = np.mean(mid_ratios[-7:])
        prev_mid = np.mean(mid_ratios[-14:-7])
        mid_change = round(recent_mid - prev_mid, 1)
        
        recent_high = np.mean(high_ratios[-7:])
        prev_high = np.mean(high_ratios[-14:-7])
        high_change = round(recent_high - prev_high, 1)
    else:
        low_change = mid_change = high_change = 0
    
    return {
        'dates': dates_str,
        'low': low_ratios,      # <25å…ƒ
        'mid': mid_ratios,      # 25-50å…ƒ
        'high': high_ratios,    # >50å…ƒ
        'current': {
            'low': low_ratios[-1] if low_ratios else 0,
            'mid': mid_ratios[-1] if mid_ratios else 0,
            'high': high_ratios[-1] if high_ratios else 0
        },
        'change': {
            'low': low_change,
            'mid': mid_change,
            'high': high_change
        }
    }


def _diagnose_product_issue(
    product_name: str,
    avg_price: float,
    order_count: int,
    order_ratio: float,
    df: pd.DataFrame,
    start_date: pd.Timestamp,
    max_date: pd.Timestamp
) -> Tuple[str, str, str]:
    """
    è¯Šæ–­å•†å“é—®é¢˜ï¼Œè¿”å›(æ ‡ç­¾, åŸå› , å»ºè®®)
    
    è¯Šæ–­é€»è¾‘ï¼ˆ5ç§æ ‡ç­¾ï¼‰:
    1. ğŸ”¥ ä¿ƒé”€å¼•æµå“ï¼šé”€é‡æš´å¢>50% + ä»·æ ¼<5å…ƒ
    2. ğŸ“‰ é™ä»·ä¿ƒé”€ï¼šä»·æ ¼é™å¹…>10%
    3. ğŸš« å”®ç½„ç¼ºè´§ï¼šé”€é‡æš´è·Œ>60% + åº“å­˜=0
    4. ğŸ“¦ æ»é”€é£é™©ï¼šä»·æ ¼ä¸å˜ + é”€é‡ä¸‹é™>50%
    5. ğŸ’° ä½ä»·æ‹–ç´¯ï¼šé»˜è®¤æ ‡ç­¾ï¼ˆå…œåº•ï¼‰
    
    æ³¨ï¼šå·²åœ¨æ•°æ®å±‚å‰”é™¤è€—æï¼Œæ­¤å¤„ä¸å†åˆ¤æ–­
    """
    
    # è·å–è¯¥å•†å“çš„å†å²æ•°æ®
    product_df = df[df['å•†å“åç§°'] == product_name].copy()
    
    # è®¡ç®—å†å²æœŸå’Œè¿‘æœŸçš„é”€é‡å˜åŒ–
    mid_date = start_date + (max_date - start_date) / 2
    history_sales = product_df[product_df['æ—¥æœŸ'] < mid_date]['è®¢å•ID'].nunique()
    recent_sales = product_df[product_df['æ—¥æœŸ'] >= mid_date]['è®¢å•ID'].nunique()
    
    if history_sales > 0:
        sales_change_rate = ((recent_sales - history_sales) / history_sales * 100)
    else:
        sales_change_rate = 0
    
    # åˆ¤æ–­1: ä¿ƒé”€å¼•æµå“ï¼ˆä½ä»·+é”€é‡æš´å¢ï¼‰
    if avg_price < 5 and sales_change_rate > 50:
        return 'ğŸ”¥ ä¿ƒé”€å¼•æµå“', f'é”€é‡æš´å¢{sales_change_rate:.0f}%ï¼Œä»·æ ¼Â¥{avg_price:.2f}', 'å»ºè®®ï¼šæ£€æŸ¥æ˜¯å¦äºæŸï¼Œè€ƒè™‘æ¶¨ä»·æˆ–é™è´­'
    
    # åˆ¤æ–­2: å”®ç½„ç¼ºè´§ï¼ˆé”€é‡æš´è·Œ+åº“å­˜ä¸º0ï¼‰
    if 'åº“å­˜' in product_df.columns or 'å‰©ä½™åº“å­˜' in product_df.columns:
        stock_col = 'åº“å­˜' if 'åº“å­˜' in product_df.columns else 'å‰©ä½™åº“å­˜'
        current_stock = product_df[stock_col].iloc[-1] if len(product_df) > 0 else 999
        if current_stock <= 0 and sales_change_rate < -60:
            return 'ğŸš« å”®ç½„ç¼ºè´§', f'åº“å­˜ä¸º0ï¼Œé”€é‡æš´è·Œ{abs(sales_change_rate):.0f}%', 'å»ºè®®ï¼šåŠæ—¶è¡¥è´§ï¼Œé¿å…ç¼ºè´§å½±å“é”€å”®'
    
    # è®¡ç®—ä»·æ ¼å˜åŒ–
    if 'å®æ”¶ä»·æ ¼' in product_df.columns:
        history_price = product_df[product_df['æ—¥æœŸ'] < mid_date]['å®æ”¶ä»·æ ¼'].mean()
        recent_price = product_df[product_df['æ—¥æœŸ'] >= mid_date]['å®æ”¶ä»·æ ¼'].mean()
        
        if history_price > 0:
            price_change_rate = ((recent_price - history_price) / history_price * 100)
        else:
            price_change_rate = 0
        
        # åˆ¤æ–­3: é™ä»·ä¿ƒé”€/ä¸´æœŸ
        if price_change_rate < -10:
            return 'ğŸ“‰ é™ä»·ä¿ƒé”€', f'ä»·æ ¼ä¸‹é™{abs(price_change_rate):.1f}%ï¼ˆÂ¥{history_price:.2f}â†’Â¥{recent_price:.2f}ï¼‰', 'å»ºè®®ï¼šä¸´æœŸæ¸…ä»“æˆ–ä¾›åº”å•†ä¿ƒé”€ï¼Œå±æ­£å¸¸æ³¢åŠ¨'
    else:
        price_change_rate = 0
    
    # åˆ¤æ–­4: æ»é”€é£é™©ï¼ˆä»·æ ¼ä¸å˜+é”€é‡æš´è·Œï¼‰
    if abs(price_change_rate) < 5 and sales_change_rate < -50:
        return 'ğŸ“¦ æ»é”€é£é™©', f'ä»·æ ¼ä¸å˜ï¼Œé”€é‡æš´è·Œ{abs(sales_change_rate):.0f}%', 'å»ºè®®ï¼šè€ƒè™‘ä¿ƒé”€æ´»åŠ¨æˆ–ä¼˜åŒ–å•†å“è¯¦æƒ…é¡µ'
    
    # åˆ¤æ–­5: ä½ä»·æ‹–ç´¯ï¼ˆé»˜è®¤å…œåº•ï¼‰
    return 'ğŸ’° ä½ä»·æ‹–ç´¯', f'ä»·æ ¼Â¥{avg_price:.2f}ä½äºæ•´ä½“å‡ä»·ï¼Œå æ¯”{order_ratio:.1f}%', 'å»ºè®®ï¼šä¼˜åŒ–å•†å“ç»„åˆï¼Œå¼•å¯¼è´­ä¹°é«˜ä»·å•†å“'


def _identify_drag_products(
    df: pd.DataFrame,
    order_agg: pd.DataFrame,
    start_date: pd.Timestamp,
    max_date: pd.Timestamp,
    price_field: str = 'å®æ”¶ä»·æ ¼'
) -> Dict:
    """
    å››å±‚å•†å“åˆ†æï¼ˆé‡æ„ç‰ˆï¼‰
    
    è¿”å›ç»“æ„:
    {
        'core_drag': [],      # ç¬¬ä¸€å±‚ï¼šæ ¸å¿ƒæ‹–ç´¯TOP10
        'abnormal': [],       # ç¬¬äºŒå±‚ï¼šå¼‚å¸¸å˜åŒ–TOP10
        'new_low': [],        # ç¬¬ä¸‰å±‚ï¼šæ–°å¢ä½ä»·TOP5
        'high_price': {       # ç¬¬å››å±‚ï¼šé«˜ä»·å¸¦æœºä¼šï¼ˆä»·æ ¼>30å…ƒï¼‰
            'star': [],       # é«˜ä»·çˆ†å“
            'stable': [],     # é«˜ä»·ç¨³å®š
            'decline': []     # é«˜ä»·æ»é”€
        },
        'summary': {}         # æ±‡æ€»ä¿¡æ¯
    }
    """
    
    result = {
        'core_drag': [],
        'abnormal': [],
        'new_low': [],
        'high_price': {'star': [], 'stable': [], 'decline': []},
        'summary': {}
    }
    
    # ç­›é€‰å‘¨æœŸå†…æ•°æ®
    period_df = df[df['æ—¥æœŸ'] >= start_date].copy()
    period_orders = order_agg[order_agg['æ—¥æœŸ'] >= start_date].copy()
    
    if len(period_df) == 0:
        print(f"    âš ï¸ å‘¨æœŸå†…æ— å•†å“æ˜ç»†æ•°æ®")
        return result
    
    # è®¡ç®—å¹³å‡å®¢å•ä»·
    avg_aov = period_orders['å®æ”¶ä»·æ ¼'].mean()
    print(f"    ğŸ“Š å¹³å‡å®¢å•ä»·: Â¥{avg_aov:.2f}")
    
    # âœ… å‰”é™¤è€—æåˆ†ç±»
    if 'ä¸€çº§åˆ†ç±»å' in period_df.columns:
        before_count = len(period_df)
        period_df = period_df[period_df['ä¸€çº§åˆ†ç±»å'] != 'è€—æ'].copy()
        after_count = len(period_df)
        if before_count > after_count:
            print(f"    âœ‚ï¸ å·²å‰”é™¤è€—æåˆ†ç±»: {before_count - after_count} æ¡è®°å½•")
    
    # åˆ†ä¸ºå†å²æœŸå’Œè¿‘æœŸ
    mid_date = start_date + (max_date - start_date) / 2
    history_df = period_df[period_df['æ—¥æœŸ'] < mid_date].copy()
    recent_df = period_df[period_df['æ—¥æœŸ'] >= mid_date].copy()
    
    # ç»Ÿè®¡æ¯ä¸ªå•†å“åœ¨å†å²æœŸå’Œè¿‘æœŸçš„æ•°æ®
    history_stats = history_df.groupby('å•†å“åç§°').agg({
        'è®¢å•ID': 'nunique',
        price_field: 'mean'
    }).reset_index()
    history_stats.columns = ['å•†å“åç§°', 'å†å²è®¢å•æ•°', 'å†å²ä»·æ ¼']
    
    recent_stats = recent_df.groupby('å•†å“åç§°').agg({
        'è®¢å•ID': 'nunique',
        price_field: 'mean'
    }).reset_index()
    recent_stats.columns = ['å•†å“åç§°', 'è¿‘æœŸè®¢å•æ•°', 'è¿‘æœŸä»·æ ¼']
    
    # åˆå¹¶æ•°æ®
    product_stats = recent_stats.merge(history_stats, on='å•†å“åç§°', how='outer').fillna(0)
    
    # è®¡ç®—æ€»è®¢å•æ•°å’Œå æ¯”
    total_orders = len(period_orders)
    product_stats['è®¢å•æ€»æ•°'] = product_stats['å†å²è®¢å•æ•°'] + product_stats['è¿‘æœŸè®¢å•æ•°']
    product_stats['è®¢å•å æ¯”'] = (product_stats['è¿‘æœŸè®¢å•æ•°'] / total_orders * 100).round(1)
    
    # è®¡ç®—å˜åŒ–ç‡
    product_stats['é”€é‡å˜åŒ–ç‡'] = product_stats.apply(
        lambda r: ((r['è¿‘æœŸè®¢å•æ•°'] - r['å†å²è®¢å•æ•°']) / r['å†å²è®¢å•æ•°'] * 100) if r['å†å²è®¢å•æ•°'] > 0 else 0,
        axis=1
    ).round(1)
    
    product_stats['ä»·æ ¼å˜åŒ–ç‡'] = product_stats.apply(
        lambda r: ((r['è¿‘æœŸä»·æ ¼'] - r['å†å²ä»·æ ¼']) / r['å†å²ä»·æ ¼'] * 100) if r['å†å²ä»·æ ¼'] > 0 else 0,
        axis=1
    ).round(1)
    
    # ä½¿ç”¨è¿‘æœŸä»·æ ¼ä½œä¸ºå•†å“ä»·æ ¼
    product_stats['å¹³å‡ä»·æ ¼'] = product_stats['è¿‘æœŸä»·æ ¼']
    
    # è®¡ç®—æ‹‰ä½é‡‘é¢
    product_stats['æ‹‰ä½é‡‘é¢'] = (
        (avg_aov - product_stats['å¹³å‡ä»·æ ¼']) * 
        product_stats['è¿‘æœŸè®¢å•æ•°']
    ).round(2)
    
    print(f"    ğŸ“¦ åˆ†æå•†å“æ•°: {len(product_stats)}")
    
    # ============ ç¬¬ä¸€å±‚ï¼šæ ¸å¿ƒæ‹–ç´¯TOP10 ============
    core_drag_df = product_stats[
        (product_stats['å¹³å‡ä»·æ ¼'] < avg_aov * 0.85) &
        (product_stats['æ‹‰ä½é‡‘é¢'] > 0)
    ].sort_values('æ‹‰ä½é‡‘é¢', ascending=False).head(10)
    
    for _, row in core_drag_df.iterrows():
        label, reason, suggestion = _diagnose_product_issue(
            row['å•†å“åç§°'], row['å¹³å‡ä»·æ ¼'], int(row['è¿‘æœŸè®¢å•æ•°']),
            row['è®¢å•å æ¯”'], df, start_date, max_date
        )
        result['core_drag'].append({
            'product': row['å•†å“åç§°'],
            'avg_price': round(row['å¹³å‡ä»·æ ¼'], 2),
            'order_count': int(row['è¿‘æœŸè®¢å•æ•°']),
            'order_ratio': row['è®¢å•å æ¯”'],
            'drag_amount': row['æ‹‰ä½é‡‘é¢'],
            'sales_change': row['é”€é‡å˜åŒ–ç‡'],
            'diagnosis_label': label,
            'diagnosis_reason': reason,
            'suggestion': suggestion
        })
    
    print(f"    ğŸ”´ æ ¸å¿ƒæ‹–ç´¯: {len(result['core_drag'])} ä¸ª")
    
    # ============ ç¬¬äºŒå±‚ï¼šå¼‚å¸¸å˜åŒ–TOP10 ============
    abnormal_df = product_stats[
        (product_stats['å†å²è®¢å•æ•°'] >= 5) &
        ((product_stats['é”€é‡å˜åŒ–ç‡'] > 100) | (product_stats['é”€é‡å˜åŒ–ç‡'] < -30)) &
        (product_stats['å¹³å‡ä»·æ ¼'] < avg_aov)
    ].copy()
    abnormal_df['å˜åŒ–å¹…åº¦'] = abs(abnormal_df['é”€é‡å˜åŒ–ç‡'])
    abnormal_df = abnormal_df.sort_values('å˜åŒ–å¹…åº¦', ascending=False).head(10)
    
    for _, row in abnormal_df.iterrows():
        result['abnormal'].append({
            'product': row['å•†å“åç§°'],
            'avg_price': round(row['å¹³å‡ä»·æ ¼'], 2),
            'history_orders': int(row['å†å²è®¢å•æ•°']),
            'recent_orders': int(row['è¿‘æœŸè®¢å•æ•°']),
            'sales_change': row['é”€é‡å˜åŒ–ç‡'],
            'price_change': row['ä»·æ ¼å˜åŒ–ç‡']
        })
    
    print(f"    ğŸŸ¡ å¼‚å¸¸å˜åŒ–: {len(result['abnormal'])} ä¸ª")
    
    # ============ ç¬¬ä¸‰å±‚ï¼šæ–°å¢ä½ä»·TOP5 ============
    new_low_df = product_stats[
        (product_stats['å†å²è®¢å•æ•°'] == 0) &
        (product_stats['è¿‘æœŸè®¢å•æ•°'] >= 3) &
        (product_stats['å¹³å‡ä»·æ ¼'] < avg_aov * 0.7)
    ].sort_values('è¿‘æœŸè®¢å•æ•°', ascending=False).head(5)
    
    for _, row in new_low_df.iterrows():
        result['new_low'].append({
            'product': row['å•†å“åç§°'],
            'avg_price': round(row['å¹³å‡ä»·æ ¼'], 2),
            'order_count': int(row['è¿‘æœŸè®¢å•æ•°']),
            'order_ratio': row['è®¢å•å æ¯”']
        })
    
    print(f"    ğŸ†• æ–°å¢ä½ä»·: {len(result['new_low'])} ä¸ª")
    
    # ============ ç¬¬å››å±‚ï¼šé«˜ä»·å¸¦æœºä¼šï¼ˆä»·æ ¼>30å…ƒï¼‰============
    HIGH_PRICE_THRESHOLD = 30  # ç”¨æˆ·æŒ‡å®šï¼šå•ä»·30å…ƒä»¥ä¸Š
    
    high_price_df = product_stats[
        (product_stats['å¹³å‡ä»·æ ¼'] > HIGH_PRICE_THRESHOLD) &
        (product_stats['è¿‘æœŸè®¢å•æ•°'] >= 3)
    ].copy()
    
    # è®¡ç®—æ‹‰å‡æ½œåŠ› = (å•†å“ä»·æ ¼ - å¹³å‡å®¢å•ä»·) Ã— è¿‘æœŸè®¢å•æ•°
    high_price_df['æ‹‰å‡æ½œåŠ›'] = (
        (high_price_df['å¹³å‡ä»·æ ¼'] - avg_aov) * 
        high_price_df['è¿‘æœŸè®¢å•æ•°']
    ).round(2)
    
    # åˆ†ç±»
    star_df = high_price_df[high_price_df['é”€é‡å˜åŒ–ç‡'] > 50].sort_values('æ‹‰å‡æ½œåŠ›', ascending=False).head(5)
    stable_df = high_price_df[
        (high_price_df['é”€é‡å˜åŒ–ç‡'] >= -20) & 
        (high_price_df['é”€é‡å˜åŒ–ç‡'] <= 50)
    ].sort_values('æ‹‰å‡æ½œåŠ›', ascending=False).head(8)
    decline_df = high_price_df[high_price_df['é”€é‡å˜åŒ–ç‡'] < -20].sort_values('é”€é‡å˜åŒ–ç‡', ascending=True).head(3)
    
    for _, row in star_df.iterrows():
        result['high_price']['star'].append({
            'product': row['å•†å“åç§°'],
            'avg_price': round(row['å¹³å‡ä»·æ ¼'], 2),
            'history_orders': int(row['å†å²è®¢å•æ•°']),
            'recent_orders': int(row['è¿‘æœŸè®¢å•æ•°']),
            'sales_change': row['é”€é‡å˜åŒ–ç‡'],
            'lift_potential': row['æ‹‰å‡æ½œåŠ›']
        })
    
    for _, row in stable_df.iterrows():
        result['high_price']['stable'].append({
            'product': row['å•†å“åç§°'],
            'avg_price': round(row['å¹³å‡ä»·æ ¼'], 2),
            'recent_orders': int(row['è¿‘æœŸè®¢å•æ•°']),
            'sales_change': row['é”€é‡å˜åŒ–ç‡'],
            'lift_potential': row['æ‹‰å‡æ½œåŠ›']
        })
    
    for _, row in decline_df.iterrows():
        result['high_price']['decline'].append({
            'product': row['å•†å“åç§°'],
            'avg_price': round(row['å¹³å‡ä»·æ ¼'], 2),
            'history_orders': int(row['å†å²è®¢å•æ•°']),
            'recent_orders': int(row['è¿‘æœŸè®¢å•æ•°']),
            'sales_change': row['é”€é‡å˜åŒ–ç‡']
        })
    
    print(f"    ğŸš€ é«˜ä»·å¸¦: çˆ†å“{len(result['high_price']['star'])} ç¨³å®š{len(result['high_price']['stable'])} æ»é”€{len(result['high_price']['decline'])}")
    
    # æ±‡æ€»ä¿¡æ¯
    result['summary'] = {
        'avg_aov': round(avg_aov, 2),
        'high_price_threshold': HIGH_PRICE_THRESHOLD,
        'total_products': len(product_stats),
        'core_drag_count': len(result['core_drag']),
        'abnormal_count': len(result['abnormal']),
        'new_low_count': len(result['new_low']),
        'high_price_star_count': len(result['high_price']['star']),
        'high_price_stable_count': len(result['high_price']['stable']),
        'high_price_decline_count': len(result['high_price']['decline'])
    }
    
    return result


def _identify_opportunity_products(
    df: pd.DataFrame,
    order_agg: pd.DataFrame,
    start_date: pd.Timestamp,
    max_date: pd.Timestamp,
    price_field: str = 'å®æ”¶ä»·æ ¼'
) -> List[Dict]:
    """è¯†åˆ«TOP5æœºä¼šå•†å“(é«˜ä»·å€¼ä½†é”€é‡ä¸‹é™)
    
    Args:
        price_field: ä»·æ ¼å­—æ®µåï¼Œé»˜è®¤'å®æ”¶ä»·æ ¼'ï¼ˆä¼˜å…ˆï¼‰ï¼Œå¤‡é€‰'å•†å“å®å”®ä»·'
    """
    
    # åˆ†ä¸ºä¸¤ä¸ªå‘¨æœŸ
    mid_date = start_date + (max_date - start_date) / 2
    
    period1_df = df[(df['æ—¥æœŸ'] >= start_date) & (df['æ—¥æœŸ'] < mid_date)].copy()
    period2_df = df[df['æ—¥æœŸ'] >= mid_date].copy()
    
    if len(period1_df) == 0 or len(period2_df) == 0:
        return []
    
    # è®¡ç®—æ¯ä¸ªå•†å“åœ¨ä¸¤ä¸ªå‘¨æœŸçš„é”€é‡
    sales1 = period1_df.groupby('å•†å“åç§°')['è®¢å•ID'].nunique()
    sales2 = period2_df.groupby('å•†å“åç§°')['è®¢å•ID'].nunique()
    
    # è®¡ç®—å¹³å‡ä»·æ ¼ï¼ˆä½¿ç”¨åŠ¨æ€ä»·æ ¼å­—æ®µï¼‰
    avg_prices = df[df['æ—¥æœŸ'] >= start_date].groupby('å•†å“åç§°')[price_field].mean()
    
    # åˆå¹¶
    comparison = pd.DataFrame({
        'å‰æœŸé”€é‡': sales1,
        'åæœŸé”€é‡': sales2,
        'å¹³å‡ä»·æ ¼': avg_prices
    }).fillna(0)
    
    # è®¡ç®—é”€é‡å˜åŒ–ç‡
    comparison['é”€é‡å˜åŒ–ç‡'] = (
        (comparison['åæœŸé”€é‡'] - comparison['å‰æœŸé”€é‡']) / 
        comparison['å‰æœŸé”€é‡'].replace(0, np.nan) * 100
    ).fillna(0).round(1)
    
    # ç­›é€‰æœºä¼šå•†å“(ä»·æ ¼>40å…ƒ ä¸” é”€é‡ä¸‹é™>10%)
    opportunity_df = comparison[
        (comparison['å¹³å‡ä»·æ ¼'] > 40) & 
        (comparison['é”€é‡å˜åŒ–ç‡'] < -10) &
        (comparison['å‰æœŸé”€é‡'] >= 3)  # å‰æœŸè‡³å°‘æœ‰3å•
    ].copy()
    
    # æŒ‰é”€é‡å˜åŒ–ç‡æ’åº(é™å¹…æœ€å¤§çš„)
    opportunity_df = opportunity_df.sort_values('é”€é‡å˜åŒ–ç‡').head(5)
    
    results = []
    for product, row in opportunity_df.iterrows():
        results.append({
            'product': product,
            'avg_price': round(row['å¹³å‡ä»·æ ¼'], 2),
            'sales_change': row['é”€é‡å˜åŒ–ç‡'],
            'prev_sales': int(row['å‰æœŸé”€é‡']),
            'current_sales': int(row['åæœŸé”€é‡'])
        })
    
    return results



def _empty_distribution_result() -> Dict:
    """è¿”å›ç©ºçš„è®¢å•åˆ†å¸ƒåˆ†æç»“æœ"""
    return {
        'severe': [],
        'moderate': [],
        'mild': [],
        'trend': {
            'dates': [],
            'severe_count': [],
            'moderate_count': [],
            'mild_count': [],
            'total_count': [],
            'distribution': []
        },
        'summary': {
            'total_downgrade': 0,
            'total_changes': 0,
            'severe_count': 0,
            'moderate_count': 0,
            'mild_count': 0,
            'avg_aov': 0,
            'history_avg_aov': 0,
            'aov_change_amount': 0,
            'aov_change_rate': 0,
            'avg_decline': 0,
            'max_decline': 0,
            'period_days': 30,
            'distribution': [],
            'suggestions': []
        }
    }


def _empty_customer_result() -> Dict:
    """è¿”å›ç©ºçš„å®¢æˆ·åˆ†æç»“æœ"""
    return {
        'severe': [],
        'moderate': [],
        'mild': [],
        'trend': {
            'dates': [],
            'severe_count': [],
            'moderate_count': [],
            'mild_count': [],
            'total_count': []
        },
        'summary': {
            'total_downgrade': 0,
            'total_changes': 0,
            'severe_count': 0,
            'moderate_count': 0,
            'mild_count': 0,
            'avg_aov': 0,
            'aov_change_amount': 0,
            'history_avg_aov': 0,
            'avg_decline': 0,
            'max_decline': 0,
            'period_days': 30
        }
    }


def _empty_product_result() -> Dict:
    """è¿”å›ç©ºçš„å•†å“åˆ†æç»“æœ"""
    return {
        'low_price_trend': {
            'dates': [],
            'ratios': [],
            'threshold': 25,
            'current_ratio': 0,
            'avg_ratio': 0,
            'peak_date': None,
            'peak_ratio': 0
        },
        'structure_change': {
            'dates': [],
            'low': [],
            'mid': [],
            'high': [],
            'current': {'low': 0, 'mid': 0, 'high': 0},
            'change': {'low': 0, 'mid': 0, 'high': 0}
        },
        'drag_products': [],
        'opportunity_products': [],
        'summary': {
            'period_days': 30,
            'total_orders': 0,
            'avg_aov': 0,
            'low_price_ratio': 0,
            'drag_product_count': 0
        }
    }
