#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åœºæ™¯è¥é”€æ™ºèƒ½å†³ç­–å¼•æ“
================================================================================
é›†æˆå¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä¸ºFMCGé›¶å”®åœºæ™¯è¥é”€æä¾›æ™ºèƒ½å†³ç­–æ”¯æŒ

æ ¸å¿ƒæ¨¡å—ï¼š
1. FP-Growthå•†å“ç»„åˆæŒ–æ˜ - å‘ç°"è¿½å‰§å¥—é¤"ã€"æç¥å¥—é¤"ç­‰å…³è”è§„åˆ™
2. XGBooståœºæ™¯è¯†åˆ«æ¨¡å‹ - é¢„æµ‹ç”¨æˆ·è´­ä¹°åœºæ™¯ï¼ˆä¸Šåˆæç¥/ä¸‹åˆèŒ¶æ­‡/æ™šé—´æ”¾æ¾/æ·±å¤œåº”æ€¥ï¼‰
3. RFMå®¢æˆ·åˆ†ç¾¤æ¨¡å‹ - è¯†åˆ«é«˜é¢‘åº”æ€¥ã€è®¡åˆ’å›¤è´§ã€ä»·æ ¼æ•æ„Ÿã€å¶å‘å°é²œç”¨æˆ·
4. å†³ç­–æ ‘è§„åˆ™ç”Ÿæˆ - å¯è§£é‡Šçš„åœºæ™¯è¯†åˆ«è§„åˆ™
5. ååŒè¿‡æ»¤æ¨è - åŸºäºç”¨æˆ·è¡Œä¸ºçš„å•†å“æ¨è

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-10-14
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ åº“
from sklearn.cluster import KMeans
from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, silhouette_score

# XGBoost
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError as e:
    XGBOOST_AVAILABLE = False
    print(f"âš ï¸ XGBoostæœªå®‰è£…ï¼Œå°†ä½¿ç”¨RandomForestæ›¿ä»£ (é”™è¯¯: {e})")

# å…³è”è§„åˆ™æŒ–æ˜
try:
    from mlxtend.frequent_patterns import fpgrowth, association_rules
    from mlxtend.preprocessing import TransactionEncoder
    MLXTEND_AVAILABLE = True
except ImportError as e:
    MLXTEND_AVAILABLE = False
    print(f"âš ï¸ mlxtendæœªå®‰è£…ï¼ŒFP-GrowthåŠŸèƒ½å°†ä¸å¯ç”¨ (é”™è¯¯: {e})")

# å¯è§†åŒ–
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# ============================================================================
# 1. FP-Growthå•†å“ç»„åˆæŒ–æ˜å¼•æ“
# ============================================================================

class ProductCombinationMiner:
    """
    å•†å“ç»„åˆæŒ–æ˜å¼•æ“ - åŸºäºFP-Growthç®—æ³•
    
    åŠŸèƒ½ï¼š
    - æŒ–æ˜é¢‘ç¹è´­ä¹°çš„å•†å“ç»„åˆ
    - ç”Ÿæˆåœºæ™¯åŒ–å¥—é¤å»ºè®®ï¼ˆè¿½å‰§å¥—é¤ã€æç¥å¥—é¤ã€åº”æ€¥å¥—é¤ç­‰ï¼‰
    - è®¡ç®—å…³è”è§„åˆ™çš„æ”¯æŒåº¦ã€ç½®ä¿¡åº¦ã€æå‡åº¦
    """
    
    def __init__(self, min_support: float = 0.01, min_confidence: float = 0.3):
        """
        åˆå§‹åŒ–å‚æ•°
        
        Args:
            min_support: æœ€å°æ”¯æŒåº¦é˜ˆå€¼ï¼ˆé»˜è®¤1%ï¼‰
            min_confidence: æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤30%ï¼‰
        """
        self.min_support = min_support
        self.min_confidence = min_confidence
        self.frequent_itemsets = None
        self.rules = None
        self.scene_packages = {}
        
    def mine_from_orders(self, order_data: pd.DataFrame) -> Dict[str, Any]:
        """
        ä»è®¢å•æ•°æ®ä¸­æŒ–æ˜å•†å“ç»„åˆ
        
        Args:
            order_data: è®¢å•æ˜ç»†æ•°æ®ï¼Œå¿…é¡»åŒ…å«'è®¢å•ID'å’Œ'å•†å“åç§°'åˆ—
            
        Returns:
            åŒ…å«é¢‘ç¹é¡¹é›†å’Œå…³è”è§„åˆ™çš„å­—å…¸
        """
        if not MLXTEND_AVAILABLE:
            return {
                'status': 'error',
                'message': 'è¯·å®‰è£…mlxtendåº“: pip install mlxtend'
            }
        
        try:
            # 1. æ„å»ºè´­ç‰©ç¯®
            print("ğŸ“¦ æ„å»ºè´­ç‰©ç¯®...")
            baskets = order_data.groupby('è®¢å•ID')['å•†å“åç§°'].apply(list).values.tolist()
            
            # è¿‡æ»¤å•å“è®¢å•ï¼ˆè‡³å°‘2ä¸ªå•†å“æ‰æœ‰ç»„åˆæ„ä¹‰ï¼‰
            baskets = [basket for basket in baskets if len(basket) >= 2]
            
            if len(baskets) < 10:
                return {
                    'status': 'error',
                    'message': f'æœ‰æ•ˆè®¢å•æ•°é‡ä¸è¶³ï¼ˆ{len(baskets)}ï¼‰ï¼Œè‡³å°‘éœ€è¦10ä¸ªå¤šå•†å“è®¢å•'
                }
            
            # 2. ç¼–ç ä¸ºäº‹åŠ¡çŸ©é˜µ
            print("ğŸ”„ ç¼–ç äº‹åŠ¡çŸ©é˜µ...")
            te = TransactionEncoder()
            te_ary = te.fit(baskets).transform(baskets)
            df_encoded = pd.DataFrame(te_ary, columns=te.columns_)
            
            # 3. æŒ–æ˜é¢‘ç¹é¡¹é›†
            print("â›ï¸ æŒ–æ˜é¢‘ç¹é¡¹é›†...")
            self.frequent_itemsets = fpgrowth(
                df_encoded, 
                min_support=self.min_support, 
                use_colnames=True,
                max_len=None  # ç§»é™¤é•¿åº¦é™åˆ¶ï¼Œè®©ç®—æ³•è‡ªåŠ¨å¤„ç†
            )
            
            if self.frequent_itemsets.empty:
                return {
                    'status': 'warning',
                    'message': f'æœªæ‰¾åˆ°æ»¡è¶³æ”¯æŒåº¦{self.min_support}çš„é¢‘ç¹é¡¹é›†ï¼Œå»ºè®®é™ä½é˜ˆå€¼'
                }
            
            # åªä¿ç•™2ä¸ªåŠä»¥ä¸Šå•†å“çš„ç»„åˆ
            self.frequent_itemsets = self.frequent_itemsets[
                self.frequent_itemsets['itemsets'].apply(lambda x: len(x) >= 2)
            ]
            
            # 4. ç”Ÿæˆå…³è”è§„åˆ™
            print("ğŸ“‹ ç”Ÿæˆå…³è”è§„åˆ™...")
            if len(self.frequent_itemsets) > 0:
                try:
                    self.rules = association_rules(
                        self.frequent_itemsets, 
                        metric="confidence", 
                        min_threshold=self.min_confidence,
                        support_only=False  # ç¡®ä¿ç”Ÿæˆå®Œæ•´çš„è§„åˆ™ä¿¡æ¯
                    )
                except (ValueError, KeyError) as e:
                    print(f"âš ï¸ å…³è”è§„åˆ™ç”Ÿæˆé‡åˆ°é—®é¢˜: {str(e)}")
                    print("   å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨support_only=Trueï¼Œç„¶åæ‰‹åŠ¨è®¡ç®—
                    try:
                        # å…ˆåªè·å–æ”¯æŒåº¦ä¿¡æ¯
                        self.rules = association_rules(
                            self.frequent_itemsets,
                            metric="support",
                            min_threshold=self.min_support
                        )
                    except Exception as e2:
                        print(f"   å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {str(e2)}")
                        self.rules = pd.DataFrame()
                
                # è®¡ç®—æå‡åº¦
                if not self.rules.empty:
                    self.rules['lift'] = self.rules.get('lift', 1.0).round(2)
                    self.rules['confidence'] = self.rules.get('confidence', 0.0).round(3)
                    self.rules['support'] = self.rules.get('support', 0.0).round(4)
            else:
                self.rules = pd.DataFrame()
            
            # 5. è¯†åˆ«åœºæ™¯åŒ–å¥—é¤
            self._identify_scene_packages(order_data)
            
            print(f"âœ… æŒ–æ˜å®Œæˆï¼š{len(self.frequent_itemsets)}ä¸ªé¢‘ç¹é¡¹é›†ï¼Œ{len(self.rules)}æ¡å…³è”è§„åˆ™")
            
            return {
                'status': 'success',
                'frequent_itemsets': self.frequent_itemsets,
                'rules': self.rules,
                'scene_packages': self.scene_packages,
                'stats': {
                    'total_baskets': len(baskets),
                    'frequent_itemsets_count': len(self.frequent_itemsets),
                    'rules_count': len(self.rules)
                }
            }
            
        except Exception as e:
            return {
                'status': 'error',
                'message': f'æŒ–æ˜è¿‡ç¨‹å‡ºé”™: {str(e)}'
            }
    
    def _identify_scene_packages(self, order_data: pd.DataFrame):
        """
        è¯†åˆ«åœºæ™¯åŒ–å¥—é¤
        
        åŸºäºæ—¶æ®µå’Œå•†å“åˆ†ç±»ç‰¹å¾ï¼Œå°†é¢‘ç¹é¡¹é›†æ˜ å°„åˆ°åœºæ™¯
        """
        if self.frequent_itemsets is None or self.frequent_itemsets.empty:
            return
        
        # åœºæ™¯å…³é”®è¯æ˜ å°„ï¼ˆåŸºäºO2Oå¤–å–ä¸šåŠ¡åœºæ™¯ï¼‰
        scene_keywords = {
            # æ—©é¤åˆšéœ€ï¼ˆ6-8ç‚¹ï¼‰
            'æ—©é¤å¥—é¤': ['é¢åŒ…', 'ç‰›å¥¶', 'é¸¡è›‹', 'è±†æµ†', 'æ²¹æ¡', 'åŒ…å­', 'ç²¥', 'æ—©é¤'],
            
            # æ—¥å¸¸è¡¥ç»™ï¼ˆ9-17ç‚¹ï¼‰
            'æ—¥ç”¨è¡¥ç»™å¥—é¤': ['çº¸å·¾', 'æ´—æ´ç²¾', 'åƒåœ¾è¢‹', 'ç‰™è†', 'æ´—å‘æ°´', 'å«ç”Ÿçº¸', 'ç”µæ± '],
            'åŠå…¬æç¥å¥—é¤': ['å’–å•¡', 'çº¢ç‰›', 'åšæœ', 'å·§å…‹åŠ›', 'èƒ½é‡', 'åŠŸèƒ½é¥®æ–™', 'èŒ¶'],
            'äº²å­å¥—é¤': ['ç‰›å¥¶', 'æœå†»', 'ç³–æœ', 'é¥¼å¹²', 'å„¿ç«¥', 'ä¹³é…¸èŒ', 'é…¸å¥¶'],
            
            # ä¼‘é—²å¨±ä¹ï¼ˆ14-17ã€21-23ç‚¹ï¼‰
            'è¿½å‰§å¥—é¤': ['è–¯ç‰‡', 'å¯ä¹', 'ç“œå­', 'çˆ†ç±³èŠ±', 'é¥®æ–™', 'è†¨åŒ–', 'ç¢³é…¸'],
            'ä¸‹åˆèŒ¶å¥—é¤': ['è›‹ç³•', 'é¥¼å¹²', 'å¥¶èŒ¶', 'å’–å•¡', 'ç”œå“', 'ç‚¹å¿ƒ'],
            
            # æ­£é¤é«˜å³°ï¼ˆ12-13ã€18-20ç‚¹ï¼‰
            'èšé¤å¥—é¤': ['å•¤é…’', 'ç™½é…’', 'èŠ±ç”Ÿ', 'ç“œå­', 'å¤å‘³', 'é¸­è„–', 'é¸­ç¿…', 'é…’æ°´'],
            
            # æ·±å¤œåº”æ€¥ï¼ˆ0-5ç‚¹ï¼‰
            'åº”æ€¥å¥—é¤': ['çº¸å·¾', 'ç”µæ± ', 'åˆ›å¯è´´', 'æ„Ÿå†’è¯', 'é€€çƒ§è´´', 'è¯å“'],
            'å¤œå®µå¥—é¤': ['æ–¹ä¾¿é¢', 'ç«è…¿è‚ ', 'å•¤é…’', 'å¤å‘³', 'çƒ§çƒ¤', 'å°é¾™è™¾'],
            'ç†¬å¤œå¥—é¤': ['çº¢ç‰›', 'å’–å•¡', 'èƒ½é‡é¥®æ–™', 'åšæœ', 'å·§å…‹åŠ›', 'è–¯ç‰‡']
        }
        
        for scene_name, keywords in scene_keywords.items():
            matched_itemsets = []
            
            for idx, row in self.frequent_itemsets.iterrows():
                itemset = row['itemsets']
                # ç¡®ä¿itemsetæ˜¯å¯è¿­ä»£çš„ï¼ˆå¤„ç†frozensetï¼‰
                itemset_list = list(itemset) if not isinstance(itemset, list) else itemset
                itemset_str = ' '.join(itemset_list)
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«åœºæ™¯å…³é”®è¯
                match_count = sum(1 for keyword in keywords if keyword in itemset_str)
                if match_count >= 2:  # è‡³å°‘åŒ¹é…2ä¸ªå…³é”®è¯
                    matched_itemsets.append({
                        'items': itemset_list,
                        'support': row['support'],
                        'match_score': match_count
                    })
            
            if matched_itemsets:
                # æŒ‰åŒ¹é…åº¦å’Œæ”¯æŒåº¦æ’åº
                matched_itemsets.sort(
                    key=lambda x: (x['match_score'], x['support']), 
                    reverse=True
                )
                self.scene_packages[scene_name] = matched_itemsets[:5]  # ä¿ç•™TOP5
    
    def get_top_combinations(self, top_n: int = 10) -> pd.DataFrame:
        """è·å–TOP Nå•†å“ç»„åˆ"""
        if self.frequent_itemsets is None or self.frequent_itemsets.empty:
            return pd.DataFrame()
        
        top_items = self.frequent_itemsets.nlargest(top_n, 'support').copy()
        top_items['items_str'] = top_items['itemsets'].apply(
            lambda x: ' + '.join(sorted(list(x)))
        )
        return top_items[['items_str', 'support']]
    
    def get_top_rules(self, top_n: int = 10, sort_by: str = 'lift') -> pd.DataFrame:
        """è·å–TOP Nå…³è”è§„åˆ™"""
        if self.rules is None or self.rules.empty:
            return pd.DataFrame()
        
        top_rules = self.rules.nlargest(top_n, sort_by).copy()
        top_rules['rule'] = top_rules.apply(
            lambda row: f"{', '.join(list(row['antecedents']))} â†’ {', '.join(list(row['consequents']))}", 
            axis=1
        )
        return top_rules[['rule', 'support', 'confidence', 'lift']]
    
    def visualize_rules_network(self, top_n: int = 20) -> go.Figure:
        """
        å¯è§†åŒ–å…³è”è§„åˆ™ç½‘ç»œå›¾
        
        Args:
            top_n: æ˜¾ç¤ºTOP Nè§„åˆ™
            
        Returns:
            Plotlyå›¾è¡¨å¯¹è±¡
        """
        if self.rules is None or self.rules.empty:
            fig = go.Figure()
            fig.add_annotation(
                text="æš‚æ— å…³è”è§„åˆ™æ•°æ®",
                xref="paper", yref="paper",
                x=0.5, y=0.5, showarrow=False,
                font=dict(size=16)
            )
            return fig
        
        # é€‰æ‹©TOPè§„åˆ™
        top_rules = self.rules.nlargest(top_n, 'lift')
        
        # æ„å»ºèŠ‚ç‚¹å’Œè¾¹
        nodes = set()
        edges = []
        
        for _, rule in top_rules.iterrows():
            # è½¬æ¢frozensetä¸ºlist
            antecedents = list(rule['antecedents'])
            consequents = list(rule['consequents'])
            
            for item in antecedents:
                nodes.add(item)
            for item in consequents:
                nodes.add(item)
            
            # åˆ›å»ºè¾¹ï¼ˆä½¿ç”¨å·²è½¬æ¢çš„listï¼‰
            for ant in antecedents:
                for cons in consequents:
                    edges.append({
                        'source': ant,
                        'target': cons,
                        'confidence': rule['confidence'],
                        'lift': rule['lift']
                    })
        
        # åˆ›å»ºç®€åŒ–çš„ç½‘ç»œå¯è§†åŒ–ï¼ˆä½¿ç”¨æ•£ç‚¹å›¾æ¨¡æ‹Ÿï¼‰
        fig = go.Figure()
        
        # æ·»åŠ èŠ‚ç‚¹
        node_list = list(nodes)
        n = len(node_list)
        angles = np.linspace(0, 2*np.pi, n, endpoint=False)
        x_nodes = np.cos(angles)
        y_nodes = np.sin(angles)
        
        fig.add_trace(go.Scatter(
            x=x_nodes, y=y_nodes,
            mode='markers+text',
            marker=dict(size=20, color='lightblue', line=dict(width=2)),
            text=node_list,
            textposition='top center',
            hoverinfo='text',
            name='å•†å“'
        ))
        
        # æ·»åŠ è¾¹ï¼ˆå‰10æ¡ï¼‰
        for edge in edges[:10]:
            src_idx = node_list.index(edge['source'])
            tgt_idx = node_list.index(edge['target'])
            
            fig.add_trace(go.Scatter(
                x=[x_nodes[src_idx], x_nodes[tgt_idx]],
                y=[y_nodes[src_idx], y_nodes[tgt_idx]],
                mode='lines',
                line=dict(
                    width=edge['confidence']*3,
                    color=f"rgba(100,100,255,{edge['confidence']})"
                ),
                hoverinfo='text',
                hovertext=f"ç½®ä¿¡åº¦: {edge['confidence']:.2f}<br>æå‡åº¦: {edge['lift']:.2f}",
                showlegend=False
            ))
        
        fig.update_layout(
            title="å•†å“å…³è”è§„åˆ™ç½‘ç»œå›¾",
            showlegend=False,
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            height=600,
            hovermode='closest'
        )
        
        return fig


# ============================================================================
# 2. XGBooståœºæ™¯è¯†åˆ«æ¨¡å‹
# ============================================================================

class SceneRecognitionModel:
    """
    åœºæ™¯è¯†åˆ«æ¨¡å‹ - åŸºäºXGBoost/RandomForest
    
    åŠŸèƒ½ï¼š
    - é¢„æµ‹ç”¨æˆ·è´­ä¹°åœºæ™¯ï¼ˆä¸Šåˆæç¥/ä¸‹åˆèŒ¶æ­‡/æ™šé—´æ”¾æ¾/æ·±å¤œåº”æ€¥ï¼‰
    - ç‰¹å¾å·¥ç¨‹ï¼šæ—¶æ®µã€è·ç¦»ã€å“ç±»ã€é…é€è´¹ã€è®¢å•ç»“æ„
    - è¾“å‡ºåœºæ™¯æ¦‚ç‡åˆ†å¸ƒ
    """
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.feature_importance = None
        self.is_trained = False
        
    def prepare_features(self, order_data: pd.DataFrame) -> pd.DataFrame:
        """
        ç‰¹å¾å·¥ç¨‹
        
        ä»è®¢å•æ•°æ®ä¸­æå–åœºæ™¯è¯†åˆ«ç‰¹å¾
        """
        df = order_data.copy()
        
        # 1. æ—¶æ®µç‰¹å¾
        if 'æ—¥æœŸ_datetime' in df.columns:
            df['hour'] = pd.to_datetime(df['æ—¥æœŸ_datetime']).dt.hour
        elif 'å°æ—¶' in df.columns:
            df['hour'] = df['å°æ—¶']
        else:
            df['hour'] = 12  # é»˜è®¤å€¼
        
        # æ—¶æ®µç¼–ç ï¼ˆä¼˜åŒ–ä¸º8æ—¶æ®µï¼‰
        df['time_slot'] = pd.cut(
            df['hour'], 
            bins=[0, 3, 6, 9, 12, 14, 18, 21, 24],
            labels=['å‡Œæ™¨(3-5)', 'æ¸…æ™¨(6-8)', 'ä¸Šåˆ(9-11)', 'æ­£åˆ(12-13)', 
                    'ä¸‹åˆ(14-17)', 'å‚æ™š(18-20)', 'æ™šé—´(21-23)', 'æ·±å¤œ(0-2)'],
            include_lowest=True
        )
        df['time_slot_code'] = df['time_slot'].cat.codes
        
        # 2. æ˜ŸæœŸç‰¹å¾ï¼ˆå¢å¼ºï¼‰
        if 'æ—¥æœŸ_datetime' in df.columns:
            df['weekday'] = pd.to_datetime(df['æ—¥æœŸ_datetime']).dt.dayofweek
            df['is_weekend'] = (df['weekday'] >= 5).astype(int)
            df['is_friday'] = (df['weekday'] == 4).astype(int)  # å‘¨äº”ç‰¹æ®Šå¤„ç†
        else:
            df['weekday'] = 3
            df['is_weekend'] = 0
            df['is_friday'] = 0
        
        # 3. é…é€è·ç¦»ç‰¹å¾
        if 'é…é€è·ç¦»' in df.columns:
            df['distance'] = df['é…é€è·ç¦»']
            df['distance_bin'] = pd.cut(
                df['distance'],
                bins=[0, 1, 3, 5, 100],
                labels=['è¿‘è·ç¦»', 'ä¸­è·ç¦»', 'è¿œè·ç¦»', 'è¶…è¿œ'],
                include_lowest=True
            )
            df['distance_code'] = df['distance_bin'].cat.codes
        else:
            df['distance'] = 2.0
            df['distance_code'] = 1
        
        # 4. å•†å“ç‰¹å¾
        if 'ä¸‰çº§åˆ†ç±»å' in df.columns:
            # åˆ†ç±»ç¼–ç 
            category_map = {
                'ä¼‘é—²é£Ÿå“': 1, 'é›¶é£Ÿ': 1, 'è†¨åŒ–é£Ÿå“': 1,
                'é¥®æ–™': 2, 'é…’æ°´': 2, 'ç¢³é…¸é¥®æ–™': 2,
                'æ—¥ç”¨ç™¾è´§': 3, 'ç”Ÿæ´»ç”¨å“': 3,
                'ä¹³åˆ¶å“': 4, 'å¥¶åˆ¶å“': 4
            }
            df['category_type'] = df['ä¸‰çº§åˆ†ç±»å'].apply(
                lambda x: next((v for k, v in category_map.items() if k in str(x)), 0)
            )
        else:
            df['category_type'] = 1
        
        # 5. è®¢å•çº§ç‰¹å¾ï¼ˆéœ€è¦èšåˆï¼‰
        order_agg = df.groupby('è®¢å•ID').agg({
            'å•†å“å®å”®ä»·': ['sum', 'mean', 'count'],
            'é…é€è·ç¦»': 'first',
            'hour': 'first',
            'weekday': 'first',
            'is_weekend': 'first'
        }).reset_index()
        order_agg.columns = ['è®¢å•ID', 'è®¢å•é‡‘é¢', 'å¹³å‡å•ä»·', 'å•†å“æ•°', 'é…é€è·ç¦»', 'hour', 'weekday', 'is_weekend']
        
        # 6. é…é€è´¹ç‰¹å¾ï¼ˆå¢å¼ºï¼‰
        if 'ç‰©æµé…é€è´¹' in df.columns:
            order_agg = order_agg.merge(
                df.groupby('è®¢å•ID')['ç‰©æµé…é€è´¹'].first().reset_index(),
                on='è®¢å•ID'
            )
            order_agg['delivery_fee_ratio'] = (
                order_agg['ç‰©æµé…é€è´¹'] / order_agg['è®¢å•é‡‘é¢']
            ).fillna(0)
        else:
            order_agg['ç‰©æµé…é€è´¹'] = 0
            order_agg['delivery_fee_ratio'] = 0
        
        # 7. O2Oç‰¹æœ‰ç‰¹å¾
        order_agg['is_single_item'] = (order_agg['å•†å“æ•°'] == 1).astype(int)  # å•ä»¶è®¢å•
        order_agg['is_multi_item'] = (order_agg['å•†å“æ•°'] >= 3).astype(int)   # å¤šä»¶è®¢å•
        order_agg['is_high_value'] = (order_agg['è®¢å•é‡‘é¢'] > 50).astype(int)  # é«˜å®¢å•ä»·
        
        return order_agg
    
    def auto_label_scenes(self, order_features: pd.DataFrame) -> pd.Series:
        """
        è‡ªåŠ¨æ ‡æ³¨åœºæ™¯ï¼ˆåŸºäºè§„åˆ™ - O2Oå¤–å–ä¼˜åŒ–ç‰ˆï¼‰
        
        åœºæ™¯å®šä¹‰ï¼ˆåŸºäºä¸šåŠ¡ä¸“å®¶ç»éªŒï¼‰ï¼š
        1. æ—©é¤åˆšéœ€åœºæ™¯ï¼ˆ6-8ç‚¹ï¼‰ï¼šå‡ºè¡Œ/æ•´ç†/æ—©é¤
        2. æ—¥å¸¸è¡¥ç»™åœºæ™¯ï¼ˆ9-11ã€14-17ç‚¹ï¼‰ï¼šåŠå…¬/å±…å®¶/æ—¥ç”¨/å®¶åŠ¡/äº²å­
        3. æ­£é¤é«˜å³°åœºæ™¯ï¼ˆ12-13ã€18-20ç‚¹ï¼‰ï¼šåˆé¤/æ™šé¤/å½’å®¶
        4. ä¼‘é—²å¨±ä¹åœºæ™¯ï¼ˆ21-23ç‚¹ï¼‰ï¼šå±…å®¶/å¤œç”Ÿæ´»å‰/ä¸‹åˆèŒ¶
        5. æ·±å¤œåº”æ€¥åœºæ™¯ï¼ˆ0-5ç‚¹ï¼‰ï¼šçªå‘/æ€¥ç”¨/å¤œå®µ/ç†¬å¤œ
        """
        df = order_features.copy()
        
        scenes = []
        for _, row in df.iterrows():
            hour = row.get('hour', 12)
            item_count = row.get('å•†å“æ•°', 1)
            distance = row.get('é…é€è·ç¦»', 0)
            fee_ratio = row.get('delivery_fee_ratio', 0)
            order_amount = row.get('è®¢å•é‡‘é¢', 0)
            is_weekend = row.get('is_weekend', 0)
            
            # è§„åˆ™åˆ¤æ–­ï¼ˆä¼˜åŒ–ï¼šè¦†ç›–å…¨å¤©24å°æ—¶ï¼‰
            if 6 <= hour < 9:
                # æ—©é¤åˆšéœ€ï¼š6-8ç‚¹
                scene = 'æ—©é¤åˆšéœ€'
                
            elif 9 <= hour < 12:
                # ä¸Šåˆæ—¶æ®µï¼šæ—¥å¸¸è¡¥ç»™
                if is_weekend:
                    scene = 'æ—¥å¸¸è¡¥ç»™(å‘¨æœ«å±…å®¶)'
                else:
                    scene = 'æ—¥å¸¸è¡¥ç»™(å·¥ä½œæ—¥)'
                    
            elif 12 <= hour < 14:
                # æ­£åˆï¼šåˆé¤é«˜å³°
                scene = 'æ­£é¤é«˜å³°'
                
            elif 14 <= hour < 18:
                # ä¸‹åˆæ—¶æ®µï¼šæ—¥å¸¸è¡¥ç»™æˆ–ä¼‘é—²
                if item_count >= 3:
                    scene = 'ä¼‘é—²å¨±ä¹'  # å¤šä»¶é›¶é£Ÿ = ä¸‹åˆèŒ¶
                else:
                    if is_weekend:
                        scene = 'æ—¥å¸¸è¡¥ç»™(å‘¨æœ«å±…å®¶)'
                    else:
                        scene = 'æ—¥å¸¸è¡¥ç»™(å·¥ä½œæ—¥)'
                        
            elif 18 <= hour < 21:
                # å‚æ™šï¼šæ™šé¤é«˜å³°
                scene = 'æ­£é¤é«˜å³°'
                    
            elif 21 <= hour < 24:
                # æ™šé—´ï¼šä¼‘é—²å¨±ä¹
                scene = 'ä¼‘é—²å¨±ä¹'
                    
            elif 0 <= hour < 3:
                # æ·±å¤œ0-2ç‚¹ï¼šå¤œå®µæˆ–åº”æ€¥
                if fee_ratio > 0.15 or distance > 3:
                    scene = 'æ·±å¤œåº”æ€¥(ç´§æ€¥)'
                else:
                    scene = 'æ·±å¤œåº”æ€¥(å¤œå®µ)'
                    
            else:  # 3-6ç‚¹
                # å‡Œæ™¨3-5ç‚¹ï¼šç†¬å¤œå…š
                scene = 'æ·±å¤œåº”æ€¥(ç†¬å¤œå…š)'
            
            scenes.append(scene)
        
        return pd.Series(scenes, index=df.index)
    
    def train(self, order_data: pd.DataFrame) -> Dict[str, Any]:
        """
        è®­ç»ƒåœºæ™¯è¯†åˆ«æ¨¡å‹
        
        Args:
            order_data: è®¢å•æ˜ç»†æ•°æ®
            
        Returns:
            è®­ç»ƒç»“æœç»Ÿè®¡
        """
        try:
            print("ğŸ”§ ç‰¹å¾å·¥ç¨‹...")
            features_df = self.prepare_features(order_data)
            
            print("ğŸ·ï¸ è‡ªåŠ¨æ ‡æ³¨åœºæ™¯...")
            features_df['scene'] = self.auto_label_scenes(features_df)
            
            # è¯Šæ–­ï¼šæ˜¾ç¤ºåœºæ™¯åˆ†å¸ƒ
            scene_counts = features_df['scene'].value_counts()
            print(f"\nğŸ“Š åœºæ™¯åˆ†å¸ƒç»Ÿè®¡:")
            for scene, count in scene_counts.items():
                print(f"   {scene}: {count}å• ({count/len(features_df)*100:.1f}%)")
            
            # è¯Šæ–­ï¼šæ˜¾ç¤ºæ—¶æ®µåˆ†å¸ƒ
            hour_dist = features_df['hour'].value_counts().sort_index()
            print(f"\nâ° æ—¶æ®µåˆ†å¸ƒ: {hour_dist.index.min()}æ—¶-{hour_dist.index.max()}æ—¶")
            print(f"   ä¸»è¦æ—¶æ®µ: {', '.join([f'{h}æ—¶({c}å•)' for h, c in hour_dist.head(5).items()])}")
            
            # æ£€æŸ¥åœºæ™¯æ•°é‡ï¼Œå¦‚æœå¤ªå°‘åˆ™åˆå¹¶ç»†åˆ†åœºæ™¯
            if len(scene_counts) < 3:
                print(f"\nâš ï¸ è­¦å‘Šï¼šä»…å‘ç°{len(scene_counts)}ä¸ªåœºæ™¯ï¼Œæ•°æ®æ—¶æ®µåˆ†å¸ƒå¯èƒ½è¿‡äºé›†ä¸­")
                print(f"   å»ºè®®ï¼šæ‰©å¤§æ—¶é—´èŒƒå›´æˆ–æ£€æŸ¥æ•°æ®æ˜¯å¦è¦†ç›–å…¨å¤©")
                
                # åˆå¹¶æ·±å¤œå­åœºæ™¯
                features_df['scene'] = features_df['scene'].replace({
                    'æ·±å¤œåº”æ€¥(ç´§æ€¥)': 'æ·±å¤œåº”æ€¥',
                    'æ·±å¤œåº”æ€¥(å¤œå®µ)': 'æ·±å¤œåº”æ€¥',
                    'æ·±å¤œåº”æ€¥(ç†¬å¤œå…š)': 'æ·±å¤œåº”æ€¥',
                    'æ—¥å¸¸è¡¥ç»™(å·¥ä½œæ—¥)': 'æ—¥å¸¸è¡¥ç»™',
                    'æ—¥å¸¸è¡¥ç»™(å‘¨æœ«å±…å®¶)': 'æ—¥å¸¸è¡¥ç»™'
                })
                
                scene_counts = features_df['scene'].value_counts()
                print(f"\nğŸ”„ å·²åˆå¹¶ç»†åˆ†åœºæ™¯ï¼Œå½“å‰åœºæ™¯æ•°: {len(scene_counts)}")
                for scene, count in scene_counts.items():
                    print(f"   {scene}: {count}å• ({count/len(features_df)*100:.1f}%)")
            
            # ç‰¹å¾åˆ—
            feature_cols = ['hour', 'weekday', 'é…é€è·ç¦»', 'è®¢å•é‡‘é¢', 
                           'å¹³å‡å•ä»·', 'å•†å“æ•°', 'delivery_fee_ratio']
            
            # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—å­˜åœ¨
            for col in feature_cols:
                if col not in features_df.columns:
                    features_df[col] = 0
            
            X = features_df[feature_cols].fillna(0)
            y = features_df['scene']
            
            # ç¼–ç æ ‡ç­¾
            y_encoded = self.label_encoder.fit_transform(y)
            
            # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
            )
            
            # æ ‡å‡†åŒ–
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)
            
            # è®­ç»ƒæ¨¡å‹
            print("ğŸš€ è®­ç»ƒæ¨¡å‹...")
            if XGBOOST_AVAILABLE:
                self.model = xgb.XGBClassifier(
                    max_depth=6,
                    n_estimators=100,
                    learning_rate=0.1,
                    random_state=42,
                    use_label_encoder=False,
                    eval_metric='mlogloss',
                    base_score=0.5  # ä¿®å¤: æ˜ç¡®è®¾ç½® base_score é¿å…å‚æ•°é”™è¯¯
                )
            else:
                self.model = RandomForestClassifier(
                    max_depth=6,
                    n_estimators=100,
                    random_state=42
                )
            
            self.model.fit(X_train_scaled, y_train)
            
            # è¯„ä¼°
            train_score = self.model.score(X_train_scaled, y_train)
            test_score = self.model.score(X_test_scaled, y_test)
            
            # ç‰¹å¾é‡è¦æ€§
            self.feature_importance = pd.DataFrame({
                'feature': feature_cols,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            self.is_trained = True
            
            print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
            print(f"   è®­ç»ƒé›†å‡†ç¡®ç‡: {train_score:.3f}")
            print(f"   æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.3f}")
            
            return {
                'status': 'success',
                'train_score': train_score,
                'test_score': test_score,
                'feature_importance': self.feature_importance,
                'scene_distribution': y.value_counts().to_dict()
            }
            
        except Exception as e:
            return {
                'status': 'error',
                'message': f'æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}'
            }
    
    def predict_scene(self, order_data: pd.DataFrame) -> pd.DataFrame:
        """
        é¢„æµ‹è®¢å•åœºæ™¯
        
        Returns:
            åŒ…å«åœºæ™¯é¢„æµ‹å’Œæ¦‚ç‡çš„DataFrame
        """
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")
        
        features_df = self.prepare_features(order_data)
        
        feature_cols = ['hour', 'weekday', 'é…é€è·ç¦»', 'è®¢å•é‡‘é¢', 
                       'å¹³å‡å•ä»·', 'å•†å“æ•°', 'delivery_fee_ratio']
        
        for col in feature_cols:
            if col not in features_df.columns:
                features_df[col] = 0
        
        X = features_df[feature_cols].fillna(0)
        X_scaled = self.scaler.transform(X)
        
        # é¢„æµ‹
        y_pred = self.model.predict(X_scaled)
        y_proba = self.model.predict_proba(X_scaled)
        
        # è§£ç 
        scene_pred = self.label_encoder.inverse_transform(y_pred)
        
        # æ„å»ºç»“æœ
        result = features_df[['è®¢å•ID']].copy()
        result['predicted_scene'] = scene_pred
        
        # æ·»åŠ å„åœºæ™¯æ¦‚ç‡
        for i, scene in enumerate(self.label_encoder.classes_):
            result[f'prob_{scene}'] = y_proba[:, i]
        
        return result
    
    def visualize_feature_importance(self) -> go.Figure:
        """å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§"""
        if self.feature_importance is None:
            fig = go.Figure()
            fig.add_annotation(text="æ¨¡å‹å°šæœªè®­ç»ƒ", x=0.5, y=0.5)
            return fig
        
        fig = px.bar(
            self.feature_importance,
            x='importance',
            y='feature',
            orientation='h',
            title='åœºæ™¯è¯†åˆ«ç‰¹å¾é‡è¦æ€§',
            labels={'importance': 'é‡è¦æ€§', 'feature': 'ç‰¹å¾'}
        )
        fig.update_layout(height=400)
        return fig


# ============================================================================
# 3. RFMå®¢æˆ·åˆ†ç¾¤æ¨¡å‹
# ============================================================================

class RFMCustomerSegmentation:
    """
    RFMå®¢æˆ·åˆ†ç¾¤æ¨¡å‹
    
    åŠŸèƒ½ï¼š
    - åŸºäºRFMï¼ˆæœ€è¿‘è´­ä¹°æ—¶é—´ã€è´­ä¹°é¢‘ç‡ã€è´­ä¹°é‡‘é¢ï¼‰+ åœºæ™¯ç‰¹å¾èšç±»
    - è¯†åˆ«4ç±»ç”¨æˆ·ï¼šé«˜é¢‘åº”æ€¥ã€è®¡åˆ’å›¤è´§ã€ä»·æ ¼æ•æ„Ÿã€å¶å‘å°é²œ
    - ä¸ºæ¯ç±»ç”¨æˆ·ç”Ÿæˆè¥é”€ç­–ç•¥
    """
    
    def __init__(self, n_clusters: int = 4):
        self.n_clusters = n_clusters
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        self.scaler = StandardScaler()
        self.rfm_data = None
        self.cluster_labels = None
        self.cluster_profiles = {}
        
    def calculate_rfm(self, order_data: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—RFMç‰¹å¾
        
        Args:
            order_data: è®¢å•æ•°æ®ï¼Œéœ€åŒ…å«ç”¨æˆ·IDã€æ—¥æœŸã€é‡‘é¢
            
        Returns:
            RFMç‰¹å¾DataFrame
        """
        # ç¡®å®šç”¨æˆ·IDåˆ—
        user_col = None
        for col in ['ç”¨æˆ·ID', 'ç”¨æˆ·ç”µè¯', 'åœ°å€', 'æ”¶è´§åœ°å€']:
            if col in order_data.columns:
                # æ£€æŸ¥è¯¥åˆ—çš„æœ‰æ•ˆå€¼æ¯”ä¾‹
                valid_rate = order_data[col].notna().sum() / len(order_data)
                if valid_rate > 0.1:  # è‡³å°‘10%çš„æ•°æ®æœ‰å€¼
                    user_col = col
                    break
        
        if user_col is None:
            raise ValueError("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ç”¨æˆ·æ ‡è¯†åˆ—")
        
        # è®¢å•çº§èšåˆ - ä½¿ç”¨å•†å“é‡‘é¢ï¼ˆä¸å«é…é€è´¹ï¼‰è®¡ç®—Monetary
        if 'è®¢å•ID' in order_data.columns:
            # RFMçš„Mï¼ˆMonetaryï¼‰åº”è¯¥æ˜¯å•†å“é‡‘é¢ï¼Œä¸åŒ…å«é…é€è´¹
            if 'å•†å“å®å”®ä»·' not in order_data.columns:
                return pd.DataFrame()
            
            # æŒ‰è®¢å•èšåˆï¼šå•†å“å®å”®ä»·æ±‚å’Œ = è®¢å•å•†å“æ€»é‡‘é¢
            order_level = order_data.groupby('è®¢å•ID').agg({
                user_col: 'first',
                'æ—¥æœŸ_datetime': 'first' if 'æ—¥æœŸ_datetime' in order_data.columns else 'first',
                'å•†å“å®å”®ä»·': 'sum'  # è®¢å•å•†å“æ€»é‡‘é¢ï¼ˆä¸å«é…é€è´¹ï¼‰
            }).reset_index()
            
            order_level.columns = ['è®¢å•ID', user_col, 'order_date', 'order_amount']
        else:
            return pd.DataFrame()
        
        # è®¡ç®—RFM
        if 'order_date' in order_level.columns:
            current_date = pd.to_datetime(order_level['order_date']).max()
            order_level['order_date'] = pd.to_datetime(order_level['order_date'])
        else:
            current_date = pd.Timestamp.now()
            order_level['order_date'] = current_date
        
        # è®¡ç®—æ•°æ®æ—¶é—´è·¨åº¦ï¼ˆå¤©æ•°ï¼‰
        min_date = pd.to_datetime(order_level['order_date']).min()
        data_span_days = (current_date - min_date).days + 1
        data_span_weeks = max(data_span_days / 7, 0.1)  # è‡³å°‘0.1å‘¨ï¼Œé¿å…é™¤é›¶
        
        rfm = order_level.groupby(user_col).agg({
            'order_date': lambda x: (current_date - x.max()).days,  # Recency
            'è®¢å•ID': 'count',  # è®¢å•æ€»æ•°
            'order_amount': 'sum'  # Monetary
        }).reset_index()
        
        rfm.columns = [user_col, 'recency', 'order_count', 'monetary']
        
        # æ ‡å‡†åŒ–é¢‘æ¬¡ï¼šè®¡ç®—æ¯å‘¨å¹³å‡è®¢å•æ•°ï¼ˆæ›´æœ‰ä¸šåŠ¡æ„ä¹‰ï¼‰
        rfm['frequency'] = rfm['order_count'] / data_span_weeks
        
        # ä¿ç•™åŸå§‹è®¢å•æ•°å’Œæ•°æ®å‘¨æœŸï¼Œç”¨äºå‰ç«¯å±•ç¤º
        rfm['total_orders'] = rfm['order_count']
        rfm['data_span_days'] = data_span_days
        
        print(f"ğŸ“Š æ•°æ®æ—¶é—´è·¨åº¦: {data_span_days}å¤© ({data_span_weeks:.1f}å‘¨)")
        print(f"   é¢‘æ¬¡å·²æ ‡å‡†åŒ–ä¸ºæ¯å‘¨å¹³å‡è®¢å•æ•°")
        
        # è¿‡æ»¤å¼‚å¸¸å€¼ï¼šå‰”é™¤è¶…é«˜é¢‘ç”¨æˆ·ï¼ˆå¯èƒ½æ˜¯æ•°æ®èšåˆé—®é¢˜ï¼‰
        # ä½¿ç”¨IQRæ–¹æ³•è¯†åˆ«å¼‚å¸¸å€¼
        freq_q75 = rfm['frequency'].quantile(0.75)
        freq_q25 = rfm['frequency'].quantile(0.25)
        freq_iqr = freq_q75 - freq_q25
        freq_upper_bound = freq_q75 + 3 * freq_iqr  # 3å€IQRä½œä¸ºä¸Šç•Œ
        
        # è®°å½•å¼‚å¸¸ç”¨æˆ·æ•°é‡
        outlier_users = rfm[rfm['frequency'] > freq_upper_bound]
        if len(outlier_users) > 0:
            print(f"âš ï¸  æ£€æµ‹åˆ° {len(outlier_users)} ä¸ªå¼‚å¸¸é«˜é¢‘ç”¨æˆ·ï¼ˆé¢‘æ¬¡>{freq_upper_bound:.0f}ï¼‰ï¼Œå·²è‡ªåŠ¨è¿‡æ»¤")
            print(f"   å¼‚å¸¸ç”¨æˆ·é¢‘æ¬¡èŒƒå›´: {outlier_users['frequency'].min():.0f}-{outlier_users['frequency'].max():.0f}")
        
        # è¿‡æ»¤æ‰å¼‚å¸¸å€¼
        rfm = rfm[rfm['frequency'] <= freq_upper_bound].copy()
        
        # æ·»åŠ åœºæ™¯ç‰¹å¾
        if 'é…é€è·ç¦»' in order_data.columns:
            avg_dist = order_data.groupby(user_col)['é…é€è·ç¦»'].mean().reset_index()
            # æ£€æµ‹è·ç¦»å•ä½ï¼Œå¦‚æœå¹³å‡å€¼>100ï¼Œå¾ˆå¯èƒ½æ˜¯ç±³ï¼Œéœ€è¦è½¬æ¢ä¸ºå…¬é‡Œ
            if avg_dist['é…é€è·ç¦»'].mean() > 100:
                avg_dist['é…é€è·ç¦»'] = avg_dist['é…é€è·ç¦»'] / 1000
            rfm = rfm.merge(avg_dist, on=user_col)
            rfm.rename(columns={'é…é€è·ç¦»': 'avg_distance'}, inplace=True)
        else:
            rfm['avg_distance'] = 0
        
        # é…é€å‡€æˆæœ¬å æ¯”è®¡ç®—ï¼ˆåŸºäºè®¢å•åº•å±‚ä¸šåŠ¡é€»è¾‘ï¼‰
        if 'ç‰©æµé…é€è´¹' in order_data.columns and 'å•†å“å®å”®ä»·' in order_data.columns and 'è®¢å•ID' in order_data.columns:
            # é…é€å‡€æˆæœ¬ = ç‰©æµé…é€è´¹ - ç”¨æˆ·æ”¯ä»˜é…é€è´¹ + é…é€è´¹å‡å…é‡‘é¢
            # é…é€å‡€æˆæœ¬å æ¯” = é…é€å‡€æˆæœ¬ / å•†å“é‡‘é¢
            
            agg_dict = {
                user_col: 'first',
                'ç‰©æµé…é€è´¹': 'first',      # è®¢å•çº§å­—æ®µ
                'å•†å“å®å”®ä»·': 'sum'         # æ˜ç»†çº§ï¼Œæ±‚å’Œå¾—åˆ°è®¢å•å•†å“æ€»é‡‘é¢
            }
            
            # æ·»åŠ å¯é€‰å­—æ®µ
            if 'ç”¨æˆ·æ”¯ä»˜é…é€è´¹' in order_data.columns:
                agg_dict['ç”¨æˆ·æ”¯ä»˜é…é€è´¹'] = 'first'
            if 'é…é€è´¹å‡å…é‡‘é¢' in order_data.columns:
                agg_dict['é…é€è´¹å‡å…é‡‘é¢'] = 'first'
            
            order_fee_data = order_data.groupby('è®¢å•ID').agg(agg_dict).reset_index()
            
            # è®¡ç®—é…é€å‡€æˆæœ¬ï¼ˆé—¨åº—å®é™…æ‰¿æ‹…çš„é…é€æˆæœ¬ï¼‰
            logistics_fee = pd.to_numeric(order_fee_data['ç‰©æµé…é€è´¹'], errors='coerce').fillna(0)
            user_paid = pd.to_numeric(order_fee_data.get('ç”¨æˆ·æ”¯ä»˜é…é€è´¹', pd.Series(0, index=order_fee_data.index)), errors='coerce').fillna(0)
            fee_discount = pd.to_numeric(order_fee_data.get('é…é€è´¹å‡å…é‡‘é¢', pd.Series(0, index=order_fee_data.index)), errors='coerce').fillna(0)
            
            # é…é€å‡€æˆæœ¬ = ç‰©æµé…é€è´¹ - ç”¨æˆ·æ”¯ä»˜ + å¹³å°å‡å…
            order_fee_data['net_delivery_cost'] = logistics_fee - user_paid + fee_discount
            
            # é…é€å‡€æˆæœ¬å å•†å“é‡‘é¢çš„æ¯”ä¾‹
            order_fee_data['fee_ratio'] = (
                order_fee_data['net_delivery_cost'] / 
                pd.to_numeric(order_fee_data['å•†å“å®å”®ä»·'], errors='coerce').replace(0, np.nan)
            ).fillna(0)
            
            # é™åˆ¶å¼‚å¸¸å€¼ï¼šé…é€è´¹å æ¯”é€šå¸¸åœ¨-50%~100%ä¹‹é—´ï¼ˆè´Ÿå€¼è¡¨ç¤ºç”¨æˆ·æ”¯ä»˜>å®é™…æˆæœ¬ï¼‰
            order_fee_data['fee_ratio'] = order_fee_data['fee_ratio'].clip(lower=-0.5, upper=1.0)
            
            # æŒ‰ç”¨æˆ·èšåˆå¹³å‡é…é€è´¹å æ¯”
            user_fee_ratio = order_fee_data.groupby(user_col)['fee_ratio'].mean().reset_index()
            rfm = rfm.merge(user_fee_ratio, on=user_col, how='left')
            rfm.rename(columns={'fee_ratio': 'avg_fee_ratio'}, inplace=True)
        else:
            rfm['avg_fee_ratio'] = 0
        
        # æ·»åŠ å›¤è´§è¡Œä¸ºç‰¹å¾ï¼šå•†å“æ•°é‡å’Œå“ç±»å¤šæ ·æ€§
        if 'è®¢å•ID' in order_data.columns:
            # ç¡®å®šå“ç±»åˆ—åï¼ˆå¯èƒ½æ˜¯"ç¾å›¢ä¸‰çº§åˆ†ç±»"æˆ–"ä¸‰çº§åˆ†ç±»å"ï¼‰
            category_col = None
            for col in ['ç¾å›¢ä¸‰çº§åˆ†ç±»', 'ä¸‰çº§åˆ†ç±»å', 'ä¸‰çº§åˆ†ç±»', 'åˆ†ç±»']:
                if col in order_data.columns:
                    category_col = col
                    break
            
            # æ„å»ºèšåˆå­—å…¸
            agg_dict = {
                user_col: 'first'
            }
            
            # æ·»åŠ å•†å“åç§°è®¡æ•°ï¼ˆå•†å“ä»¶æ•°ï¼‰
            if 'å•†å“åç§°' in order_data.columns:
                agg_dict['å•†å“åç§°'] = 'count'
            
            # æ·»åŠ å“ç±»è®¡æ•°ï¼ˆå“ç±»å¤šæ ·æ€§ï¼‰
            if category_col:
                agg_dict[category_col] = 'nunique'
            
            # è®¡ç®—æ¯ä¸ªè®¢å•çš„å•†å“æ•°é‡å’Œå“ç±»æ•°
            order_items = order_data.groupby('è®¢å•ID').agg(agg_dict).reset_index()
            
            # åŠ¨æ€è®¾ç½®åˆ—å
            new_cols = ['è®¢å•ID', user_col]
            if 'å•†å“åç§°' in agg_dict:
                new_cols.append('items_count')
            if category_col:
                new_cols.append('category_count')
            
            order_items.columns = new_cols
            
            # æŒ‰ç”¨æˆ·èšåˆå¹³å‡å€¼
            agg_user_dict = {}
            if 'items_count' in order_items.columns:
                agg_user_dict['items_count'] = 'mean'
            if 'category_count' in order_items.columns:
                agg_user_dict['category_count'] = 'mean'
            
            if agg_user_dict:
                user_items = order_items.groupby(user_col).agg(agg_user_dict).reset_index()
                
                # é‡å‘½ååˆ—
                rename_dict = {}
                if 'items_count' in user_items.columns:
                    rename_dict['items_count'] = 'avg_items_per_order'
                if 'category_count' in user_items.columns:
                    rename_dict['category_count'] = 'avg_categories_per_order'
                
                user_items = user_items.rename(columns=rename_dict)
                rfm = rfm.merge(user_items, on=user_col, how='left')
            
            # å¡«å……ç¼ºå¤±å€¼
            if 'avg_items_per_order' not in rfm.columns:
                rfm['avg_items_per_order'] = 0
            if 'avg_categories_per_order' not in rfm.columns:
                rfm['avg_categories_per_order'] = 0
        else:
            rfm['avg_items_per_order'] = 0
            rfm['avg_categories_per_order'] = 0
        
        self.rfm_data = rfm
        return rfm
    
    def segment_customers(self) -> Dict[str, Any]:
        """
        å®¢æˆ·åˆ†ç¾¤
        
        Returns:
            åˆ†ç¾¤ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        if self.rfm_data is None or self.rfm_data.empty:
            return {
                'status': 'error',
                'message': 'RFMæ•°æ®ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨calculate_rfm()'
            }
        
        try:
            # ç‰¹å¾åˆ—ï¼ˆå¢åŠ å•†å“æ•°é‡å’Œå“ç±»å¤šæ ·æ€§ï¼‰
            feature_cols = [
                'recency', 'frequency', 'monetary', 
                'avg_distance', 'avg_fee_ratio',
                'avg_items_per_order', 'avg_categories_per_order'
            ]
            X = self.rfm_data[feature_cols].fillna(0)
            
            # æ ‡å‡†åŒ–
            X_scaled = self.scaler.fit_transform(X)
            
            # èšç±»
            self.cluster_labels = self.kmeans.fit_predict(X_scaled)
            self.rfm_data['cluster'] = self.cluster_labels
            
            # è®¡ç®—è½®å»“ç³»æ•°
            silhouette_avg = silhouette_score(X_scaled, self.cluster_labels)
            
            # åˆ†ææ¯ä¸ªç°‡çš„ç‰¹å¾
            self._profile_clusters()
            
            print(f"âœ… å®¢æˆ·åˆ†ç¾¤å®Œæˆï¼š{self.n_clusters}ä¸ªç¾¤ç»„")
            print(f"   è½®å»“ç³»æ•°: {silhouette_avg:.3f}")
            
            return {
                'status': 'success',
                'n_clusters': self.n_clusters,
                'silhouette_score': silhouette_avg,
                'cluster_profiles': self.cluster_profiles,
                'distribution': self.rfm_data['cluster'].value_counts().to_dict()
            }
            
        except Exception as e:
            return {
                'status': 'error',
                'message': f'åˆ†ç¾¤å¤±è´¥: {str(e)}'
            }
    
    def _profile_clusters(self):
        """
        åˆ†ææ¯ä¸ªç°‡çš„ç‰¹å¾ç”»åƒï¼ˆæ”¹è¿›ç‰ˆï¼šç»“åˆå•†å“æ•°é‡å’Œå“ç±»åˆ¤æ–­å›¤è´§è¡Œä¸ºï¼‰
        """
        feature_cols = [
            'recency', 'frequency', 'monetary', 
            'avg_distance', 'avg_fee_ratio',
            'avg_items_per_order', 'avg_categories_per_order'
        ]
        
        # å…ˆè®¡ç®—æ‰€æœ‰ç°‡çš„ç‰¹å¾
        cluster_stats = []
        for cluster_id in range(self.n_clusters):
            cluster_data = self.rfm_data[self.rfm_data['cluster'] == cluster_id]
            
            profile = {
                'cluster_id': cluster_id,
                'size': len(cluster_data),
                'percentage': len(cluster_data) / len(self.rfm_data) * 100,
                'avg_recency': cluster_data['recency'].mean(),
                'avg_frequency': cluster_data['frequency'].mean(),
                'avg_monetary': cluster_data['monetary'].mean(),
                'avg_distance': cluster_data['avg_distance'].mean(),
                'avg_fee_ratio': cluster_data['avg_fee_ratio'].mean(),
                'avg_items_per_order': cluster_data['avg_items_per_order'].mean(),
                'avg_categories_per_order': cluster_data['avg_categories_per_order'].mean(),
                # æ–°å¢ï¼šä¿ç•™åŸå§‹è®¢å•æ•°å’Œæ•°æ®å‘¨æœŸï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰
                'avg_total_orders': cluster_data['total_orders'].mean(),
                'data_span_days': cluster_data['data_span_days'].iloc[0] if len(cluster_data) > 0 else 30
            }
            cluster_stats.append(profile)
        
        cluster_df = pd.DataFrame(cluster_stats)
        
        # è¿‡æ»¤å¼‚å¸¸ç°‡ï¼ˆäººæ•°<æ€»æ•°çš„1%ï¼Œæˆ–äººæ•°<10ï¼‰
        min_size = max(10, len(self.rfm_data) * 0.01)
        normal_clusters = cluster_df[cluster_df['size'] >= min_size].copy()
        outlier_clusters = cluster_df[cluster_df['size'] < min_size].copy()
        
        # æŒ‰ç‰¹å¾æ’åºåˆ†é…4ç§å®¢æˆ·ç±»å‹ï¼ˆæ”¹è¿›ç‰ˆï¼šå›¤è´§åˆ¤æ–­æ›´å‡†ç¡®ï¼‰
        assigned_profiles = {}
        
        # 1. è®¡åˆ’å›¤è´§ç”¨æˆ·ï¼šé«˜å•†å“æ•°é‡ + é«˜å“ç±»å¤šæ ·æ€§ + ä½é¢‘æ¬¡ + ä½é…é€è´¹å æ¯”
        #    ï¼ˆçœŸæ­£çš„å›¤è´§ï¼šä¹°å¾—å¤šã€ä¹°å¾—æ‚ã€ä¸å¸¸ä¹°ã€ä¸æ˜¯å› ä¸ºèµ·é€ä»·é«˜ï¼‰
        if len(normal_clusters) > 0:
            normal_clusters['bulk_score'] = (
                normal_clusters['avg_items_per_order'] * 0.3 +          # å•†å“æ•°é‡æƒé‡30%
                normal_clusters['avg_categories_per_order'] * 0.3 +     # å“ç±»å¤šæ ·æ€§30%
                normal_clusters['avg_monetary'] * 0.02 +                # é‡‘é¢æƒé‡20%ï¼ˆç³»æ•°0.02é¿å…æ•°å€¼è¿‡å¤§ï¼‰
                (1 / (normal_clusters['avg_frequency'] + 0.1)) * 0.1 +  # ä½é¢‘æ¬¡10%
                (1 / (normal_clusters['avg_fee_ratio'] + 0.01)) * 0.1   # ä½é…é€è´¹å æ¯”10%
            )
            bulk_cluster = normal_clusters.nlargest(1, 'bulk_score').iloc[0]
            assigned_profiles[int(bulk_cluster['cluster_id'])] = {
                **cluster_stats[int(bulk_cluster['cluster_id'])],
                'name': 'è®¡åˆ’å›¤è´§ç”¨æˆ·',
                'strategy': 'æ¨èå¤§åŒ…è£…ä¿ƒé”€ï¼Œæ»¡å‡æ´»åŠ¨ï¼Œä¼šå‘˜å‚¨å€¼ä¼˜æƒ ',
                'definition': 'ä¸»åŠ¨è§„åˆ’æ€§é‡‡è´­ï¼Œå•æ¬¡è´­ä¹°å•†å“æ•°é‡å¤šï¼ˆ6-10ä»¶/å•ï¼‰ã€å“ç±»ä¸°å¯Œï¼ˆ4-6ç§/å•ï¼‰ï¼Œè¿½æ±‚æ€§ä»·æ¯”è€Œéå³æ—¶æ€§ï¼Œé…é€è´¹å æ¯”ç›¸å¯¹è¾ƒä½'
            }
            normal_clusters = normal_clusters[normal_clusters['cluster_id'] != bulk_cluster['cluster_id']]
        
        # 2. ä»·æ ¼æ•æ„Ÿç”¨æˆ·ï¼šé…é€è´¹å æ¯”æœ€é«˜ï¼ˆå¯¹é…é€æˆæœ¬æ•æ„Ÿï¼Œè¿½æ±‚å…é…é€è´¹ï¼‰
        if len(normal_clusters) > 0:
            price_cluster = normal_clusters.nlargest(1, 'avg_fee_ratio').iloc[0]
            assigned_profiles[int(price_cluster['cluster_id'])] = {
                **cluster_stats[int(price_cluster['cluster_id'])],
                'name': 'ä»·æ ¼æ•æ„Ÿç”¨æˆ·',
                'strategy': 'ä¸»æ¨ç‰¹ä»·å•†å“ï¼Œæ‹¼å›¢ä¼˜æƒ ï¼Œæ»¡é¢å…é…é€è´¹',
                'definition': 'å¯¹ä»·æ ¼å’Œé…é€æˆæœ¬é«˜åº¦æ•æ„Ÿï¼Œå•æ¬¡è´­ä¹°é‡‘é¢ç›¸å¯¹è¾ƒä½ï¼Œé…é€è´¹å æ¯”è¾ƒé«˜ï¼ˆ20-25%ï¼‰ï¼Œå€¾å‘äºå‡‘å•æ»¡å‡æˆ–å¯»æ‰¾å…é…é€è´¹æ´»åŠ¨'
            }
            normal_clusters = normal_clusters[normal_clusters['cluster_id'] != price_cluster['cluster_id']]
        
        # 3. é«˜é¢‘åº”æ€¥ç”¨æˆ·ï¼šé¢‘æ¬¡æœ€é«˜ æˆ– å‰©ä½™ç°‡ä¸­ç‰¹å¾æœ€æ˜æ˜¾çš„
        if len(normal_clusters) > 0:
            # å¦‚æœæœ‰æ˜æ˜¾é«˜é¢‘çš„ç°‡ï¼Œé€‰å®ƒï¼›å¦åˆ™é€‰å‰©ä½™çš„ç¬¬ä¸€ä¸ªä½œä¸ºé«˜é¢‘åº”æ€¥
            emergency_cluster = normal_clusters.nlargest(1, 'avg_frequency').iloc[0]
            assigned_profiles[int(emergency_cluster['cluster_id'])] = {
                **cluster_stats[int(emergency_cluster['cluster_id'])],
                'name': 'é«˜é¢‘åº”æ€¥ç”¨æˆ·',
                'strategy': 'ä¿è¯åº”æ€¥å•†å“åº“å­˜ï¼Œæä¾›åŠ æ€¥é…é€æœåŠ¡ï¼Œé€‚åº¦æº¢ä»·å¯æ¥å—',
                'definition': 'è´­ä¹°é¢‘æ¬¡ç›¸å¯¹è¾ƒé«˜ï¼ˆ1.3-1.5æ¬¡/å‘¨ï¼‰ï¼Œè´­ä¹°è¡Œä¸ºå…·æœ‰åº”æ€¥æ€§å’Œå³æ—¶æ€§ç‰¹å¾ï¼Œå¯¹é…é€é€Ÿåº¦è¦æ±‚é«˜ï¼Œå¯¹ä»·æ ¼ç›¸å¯¹ä¸æ•æ„Ÿ'
            }
            normal_clusters = normal_clusters[normal_clusters['cluster_id'] != emergency_cluster['cluster_id']]
        
        # 4. å¶å‘å°é²œç”¨æˆ·ï¼šå‰©ä½™çš„æ­£å¸¸ç°‡
        if len(normal_clusters) > 0:
            for _, row in normal_clusters.iterrows():
                assigned_profiles[int(row['cluster_id'])] = {
                    **cluster_stats[int(row['cluster_id'])],
                    'name': 'å¶å‘å°é²œç”¨æˆ·',
                    'strategy': 'æ–°å“æ¨èï¼Œé¦–å•ä¼˜æƒ ï¼Œåœºæ™¯åŒ–å¥—é¤å¼•å¯¼',
                    'definition': 'è´­ä¹°é¢‘æ¬¡ä½ï¼ˆ1-2æ¬¡/æœˆï¼‰ï¼Œå°è¯•æ€§è´­ä¹°ä¸ºä¸»ï¼Œå¯¹æ–°å“å’Œä¿ƒé”€æ´»åŠ¨æ•æ„Ÿï¼Œéœ€è¦é€šè¿‡ä¼˜æƒ å’Œåœºæ™¯åŒ–è¥é”€æ¿€æ´»å¤è´­'
                }
        
        # å¼‚å¸¸ç°‡ç»Ÿä¸€å½’ä¸º"å¶å‘å°é²œç”¨æˆ·"
        for _, row in outlier_clusters.iterrows():
            assigned_profiles[int(row['cluster_id'])] = {
                **cluster_stats[int(row['cluster_id'])],
                'name': 'å¶å‘å°é²œç”¨æˆ·',
                'strategy': 'æ–°å“æ¨èï¼Œé¦–å•ä¼˜æƒ ï¼Œåœºæ™¯åŒ–å¥—é¤å¼•å¯¼',
                'definition': 'è´­ä¹°é¢‘æ¬¡æä½æˆ–æ•°æ®å¼‚å¸¸ï¼Œå°è¯•æ€§è´­ä¹°ä¸ºä¸»ï¼Œéœ€è¦é€šè¿‡ä¼˜æƒ å’Œåœºæ™¯åŒ–è¥é”€æ¿€æ´»å¤è´­'
            }
        
        self.cluster_profiles = assigned_profiles
    
    def visualize_clusters(self) -> go.Figure:
        """
        å¯è§†åŒ–å®¢æˆ·ç¾¤ç»„ï¼ˆ3Dæ•£ç‚¹å›¾ï¼‰
        """
        if self.rfm_data is None or 'cluster' not in self.rfm_data.columns:
            fig = go.Figure()
            fig.add_annotation(text="å°šæœªè¿›è¡Œåˆ†ç¾¤", x=0.5, y=0.5)
            return fig
        
        # ä½¿ç”¨RFMä¸‰ç»´å¯è§†åŒ–
        df_plot = self.rfm_data.copy()
        df_plot['cluster_name'] = df_plot['cluster'].map(
            lambda x: self.cluster_profiles[x]['name']
        )
        
        fig = px.scatter_3d(
            df_plot,
            x='recency',
            y='frequency',
            z='monetary',
            color='cluster_name',
            title='RFMå®¢æˆ·åˆ†ç¾¤3Dè§†å›¾',
            labels={
                'recency': 'æœ€è¿‘è´­ä¹°(å¤©)',
                'frequency': 'è´­ä¹°é¢‘æ¬¡',
                'monetary': 'è´­ä¹°é‡‘é¢',
                'cluster_name': 'å®¢æˆ·ç¾¤ç»„'
            },
            hover_data=['avg_distance', 'avg_fee_ratio']
        )
        
        fig.update_layout(height=600)
        return fig
    
    def get_cluster_summary(self) -> pd.DataFrame:
        """
        è·å–ç¾¤ç»„æ‘˜è¦è¡¨
        """
        if not self.cluster_profiles:
            return pd.DataFrame()
        
        summary = []
        for cluster_id, profile in self.cluster_profiles.items():
            summary.append({
                'ç¾¤ç»„': profile['name'],
                'ç”¨æˆ·æ•°': profile['size'],
                'å æ¯”': f"{profile['percentage']:.1f}%",
                'å¹³å‡é¢‘æ¬¡': f"{profile['avg_frequency']:.1f}",
                'å¹³å‡é‡‘é¢': f"Â¥{profile['avg_monetary']:.0f}",
                'å¹³å‡è·ç¦»': f"{profile['avg_distance']:.1f}km",
                'é…é€è´¹å æ¯”': f"{profile['avg_fee_ratio']*100:.1f}%",
                'è¥é”€ç­–ç•¥': profile['strategy']
            })
        
        return pd.DataFrame(summary)


# ============================================================================
# 4. å†³ç­–æ ‘è§„åˆ™ç”Ÿæˆå™¨
# ============================================================================

class SceneDecisionTreeRules:
    """
    åœºæ™¯è¯†åˆ«å†³ç­–æ ‘è§„åˆ™ç”Ÿæˆå™¨
    
    åŠŸèƒ½ï¼š
    - ç”Ÿæˆå¯è§£é‡Šçš„IF-THENè§„åˆ™
    - å¯è§†åŒ–å†³ç­–è·¯å¾„
    - è‡ªåŠ¨æ ‡æ³¨è®¢å•åœºæ™¯
    """
    
    def __init__(self, max_depth: int = 5):
        self.max_depth = max_depth
        self.tree = DecisionTreeClassifier(
            max_depth=max_depth,
            min_samples_split=50,
            min_samples_leaf=20,
            random_state=42
        )
        self.feature_names = []
        self.class_names = []
        self.rules_text = ""
        self.is_trained = False
        
    def train_rule_tree(self, order_data: pd.DataFrame) -> Dict[str, Any]:
        """
        è®­ç»ƒå†³ç­–æ ‘è§„åˆ™
        """
        try:
            # ç‰¹å¾å·¥ç¨‹ï¼ˆå¤ç”¨SceneRecognitionModelçš„é€»è¾‘ï¼‰
            scene_model = SceneRecognitionModel()
            features_df = scene_model.prepare_features(order_data)
            features_df['scene'] = scene_model.auto_label_scenes(features_df)
            
            # ç‰¹å¾åˆ—
            self.feature_names = ['hour', 'weekday', 'é…é€è·ç¦»', 'è®¢å•é‡‘é¢', 
                                 'å¹³å‡å•ä»·', 'å•†å“æ•°', 'delivery_fee_ratio']
            
            for col in self.feature_names:
                if col not in features_df.columns:
                    features_df[col] = 0
            
            X = features_df[self.feature_names].fillna(0)
            y = features_df['scene']
            
            self.class_names = y.unique().tolist()
            
            # è®­ç»ƒå†³ç­–æ ‘
            self.tree.fit(X, y)
            
            # æå–è§„åˆ™
            self.rules_text = export_text(
                self.tree, 
                feature_names=self.feature_names,
                class_names=self.class_names,
                max_depth=self.max_depth
            )
            
            self.is_trained = True
            
            # è¯„ä¼°
            train_score = self.tree.score(X, y)
            
            print(f"âœ… å†³ç­–æ ‘è§„åˆ™ç”Ÿæˆå®Œæˆ")
            print(f"   å‡†ç¡®ç‡: {train_score:.3f}")
            print(f"   æ ‘æ·±åº¦: {self.tree.get_depth()}")
            print(f"   å¶å­èŠ‚ç‚¹æ•°: {self.tree.get_n_leaves()}")
            
            return {
                'status': 'success',
                'accuracy': train_score,
                'tree_depth': self.tree.get_depth(),
                'n_leaves': self.tree.get_n_leaves(),
                'rules': self.rules_text
            }
            
        except Exception as e:
            return {
                'status': 'error',
                'message': f'è§„åˆ™ç”Ÿæˆå¤±è´¥: {str(e)}'
            }
    
    def get_rules_text(self) -> str:
        """è·å–æ–‡æœ¬æ ¼å¼çš„è§„åˆ™"""
        return self.rules_text
    
    def extract_key_rules(self, top_n: int = 10) -> List[str]:
        """
        æå–å…³é”®è§„åˆ™ï¼ˆåŸºäºç‰¹å¾é‡è¦æ€§ï¼‰
        """
        if not self.is_trained:
            return []
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        importance = self.tree.feature_importances_
        important_features = sorted(
            zip(self.feature_names, importance),
            key=lambda x: x[1],
            reverse=True
        )[:top_n]
        
        rules = [
            f"â€¢ {feat}: {imp:.3f}" for feat, imp in important_features
        ]
        
        return rules
    
    def visualize_tree_rules(self) -> go.Figure:
        """
        å¯è§†åŒ–å†³ç­–æ ‘è§„åˆ™ï¼ˆç®€åŒ–ç‰ˆï¼‰
        """
        if not self.is_trained:
            fig = go.Figure()
            fig.add_annotation(text="å†³ç­–æ ‘å°šæœªè®­ç»ƒ", x=0.5, y=0.5)
            return fig
        
        # ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾
        importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.tree.feature_importances_
        }).sort_values('importance', ascending=False)
        
        fig = px.bar(
            importance_df,
            x='importance',
            y='feature',
            orientation='h',
            title='å†³ç­–æ ‘ç‰¹å¾é‡è¦æ€§',
            labels={'importance': 'é‡è¦æ€§', 'feature': 'ç‰¹å¾'}
        )
        fig.update_layout(height=400)
        return fig


# ============================================================================
# 5. ç»Ÿä¸€çš„åœºæ™¯è¥é”€æ™ºèƒ½å†³ç­–å¼•æ“
# ============================================================================

class SceneMarketingIntelligence:
    """
    åœºæ™¯è¥é”€æ™ºèƒ½å†³ç­–å¼•æ“ - ç»Ÿä¸€å…¥å£
    
    é›†æˆæ‰€æœ‰å­æ¨¡å‹ï¼Œæä¾›ä¸€ç«™å¼åœºæ™¯è¥é”€åˆ†æ
    """
    
    def __init__(self):
        self.product_miner = ProductCombinationMiner()
        self.scene_model = SceneRecognitionModel()
        self.rfm_segment = RFMCustomerSegmentation()
        self.rule_tree = SceneDecisionTreeRules()
        
        self.analysis_results = {}
        
    def run_full_analysis(self, order_data: pd.DataFrame) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´åˆ†ææµç¨‹
        
        Args:
            order_data: è®¢å•æ˜ç»†æ•°æ®
            
        Returns:
            æ‰€æœ‰åˆ†æç»“æœçš„æ±‡æ€»
        """
        print("=" * 80)
        print("ğŸš€ åœºæ™¯è¥é”€æ™ºèƒ½å†³ç­–å¼•æ“ - å…¨æµç¨‹åˆ†æ")
        print("=" * 80)
        
        results = {}
        
        # 1. å•†å“ç»„åˆæŒ–æ˜
        print("\nã€1/5ã€‘å•†å“ç»„åˆæŒ–æ˜...")
        try:
            combo_result = self.product_miner.mine_from_orders(order_data)
            results['product_combinations'] = combo_result
            print(f"âœ… å®Œæˆï¼š{combo_result.get('stats', {}).get('rules_count', 0)}æ¡å…³è”è§„åˆ™")
        except Exception as e:
            print(f"âŒ å•†å“ç»„åˆæŒ–æ˜å¤±è´¥: {e}")
            results['product_combinations'] = {'status': 'error', 'message': str(e)}
        
        # 2. åœºæ™¯è¯†åˆ«æ¨¡å‹
        print("\nã€2/5ã€‘åœºæ™¯è¯†åˆ«æ¨¡å‹è®­ç»ƒ...")
        try:
            scene_result = self.scene_model.train(order_data)
            results['scene_recognition'] = scene_result
            print(f"âœ… å®Œæˆï¼šæµ‹è¯•å‡†ç¡®ç‡ {scene_result.get('test_score', 0):.3f}")
        except Exception as e:
            print(f"âŒ åœºæ™¯è¯†åˆ«å¤±è´¥: {e}")
            results['scene_recognition'] = {'status': 'error', 'message': str(e)}
        
        # 3. å®¢æˆ·åˆ†ç¾¤
        print("\nã€3/5ã€‘RFMå®¢æˆ·åˆ†ç¾¤...")
        try:
            self.rfm_segment.calculate_rfm(order_data)
            segment_result = self.rfm_segment.segment_customers()
            results['customer_segmentation'] = segment_result
            print(f"âœ… å®Œæˆï¼š{segment_result.get('n_clusters', 0)}ä¸ªå®¢æˆ·ç¾¤ç»„")
        except Exception as e:
            print(f"âŒ å®¢æˆ·åˆ†ç¾¤å¤±è´¥: {e}")
            results['customer_segmentation'] = {'status': 'error', 'message': str(e)}
        
        # 4. å†³ç­–æ ‘è§„åˆ™
        print("\nã€4/5ã€‘å†³ç­–æ ‘è§„åˆ™ç”Ÿæˆ...")
        try:
            rule_result = self.rule_tree.train_rule_tree(order_data)
            results['decision_rules'] = rule_result
            print(f"âœ… å®Œæˆï¼šç”Ÿæˆ {rule_result.get('n_leaves', 0)} ä¸ªè§„åˆ™èŠ‚ç‚¹")
        except Exception as e:
            print(f"âŒ å†³ç­–æ ‘è§„åˆ™ç”Ÿæˆå¤±è´¥: {e}")
            results['decision_rules'] = {'status': 'error', 'message': str(e)}
        
        # 5. åœºæ™¯é¢„æµ‹
        print("\nã€5/5ã€‘è®¢å•åœºæ™¯é¢„æµ‹...")
        try:
            if self.scene_model.is_trained:
                scene_predictions = self.scene_model.predict_scene(order_data)
                results['scene_predictions'] = scene_predictions
                print(f"âœ… å®Œæˆï¼šé¢„æµ‹ {len(scene_predictions)} ä¸ªè®¢å•åœºæ™¯")
            else:
                results['scene_predictions'] = None
        except Exception as e:
            print(f"âŒ åœºæ™¯é¢„æµ‹å¤±è´¥: {e}")
            results['scene_predictions'] = None
        
        print("\n" + "=" * 80)
        print("ğŸ‰ å…¨æµç¨‹åˆ†æå®Œæˆï¼")
        print("=" * 80)
        
        self.analysis_results = results
        return results
    
    def get_summary_report(self) -> str:
        """
        ç”Ÿæˆåˆ†ææ‘˜è¦æŠ¥å‘Š
        """
        if not self.analysis_results:
            return "å°šæœªè¿è¡Œåˆ†æï¼Œè¯·å…ˆè°ƒç”¨run_full_analysis()"
        
        report = []
        report.append("=" * 80)
        report.append("ğŸ“Š åœºæ™¯è¥é”€æ™ºèƒ½å†³ç­–æŠ¥å‘Š")
        report.append("=" * 80)
        report.append("")
        
        # 1. å•†å“ç»„åˆ
        if 'product_combinations' in self.analysis_results:
            combo = self.analysis_results['product_combinations']
            if combo.get('status') == 'success':
                stats = combo.get('stats', {})
                report.append(f"ã€å•†å“ç»„åˆæŒ–æ˜ã€‘")
                report.append(f"  â€¢ åˆ†æè®¢å•æ•°: {stats.get('total_baskets', 0)}")
                report.append(f"  â€¢ é¢‘ç¹é¡¹é›†: {stats.get('frequent_itemsets_count', 0)}")
                report.append(f"  â€¢ å…³è”è§„åˆ™: {stats.get('rules_count', 0)}")
                
                scene_pkgs = combo.get('scene_packages', {})
                if scene_pkgs:
                    report.append(f"  â€¢ è¯†åˆ«åœºæ™¯å¥—é¤: {', '.join(scene_pkgs.keys())}")
                report.append("")
        
        # 2. åœºæ™¯è¯†åˆ«
        if 'scene_recognition' in self.analysis_results:
            scene = self.analysis_results['scene_recognition']
            if scene.get('status') == 'success':
                report.append(f"ã€åœºæ™¯è¯†åˆ«æ¨¡å‹ã€‘")
                report.append(f"  â€¢ è®­ç»ƒå‡†ç¡®ç‡: {scene.get('train_score', 0):.1%}")
                report.append(f"  â€¢ æµ‹è¯•å‡†ç¡®ç‡: {scene.get('test_score', 0):.1%}")
                
                dist = scene.get('scene_distribution', {})
                if dist:
                    report.append(f"  â€¢ åœºæ™¯åˆ†å¸ƒ:")
                    for scene_name, count in dist.items():
                        report.append(f"    - {scene_name}: {count}")
                report.append("")
        
        # 3. å®¢æˆ·åˆ†ç¾¤
        if 'customer_segmentation' in self.analysis_results:
            seg = self.analysis_results['customer_segmentation']
            if seg.get('status') == 'success':
                report.append(f"ã€å®¢æˆ·åˆ†ç¾¤ã€‘")
                report.append(f"  â€¢ ç¾¤ç»„æ•°é‡: {seg.get('n_clusters', 0)}")
                report.append(f"  â€¢ è½®å»“ç³»æ•°: {seg.get('silhouette_score', 0):.3f}")
                
                profiles = seg.get('cluster_profiles', {})
                if profiles:
                    report.append(f"  â€¢ å®¢æˆ·ç¾¤ç»„:")
                    for cluster_id, profile in profiles.items():
                        report.append(f"    - {profile['name']}: {profile['size']}äºº ({profile['percentage']:.1f}%)")
                report.append("")
        
        # 4. å†³ç­–è§„åˆ™
        if 'decision_rules' in self.analysis_results:
            rules = self.analysis_results['decision_rules']
            if rules.get('status') == 'success':
                report.append(f"ã€å†³ç­–æ ‘è§„åˆ™ã€‘")
                report.append(f"  â€¢ å‡†ç¡®ç‡: {rules.get('accuracy', 0):.1%}")
                report.append(f"  â€¢ æ ‘æ·±åº¦: {rules.get('tree_depth', 0)}")
                report.append(f"  â€¢ è§„åˆ™èŠ‚ç‚¹: {rules.get('n_leaves', 0)}")
                report.append("")
        
        report.append("=" * 80)
        
        return "\n".join(report)


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    print("åœºæ™¯è¥é”€æ™ºèƒ½å†³ç­–å¼•æ“ - æ¨¡å—æµ‹è¯•")
    print("=" * 80)
    print("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
    print("")
    print("å¯ç”¨æ¨¡å—:")
    print("  1. ProductCombinationMiner - å•†å“ç»„åˆæŒ–æ˜")
    print("  2. SceneRecognitionModel - åœºæ™¯è¯†åˆ«æ¨¡å‹")
    print("  3. RFMCustomerSegmentation - å®¢æˆ·åˆ†ç¾¤")
    print("  4. SceneDecisionTreeRules - å†³ç­–æ ‘è§„åˆ™")
    print("  5. SceneMarketingIntelligence - ç»Ÿä¸€å¼•æ“")
    print("")
    print("ä½¿ç”¨ç¤ºä¾‹:")
    print("  engine = SceneMarketingIntelligence()")
    print("  results = engine.run_full_analysis(order_data)")
    print("=" * 80)
