#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªé€‚åº”å­¦ä¹ å¼•æ“ - AIæ¨¡å‹æŒç»­ä¼˜åŒ–ç³»ç»Ÿ
æ”¯æŒåœ¨çº¿å­¦ä¹ ã€æ¨¡å‹ç‰ˆæœ¬ç®¡ç†ã€æ€§èƒ½ç›‘æ§å’Œæ™ºèƒ½è°ƒå‚
"""

import os
import json
import pickle
import joblib
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import SGDRegressor, PassiveAggressiveRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdaptiveLearningEngine:
    """è‡ªé€‚åº”å­¦ä¹ å¼•æ“"""
    
    def __init__(self, model_dir: str = "æ™ºèƒ½æ¨¡å‹ä»“åº“"):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”å­¦ä¹ å¼•æ“
        
        Args:
            model_dir: æ¨¡å‹å­˜å‚¨ç›®å½•
        """
        self.model_dir = model_dir
        self.ensure_model_directory()
        
        # å­¦ä¹ å†å²å­˜å‚¨
        self.learning_history_file = os.path.join(model_dir, "learning_history.json")
        self.model_performance_file = os.path.join(model_dir, "model_performance.json")
        self.feature_importance_file = os.path.join(model_dir, "feature_importance.json")
        
        # åˆå§‹åŒ–åœ¨çº¿å­¦ä¹ æ¨¡å‹
        self.online_models = {
            'sales_predictor': SGDRegressor(learning_rate='adaptive', eta0=0.01, max_iter=1000),
            'profit_predictor': PassiveAggressiveRegressor(max_iter=1000, random_state=42),
            'demand_predictor': SGDRegressor(learning_rate='invscaling', eta0=0.1, max_iter=1000),
            'price_optimizer': SGDRegressor(learning_rate='constant', eta0=0.05, max_iter=1000)
        }
        
        # æ‰¹é‡å­¦ä¹ æ¨¡å‹
        self.batch_models = {
            'sales_predictor': RandomForestRegressor(n_estimators=100, random_state=42),
            'profit_predictor': GradientBoostingRegressor(n_estimators=100, random_state=42),
            'demand_predictor': RandomForestRegressor(n_estimators=80, random_state=42),
            'price_optimizer': GradientBoostingRegressor(n_estimators=80, random_state=42)
        }
        
        # ç‰¹å¾é¢„å¤„ç†å™¨
        self.scalers = {}
        self.label_encoders = {}
        
        # å­¦ä¹ å‚æ•°
        self.learning_config = {
            'batch_size': 50,
            'learning_rate_decay': 0.95,
            'performance_threshold': 0.85,
            'retrain_threshold': 0.1,
            'max_memory_samples': 10000
        }
        
        # åŠ è½½å·²æœ‰æ¨¡å‹å’Œå†å²
        self.load_existing_models()
        self.learning_history = self.load_learning_history()
        self.model_performance = self.load_model_performance()
        
        logger.info("è‡ªé€‚åº”å­¦ä¹ å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def ensure_model_directory(self):
        """ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.model_dir):
            os.makedirs(self.model_dir)
        
        # åˆ›å»ºå­ç›®å½•
        subdirs = ['online_models', 'batch_models', 'scalers', 'archives']
        for subdir in subdirs:
            path = os.path.join(self.model_dir, subdir)
            if not os.path.exists(path):
                os.makedirs(path)
    
    def load_existing_models(self):
        """åŠ è½½å·²å­˜åœ¨çš„æ¨¡å‹"""
        try:
            # åŠ è½½åœ¨çº¿æ¨¡å‹
            online_models_dir = os.path.join(self.model_dir, 'online_models')
            for model_name in self.online_models.keys():
                model_path = os.path.join(online_models_dir, f'{model_name}.joblib')
                if os.path.exists(model_path):
                    self.online_models[model_name] = joblib.load(model_path)
                    logger.info(f"åŠ è½½åœ¨çº¿æ¨¡å‹: {model_name}")
            
            # åŠ è½½æ‰¹é‡æ¨¡å‹
            batch_models_dir = os.path.join(self.model_dir, 'batch_models')
            for model_name in self.batch_models.keys():
                model_path = os.path.join(batch_models_dir, f'{model_name}.joblib')
                if os.path.exists(model_path):
                    self.batch_models[model_name] = joblib.load(model_path)
                    logger.info(f"åŠ è½½æ‰¹é‡æ¨¡å‹: {model_name}")
            
            # åŠ è½½é¢„å¤„ç†å™¨
            scalers_dir = os.path.join(self.model_dir, 'scalers')
            for file_name in os.listdir(scalers_dir):
                if file_name.endswith('.joblib'):
                    name = file_name.replace('.joblib', '')
                    if 'scaler' in name:
                        self.scalers[name] = joblib.load(os.path.join(scalers_dir, file_name))
                    elif 'encoder' in name:
                        self.label_encoders[name] = joblib.load(os.path.join(scalers_dir, file_name))
                        
        except Exception as e:
            logger.warning(f"åŠ è½½å·²æœ‰æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
    
    def save_models(self):
        """ä¿å­˜æ‰€æœ‰æ¨¡å‹"""
        try:
            # ä¿å­˜åœ¨çº¿æ¨¡å‹
            online_models_dir = os.path.join(self.model_dir, 'online_models')
            for model_name, model in self.online_models.items():
                model_path = os.path.join(online_models_dir, f'{model_name}.joblib')
                joblib.dump(model, model_path)
            
            # ä¿å­˜æ‰¹é‡æ¨¡å‹
            batch_models_dir = os.path.join(self.model_dir, 'batch_models')
            for model_name, model in self.batch_models.items():
                model_path = os.path.join(batch_models_dir, f'{model_name}.joblib')
                joblib.dump(model, model_path)
            
            # ä¿å­˜é¢„å¤„ç†å™¨
            scalers_dir = os.path.join(self.model_dir, 'scalers')
            for name, scaler in self.scalers.items():
                scaler_path = os.path.join(scalers_dir, f'{name}.joblib')
                joblib.dump(scaler, scaler_path)
            
            for name, encoder in self.label_encoders.items():
                encoder_path = os.path.join(scalers_dir, f'{name}.joblib')
                joblib.dump(encoder, encoder_path)
                
            logger.info("æ¨¡å‹ä¿å­˜å®Œæˆ")
                
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
    
    def load_learning_history(self) -> List[Dict]:
        """åŠ è½½å­¦ä¹ å†å²"""
        try:
            if os.path.exists(self.learning_history_file):
                with open(self.learning_history_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½å­¦ä¹ å†å²å¤±è´¥: {str(e)}")
        return []
    
    def save_learning_history(self):
        """ä¿å­˜å­¦ä¹ å†å²"""
        try:
            with open(self.learning_history_file, 'w', encoding='utf-8') as f:
                json.dump(self.learning_history, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜å­¦ä¹ å†å²å¤±è´¥: {str(e)}")
    
    def load_model_performance(self) -> Dict:
        """åŠ è½½æ¨¡å‹æ€§èƒ½è®°å½•"""
        try:
            if os.path.exists(self.model_performance_file):
                with open(self.model_performance_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½æ€§èƒ½è®°å½•å¤±è´¥: {str(e)}")
        return {}
    
    def save_model_performance(self):
        """ä¿å­˜æ¨¡å‹æ€§èƒ½è®°å½•"""
        try:
            with open(self.model_performance_file, 'w', encoding='utf-8') as f:
                json.dump(self.model_performance, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜æ€§èƒ½è®°å½•å¤±è´¥: {str(e)}")
    
    def extract_learning_features(self, analysis_data: Dict) -> pd.DataFrame:
        """
        ä»åˆ†ææ•°æ®ä¸­æå–å­¦ä¹ ç‰¹å¾
        
        Args:
            analysis_data: åˆ†ææ•°æ®å­—å…¸
            
        Returns:
            ç‰¹å¾DataFrame
        """
        features_list = []
        
        try:
            # ä»äº§å“æ•°æ®æå–ç‰¹å¾
            if 'product_data' in analysis_data and isinstance(analysis_data['product_data'], pd.DataFrame):
                product_df = analysis_data['product_data']
                
                for _, row in product_df.iterrows():
                    feature = {
                        'timestamp': datetime.now().isoformat(),
                        'product_name': str(row.get('å•†å“åç§°', 'unknown')),
                        'price': float(row.get('å”®ä»·', 0)),
                        'original_price': float(row.get('åŸä»·', 0)),
                        'monthly_sales': int(row.get('æœˆå”®', 0)),
                        'stock': int(row.get('åº“å­˜', 0)),
                        'category_l1': str(row.get('ç¾å›¢ä¸€çº§åˆ†ç±»', 'unknown')),
                        'category_l3': str(row.get('ç¾å›¢ä¸‰çº§åˆ†ç±»', 'unknown')),
                        'discount_rate': 1 - (float(row.get('å”®ä»·', 0)) / max(float(row.get('åŸä»·', 1)), 1)),
                        'stock_turnover': float(row.get('æœˆå”®', 0)) / max(float(row.get('åº“å­˜', 1)), 1)
                    }
                    features_list.append(feature)
            
            # æ·»åŠ æ—¶é—´ç‰¹å¾
            now = datetime.now()
            time_features = {
                'hour': now.hour,
                'day_of_week': now.weekday(),
                'day_of_month': now.day,
                'month': now.month,
                'is_weekend': 1 if now.weekday() >= 5 else 0,
                'is_peak_hour': 1 if now.hour in [11, 12, 18, 19, 20] else 0
            }
            
            # ä¸ºæ¯ä¸ªäº§å“ç‰¹å¾æ·»åŠ æ—¶é—´ç‰¹å¾
            for feature in features_list:
                feature.update(time_features)
            
            df = pd.DataFrame(features_list)
            
            if len(df) > 0:
                logger.info(f"æå–åˆ° {len(df)} æ¡å­¦ä¹ ç‰¹å¾")
            
            return df
            
        except Exception as e:
            logger.error(f"ç‰¹å¾æå–å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def prepare_features_and_targets(self, features_df: pd.DataFrame) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
        """
        å‡†å¤‡è®­ç»ƒç‰¹å¾å’Œç›®æ ‡å˜é‡
        
        Args:
            features_df: ç‰¹å¾DataFrame
            
        Returns:
            åŒ…å«ä¸åŒé¢„æµ‹ä»»åŠ¡çš„ç‰¹å¾å’Œç›®æ ‡å­—å…¸
        """
        if len(features_df) == 0:
            return {}
        
        try:
            # å¤„ç†åˆ†ç±»ç‰¹å¾
            categorical_columns = ['category_l1', 'category_l3']
            for col in categorical_columns:
                if col in features_df.columns:
                    encoder_name = f'{col}_encoder'
                    if encoder_name not in self.label_encoders:
                        self.label_encoders[encoder_name] = LabelEncoder()
                        features_df[col] = self.label_encoders[encoder_name].fit_transform(features_df[col].astype(str))
                    else:
                        # å¤„ç†æ–°ç±»åˆ«
                        known_classes = set(self.label_encoders[encoder_name].classes_)
                        new_values = []
                        for val in features_df[col].astype(str):
                            if val in known_classes:
                                new_values.append(val)
                            else:
                                new_values.append('unknown' if 'unknown' in known_classes else self.label_encoders[encoder_name].classes_[0])
                        features_df[col] = self.label_encoders[encoder_name].transform(new_values)
            
            # é€‰æ‹©æ•°å€¼ç‰¹å¾
            numeric_features = [
                'price', 'original_price', 'discount_rate', 'stock_turnover',
                'hour', 'day_of_week', 'day_of_month', 'month', 'is_weekend', 'is_peak_hour',
                'category_l1', 'category_l3'
            ]
            
            available_features = [col for col in numeric_features if col in features_df.columns]
            
            if len(available_features) == 0:
                logger.warning("æ²¡æœ‰å¯ç”¨çš„æ•°å€¼ç‰¹å¾")
                return {}
            
            X = features_df[available_features].fillna(0).values
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            scaler_name = 'main_scaler'
            if scaler_name not in self.scalers:
                self.scalers[scaler_name] = StandardScaler()
                X = self.scalers[scaler_name].fit_transform(X)
            else:
                X = self.scalers[scaler_name].transform(X)
            
            # æ„å»ºä¸åŒçš„ç›®æ ‡å˜é‡
            targets = {}
            
            # é”€é‡é¢„æµ‹ç›®æ ‡
            if 'monthly_sales' in features_df.columns:
                targets['sales_predictor'] = features_df['monthly_sales'].fillna(0).values
            
            # åˆ©æ¶¦é¢„æµ‹ç›®æ ‡ï¼ˆåŸºäºä»·æ ¼å’Œé”€é‡ï¼‰
            if 'price' in features_df.columns and 'monthly_sales' in features_df.columns:
                profit_estimate = features_df['price'] * features_df['monthly_sales'] * 0.3  # å‡è®¾30%æ¯›åˆ©ç‡
                targets['profit_predictor'] = profit_estimate.fillna(0).values
            
            # éœ€æ±‚é¢„æµ‹ç›®æ ‡ï¼ˆåŸºäºé”€é‡å’Œåº“å­˜å‘¨è½¬ï¼‰
            if 'stock_turnover' in features_df.columns:
                targets['demand_predictor'] = features_df['stock_turnover'].fillna(0).values
            
            # ä»·æ ¼ä¼˜åŒ–ç›®æ ‡ï¼ˆåŸºäºæŠ˜æ‰£ç‡å’Œé”€é‡çš„æƒè¡¡ï¼‰
            if 'discount_rate' in features_df.columns and 'monthly_sales' in features_df.columns:
                # ä»·æ ¼æ•ˆç‡ = é”€é‡ * (1 - æŠ˜æ‰£ç‡)
                price_efficiency = features_df['monthly_sales'] * (1 - features_df['discount_rate'])
                targets['price_optimizer'] = price_efficiency.fillna(0).values
            
            result = {}
            for target_name, y in targets.items():
                if len(y) > 0:
                    result[target_name] = (X, y)
            
            logger.info(f"å‡†å¤‡äº† {len(result)} ä¸ªé¢„æµ‹ä»»åŠ¡çš„æ•°æ®")
            return result
            
        except Exception as e:
            logger.error(f"å‡†å¤‡è®­ç»ƒæ•°æ®å¤±è´¥: {str(e)}")
            return {}
    
    def online_learning_update(self, analysis_data: Dict, feedback_data: Optional[Dict] = None):
        """
        åœ¨çº¿å­¦ä¹ æ›´æ–°
        
        Args:
            analysis_data: åˆ†ææ•°æ®
            feedback_data: åé¦ˆæ•°æ®ï¼ˆå®é™…ç»“æœï¼‰
        """
        try:
            # æå–ç‰¹å¾
            features_df = self.extract_learning_features(analysis_data)
            
            if len(features_df) == 0:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾æ•°æ®è¿›è¡Œå­¦ä¹ ")
                return
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            training_data = self.prepare_features_and_targets(features_df)
            
            if not training_data:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
                return
            
            # åœ¨çº¿æ›´æ–°æ¯ä¸ªæ¨¡å‹
            for model_name, (X, y) in training_data.items():
                if model_name in self.online_models and len(X) > 0:
                    try:
                        # å¯¹äºåœ¨çº¿å­¦ä¹ ï¼Œæˆ‘ä»¬ä½¿ç”¨éƒ¨åˆ†æ‹Ÿåˆ
                        if hasattr(self.online_models[model_name], 'partial_fit'):
                            self.online_models[model_name].partial_fit(X, y)
                        else:
                            # å¦‚æœæ¨¡å‹ä¸æ”¯æŒpartial_fitï¼Œä½¿ç”¨å¢é‡è®­ç»ƒ
                            self.online_models[model_name].fit(X, y)
                        
                        logger.info(f"åœ¨çº¿æ›´æ–°æ¨¡å‹: {model_name}, æ ·æœ¬æ•°: {len(X)}")
                        
                        # è®°å½•å­¦ä¹ å†å²
                        learning_record = {
                            'timestamp': datetime.now().isoformat(),
                            'model_name': model_name,
                            'learning_type': 'online',
                            'sample_count': len(X),
                            'feature_count': X.shape[1] if len(X.shape) > 1 else 0
                        }
                        
                        # å¦‚æœæœ‰åé¦ˆæ•°æ®ï¼Œè®¡ç®—æ€§èƒ½
                        if feedback_data and model_name in feedback_data:
                            actual_values = feedback_data[model_name]
                            if len(actual_values) == len(y):
                                mae = mean_absolute_error(actual_values, y)
                                learning_record['mae'] = float(mae)
                                learning_record['performance_improvement'] = self.calculate_performance_improvement(model_name, mae)
                        
                        self.learning_history.append(learning_record)
                        
                    except Exception as e:
                        logger.error(f"åœ¨çº¿å­¦ä¹ æ›´æ–°å¤±è´¥ {model_name}: {str(e)}")
            
            # ä¿å­˜æ›´æ–°çš„æ¨¡å‹å’Œå†å²
            self.save_models()
            self.save_learning_history()
            
            # è¯„ä¼°æ˜¯å¦éœ€è¦æ‰¹é‡é‡è®­ç»ƒ
            self.evaluate_retrain_need()
            
            logger.info("åœ¨çº¿å­¦ä¹ æ›´æ–°å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åœ¨çº¿å­¦ä¹ æ›´æ–°è¿‡ç¨‹å¤±è´¥: {str(e)}")
    
    def batch_learning_update(self, analysis_data_list: List[Dict]):
        """
        æ‰¹é‡å­¦ä¹ æ›´æ–°
        
        Args:
            analysis_data_list: åˆ†ææ•°æ®åˆ—è¡¨
        """
        try:
            if not analysis_data_list:
                logger.warning("æ²¡æœ‰æ‰¹é‡å­¦ä¹ æ•°æ®")
                return
            
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾
            all_features = []
            for data in analysis_data_list:
                features_df = self.extract_learning_features(data)
                if len(features_df) > 0:
                    all_features.append(features_df)
            
            if not all_features:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ‰¹é‡ç‰¹å¾æ•°æ®")
                return
            
            combined_features = pd.concat(all_features, ignore_index=True)
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            training_data = self.prepare_features_and_targets(combined_features)
            
            if not training_data:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ‰¹é‡è®­ç»ƒæ•°æ®")
                return
            
            # æ‰¹é‡è®­ç»ƒæ¯ä¸ªæ¨¡å‹
            for model_name, (X, y) in training_data.items():
                if model_name in self.batch_models and len(X) > 10:  # éœ€è¦è¶³å¤Ÿçš„æ ·æœ¬
                    try:
                        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®
                        X_train, X_val, y_train, y_val = train_test_split(
                            X, y, test_size=0.2, random_state=42
                        )
                        
                        # è®­ç»ƒæ¨¡å‹
                        self.batch_models[model_name].fit(X_train, y_train)
                        
                        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                        y_pred = self.batch_models[model_name].predict(X_val)
                        mae = mean_absolute_error(y_val, y_pred)
                        mse = mean_squared_error(y_val, y_pred)
                        r2 = r2_score(y_val, y_pred)
                        
                        # æ›´æ–°æ€§èƒ½è®°å½•
                        if model_name not in self.model_performance:
                            self.model_performance[model_name] = []
                        
                        performance_record = {
                            'timestamp': datetime.now().isoformat(),
                            'training_type': 'batch',
                            'sample_count': len(X_train),
                            'mae': float(mae),
                            'mse': float(mse),
                            'r2': float(r2),
                            'feature_count': X.shape[1]
                        }
                        
                        self.model_performance[model_name].append(performance_record)
                        
                        logger.info(f"æ‰¹é‡è®­ç»ƒå®Œæˆ: {model_name}, MAE: {mae:.4f}, R2: {r2:.4f}")
                        
                    except Exception as e:
                        logger.error(f"æ‰¹é‡è®­ç»ƒå¤±è´¥ {model_name}: {str(e)}")
            
            # ä¿å­˜æ¨¡å‹å’Œæ€§èƒ½è®°å½•
            self.save_models()
            self.save_model_performance()
            
            # è®°å½•æ‰¹é‡å­¦ä¹ å†å²
            batch_record = {
                'timestamp': datetime.now().isoformat(),
                'learning_type': 'batch',
                'total_samples': len(combined_features),
                'models_updated': list(training_data.keys()),
                'data_sources': len(analysis_data_list)
            }
            self.learning_history.append(batch_record)
            self.save_learning_history()
            
            logger.info("æ‰¹é‡å­¦ä¹ æ›´æ–°å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å­¦ä¹ æ›´æ–°è¿‡ç¨‹å¤±è´¥: {str(e)}")
    
    def predict_with_ensemble(self, analysis_data: Dict) -> Dict[str, Any]:
        """
        ä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        Args:
            analysis_data: åˆ†ææ•°æ®
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        try:
            # æå–ç‰¹å¾
            features_df = self.extract_learning_features(analysis_data)
            
            if len(features_df) == 0:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾æ•°æ®è¿›è¡Œé¢„æµ‹")
                return {}
            
            # å‡†å¤‡é¢„æµ‹æ•°æ®
            training_data = self.prepare_features_and_targets(features_df)
            
            if not training_data:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹æ•°æ®")
                return {}
            
            predictions = {}
            
            for model_name, (X, _) in training_data.items():
                try:
                    # åœ¨çº¿æ¨¡å‹é¢„æµ‹
                    online_pred = None
                    if model_name in self.online_models:
                        if hasattr(self.online_models[model_name], 'predict'):
                            online_pred = self.online_models[model_name].predict(X)
                    
                    # æ‰¹é‡æ¨¡å‹é¢„æµ‹
                    batch_pred = None
                    if model_name in self.batch_models:
                        if hasattr(self.batch_models[model_name], 'predict'):
                            batch_pred = self.batch_models[model_name].predict(X)
                    
                    # é›†æˆé¢„æµ‹ç»“æœ
                    if online_pred is not None and batch_pred is not None:
                        # åŠ æƒå¹³å‡ï¼ˆåœ¨çº¿æ¨¡å‹æƒé‡è¾ƒä½ï¼Œæ‰¹é‡æ¨¡å‹æƒé‡è¾ƒé«˜ï¼‰
                        ensemble_pred = 0.3 * online_pred + 0.7 * batch_pred
                    elif online_pred is not None:
                        ensemble_pred = online_pred
                    elif batch_pred is not None:
                        ensemble_pred = batch_pred
                    else:
                        continue
                    
                    # è®¡ç®—é¢„æµ‹ç»Ÿè®¡ä¿¡æ¯
                    pred_stats = {
                        'mean': float(np.mean(ensemble_pred)),
                        'std': float(np.std(ensemble_pred)),
                        'min': float(np.min(ensemble_pred)),
                        'max': float(np.max(ensemble_pred)),
                        'median': float(np.median(ensemble_pred)),
                        'predictions': ensemble_pred.tolist() if len(ensemble_pred) <= 100 else ensemble_pred[:100].tolist()
                    }
                    
                    predictions[model_name] = pred_stats
                    
                    logger.info(f"æ¨¡å‹ {model_name} é¢„æµ‹å®Œæˆï¼Œå‡å€¼: {pred_stats['mean']:.4f}")
                    
                except Exception as e:
                    logger.error(f"æ¨¡å‹ {model_name} é¢„æµ‹å¤±è´¥: {str(e)}")
            
            # æ·»åŠ é¢„æµ‹å…ƒä¿¡æ¯
            predictions['meta'] = {
                'prediction_time': datetime.now().isoformat(),
                'feature_count': X.shape[1] if len(list(training_data.values())) > 0 else 0,
                'sample_count': len(features_df),
                'models_used': list(predictions.keys())
            }
            
            return predictions
            
        except Exception as e:
            logger.error(f"é›†æˆé¢„æµ‹å¤±è´¥: {str(e)}")
            return {}
    
    def calculate_performance_improvement(self, model_name: str, current_mae: float) -> float:
        """è®¡ç®—æ€§èƒ½æ”¹å–„ç¨‹åº¦"""
        try:
            if model_name in self.model_performance and self.model_performance[model_name]:
                recent_performances = [record['mae'] for record in self.model_performance[model_name][-5:]]
                if recent_performances:
                    avg_historical_mae = np.mean(recent_performances)
                    improvement = (avg_historical_mae - current_mae) / avg_historical_mae
                    return float(improvement)
        except Exception:
            pass
        return 0.0
    
    def evaluate_retrain_need(self) -> bool:
        """è¯„ä¼°æ˜¯å¦éœ€è¦é‡æ–°è®­ç»ƒ"""
        try:
            # æ£€æŸ¥å­¦ä¹ å†å²ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ‰¹é‡é‡è®­ç»ƒ
            recent_online_updates = [
                record for record in self.learning_history[-50:] 
                if record.get('learning_type') == 'online'
            ]
            
            if len(recent_online_updates) >= self.learning_config['batch_size']:
                logger.info("æ£€æµ‹åˆ°è¶³å¤Ÿçš„åœ¨çº¿æ›´æ–°ï¼Œå»ºè®®è¿›è¡Œæ‰¹é‡é‡è®­ç»ƒ")
                return True
            
            # æ£€æŸ¥æ€§èƒ½ä¸‹é™
            for model_name, performance_history in self.model_performance.items():
                if len(performance_history) >= 3:
                    recent_mae = [record['mae'] for record in performance_history[-3:]]
                    if len(recent_mae) >= 2:
                        performance_trend = (recent_mae[-1] - recent_mae[0]) / recent_mae[0]
                        if performance_trend > self.learning_config['retrain_threshold']:
                            logger.warning(f"æ¨¡å‹ {model_name} æ€§èƒ½ä¸‹é™ {performance_trend:.2%}ï¼Œå»ºè®®é‡è®­ç»ƒ")
                            return True
            
        except Exception as e:
            logger.error(f"è¯„ä¼°é‡è®­ç»ƒéœ€æ±‚å¤±è´¥: {str(e)}")
        
        return False
    
    def get_learning_statistics(self) -> Dict[str, Any]:
        """è·å–å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                'total_learning_sessions': len(self.learning_history),
                'online_updates': len([r for r in self.learning_history if r.get('learning_type') == 'online']),
                'batch_updates': len([r for r in self.learning_history if r.get('learning_type') == 'batch']),
                'models_count': {
                    'online': len(self.online_models),
                    'batch': len(self.batch_models)
                },
                'learning_timeline': []
            }
            
            # æœ€è¿‘7å¤©çš„å­¦ä¹ æ´»åŠ¨
            week_ago = datetime.now() - timedelta(days=7)
            recent_activities = [
                record for record in self.learning_history
                if datetime.fromisoformat(record['timestamp']) > week_ago
            ]
            
            stats['recent_activity'] = {
                'total_sessions': len(recent_activities),
                'online_sessions': len([r for r in recent_activities if r.get('learning_type') == 'online']),
                'batch_sessions': len([r for r in recent_activities if r.get('learning_type') == 'batch'])
            }
            
            # æ¨¡å‹æ€§èƒ½è¶‹åŠ¿
            performance_trends = {}
            for model_name, performance_history in self.model_performance.items():
                if performance_history:
                    recent_performance = performance_history[-5:] if len(performance_history) >= 5 else performance_history
                    mae_trend = [record['mae'] for record in recent_performance]
                    
                    if len(mae_trend) >= 2:
                        trend_direction = "improving" if mae_trend[-1] < mae_trend[0] else "declining"
                        trend_rate = abs((mae_trend[-1] - mae_trend[0]) / mae_trend[0])
                    else:
                        trend_direction = "stable"
                        trend_rate = 0.0
                    
                    performance_trends[model_name] = {
                        'direction': trend_direction,
                        'rate': float(trend_rate),
                        'current_mae': float(mae_trend[-1]) if mae_trend else 0.0,
                        'sample_count': len(performance_history)
                    }
            
            stats['performance_trends'] = performance_trends
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}
    
    def export_learning_report(self) -> str:
        """å¯¼å‡ºå­¦ä¹ æŠ¥å‘Š"""
        try:
            stats = self.get_learning_statistics()
            
            report = f"""
# æ™ºèƒ½æ¨¡å‹å­¦ä¹ æŠ¥å‘Š

## å­¦ä¹ æ¦‚å†µ
- æ€»å­¦ä¹ ä¼šè¯: {stats.get('total_learning_sessions', 0)} æ¬¡
- åœ¨çº¿æ›´æ–°: {stats.get('online_updates', 0)} æ¬¡
- æ‰¹é‡æ›´æ–°: {stats.get('batch_updates', 0)} æ¬¡
- æ´»è·ƒæ¨¡å‹æ•°: åœ¨çº¿æ¨¡å‹ {stats.get('models_count', {}).get('online', 0)} ä¸ªï¼Œæ‰¹é‡æ¨¡å‹ {stats.get('models_count', {}).get('batch', 0)} ä¸ª

## è¿‘æœŸå­¦ä¹ æ´»åŠ¨ï¼ˆæœ€è¿‘7å¤©ï¼‰
- å­¦ä¹ ä¼šè¯: {stats.get('recent_activity', {}).get('total_sessions', 0)} æ¬¡
- åœ¨çº¿å­¦ä¹ : {stats.get('recent_activity', {}).get('online_sessions', 0)} æ¬¡
- æ‰¹é‡å­¦ä¹ : {stats.get('recent_activity', {}).get('batch_sessions', 0)} æ¬¡

## æ¨¡å‹æ€§èƒ½è¶‹åŠ¿
"""
            
            for model_name, trend_info in stats.get('performance_trends', {}).items():
                direction_emoji = "ğŸ“ˆ" if trend_info['direction'] == 'improving' else "ğŸ“‰" if trend_info['direction'] == 'declining' else "â¡ï¸"
                report += f"""
### {model_name}
- è¶‹åŠ¿: {direction_emoji} {trend_info['direction']} ({trend_info['rate']:.2%})
- å½“å‰è¯¯å·®: {trend_info['current_mae']:.4f}
- è®­ç»ƒæ ·æœ¬: {trend_info['sample_count']} æ‰¹æ¬¡
"""
            
            report += f"""
## å»ºè®®
- å­¦ä¹ å¼•æ“è¿è¡ŒçŠ¶æ€: {'ğŸŸ¢ è‰¯å¥½' if stats.get('total_learning_sessions', 0) > 0 else 'ğŸŸ¡ éœ€è¦æ›´å¤šæ•°æ®'}
- é‡è®­ç»ƒå»ºè®®: {'ğŸ”„ å»ºè®®é‡è®­ç»ƒ' if self.evaluate_retrain_need() else 'âœ… å½“å‰æ¨¡å‹æ€§èƒ½è‰¯å¥½'}

æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
            
            # ä¿å­˜æŠ¥å‘Š
            report_path = os.path.join(self.model_dir, f"learning_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            
            logger.info(f"å­¦ä¹ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            return report_path
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºå­¦ä¹ æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return ""


def main():
    """æµ‹è¯•è‡ªé€‚åº”å­¦ä¹ å¼•æ“"""
    # åˆ›å»ºå­¦ä¹ å¼•æ“
    engine = AdaptiveLearningEngine()
    
    # æ¨¡æ‹Ÿåˆ†ææ•°æ®
    sample_data = {
        'product_data': pd.DataFrame({
            'å•†å“åç§°': ['æµ‹è¯•å•†å“1', 'æµ‹è¯•å•†å“2'],
            'å”®ä»·': [10.0, 20.0],
            'åŸä»·': [15.0, 25.0],
            'æœˆå”®': [100, 200],
            'åº“å­˜': [50, 80],
            'ç¾å›¢ä¸€çº§åˆ†ç±»': ['é£Ÿå“', 'é¥®å“'],
            'ç¾å›¢ä¸‰çº§åˆ†ç±»': ['é›¶é£Ÿ', 'èŒ¶é¥®æ–™']
        })
    }
    
    # åœ¨çº¿å­¦ä¹ æ›´æ–°
    print("æ‰§è¡Œåœ¨çº¿å­¦ä¹ æ›´æ–°...")
    engine.online_learning_update(sample_data)
    
    # é¢„æµ‹æµ‹è¯•
    print("æ‰§è¡Œé¢„æµ‹æµ‹è¯•...")
    predictions = engine.predict_with_ensemble(sample_data)
    print("é¢„æµ‹ç»“æœ:", json.dumps(predictions, indent=2, ensure_ascii=False))
    
    # è·å–å­¦ä¹ ç»Ÿè®¡
    print("è·å–å­¦ä¹ ç»Ÿè®¡...")
    stats = engine.get_learning_statistics()
    print("å­¦ä¹ ç»Ÿè®¡:", json.dumps(stats, indent=2, ensure_ascii=False))
    
    # å¯¼å‡ºæŠ¥å‘Š
    print("å¯¼å‡ºå­¦ä¹ æŠ¥å‘Š...")
    report_path = engine.export_learning_report()
    print(f"æŠ¥å‘Šè·¯å¾„: {report_path}")


if __name__ == "__main__":
    main()