#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å­¦ä¹ æ•°æ®ç®¡ç†ç³»ç»Ÿ - æ™ºèƒ½æ•°æ®è´¨é‡è¯„ä¼°ä¸å¼‚å¸¸æ£€æµ‹
è´Ÿè´£å­¦ä¹ æ•°æ®çš„æ”¶é›†ã€éªŒè¯ã€å­˜å‚¨å’Œç‰ˆæœ¬ç®¡ç†
"""

import os
import json
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import hashlib
import pickle
import gzip
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest
from sklearn.metrics.pairwise import cosine_similarity
import logging

logger = logging.getLogger(__name__)

class LearningDataManager:
    """å­¦ä¹ æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, data_dir: str = "å­¦ä¹ æ•°æ®ä»“åº“"):
        """
        åˆå§‹åŒ–å­¦ä¹ æ•°æ®ç®¡ç†å™¨
        
        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.data_dir = data_dir
        self.ensure_data_directory()
        
        # æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        self.db_path = os.path.join(data_dir, "learning_data.db")
        self.init_database()
        
        # æ•°æ®è´¨é‡è¯„ä¼°é…ç½®
        self.quality_config = {
            'completeness_threshold': 0.8,  # å®Œæ•´æ€§é˜ˆå€¼
            'consistency_threshold': 0.9,   # ä¸€è‡´æ€§é˜ˆå€¼
            'accuracy_threshold': 0.85,     # å‡†ç¡®æ€§é˜ˆå€¼
            'freshness_days': 30,           # æ•°æ®æ–°é²œåº¦ï¼ˆå¤©ï¼‰
            'duplicate_threshold': 0.95,    # é‡å¤æ•°æ®ç›¸ä¼¼åº¦é˜ˆå€¼
            'outlier_contamination': 0.05   # å¼‚å¸¸æ•°æ®æ±¡æŸ“ç‡
        }
        
        # å¼‚å¸¸æ£€æµ‹å™¨
        self.anomaly_detector = IsolationForest(
            contamination=self.quality_config['outlier_contamination'],
            random_state=42
        )
        
        # æ•°æ®æ ‡å‡†åŒ–å™¨
        self.data_scaler = StandardScaler()
        
        # ç¼“å­˜ç®¡ç†
        self.cache_size_limit = 1000  # æœ€å¤§ç¼“å­˜æ ·æœ¬æ•°
        self.cached_data = {}
        
        logger.info("å­¦ä¹ æ•°æ®ç®¡ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def ensure_data_directory(self):
        """ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
        
        # åˆ›å»ºå­ç›®å½•
        subdirs = ['raw_data', 'processed_data', 'quality_reports', 'backups', 'cache']
        for subdir in subdirs:
            path = os.path.join(self.data_dir, subdir)
            if not os.path.exists(path):
                os.makedirs(path)
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åˆ›å»ºæ•°æ®é›†è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS datasets (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    dataset_id TEXT UNIQUE,
                    name TEXT,
                    description TEXT,
                    created_time TEXT,
                    updated_time TEXT,
                    data_type TEXT,
                    sample_count INTEGER,
                    feature_count INTEGER,
                    quality_score REAL,
                    file_path TEXT,
                    file_size INTEGER,
                    checksum TEXT,
                    version INTEGER DEFAULT 1
                )
            ''')
            
            # åˆ›å»ºæ•°æ®è´¨é‡è¯„ä¼°è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS quality_assessments (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    dataset_id TEXT,
                    assessment_time TEXT,
                    completeness_score REAL,
                    consistency_score REAL,
                    accuracy_score REAL,
                    freshness_score REAL,
                    overall_score REAL,
                    anomalies_detected INTEGER,
                    quality_report TEXT,
                    FOREIGN KEY (dataset_id) REFERENCES datasets (dataset_id)
                )
            ''')
            
            # åˆ›å»ºå­¦ä¹ å†å²è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS learning_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    dataset_id TEXT,
                    model_name TEXT,
                    training_time TEXT,
                    sample_count INTEGER,
                    performance_metrics TEXT,
                    feature_importance TEXT,
                    training_duration REAL,
                    FOREIGN KEY (dataset_id) REFERENCES datasets (dataset_id)
                )
            ''')
            
            # åˆ›å»ºå¼‚å¸¸æ•°æ®è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS anomaly_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    dataset_id TEXT,
                    detection_time TEXT,
                    anomaly_type TEXT,
                    severity TEXT,
                    sample_indices TEXT,
                    description TEXT,
                    resolution_status TEXT DEFAULT 'pending',
                    FOREIGN KEY (dataset_id) REFERENCES datasets (dataset_id)
                )
            ''')
            
            conn.commit()
            conn.close()
            
            logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def generate_dataset_id(self, data: Any, name: str = "") -> str:
        """ç”Ÿæˆæ•°æ®é›†ID"""
        try:
            # ä½¿ç”¨æ•°æ®çš„å“ˆå¸Œå€¼å’Œæ—¶é—´æˆ³ç”ŸæˆID
            if isinstance(data, pd.DataFrame):
                data_hash = hashlib.md5(data.to_string().encode()).hexdigest()[:8]
            else:
                data_hash = hashlib.md5(str(data).encode()).hexdigest()[:8]
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            name_part = name.replace(" ", "_") if name else "dataset"
            
            return f"{name_part}_{data_hash}_{timestamp}"
        
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ•°æ®é›†IDå¤±è´¥: {str(e)}")
            return f"dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    def calculate_file_checksum(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œ"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œå¤±è´¥: {str(e)}")
            return ""
    
    def save_dataset(self, data: pd.DataFrame, name: str = "", description: str = "", data_type: str = "training") -> str:
        """
        ä¿å­˜æ•°æ®é›†
        
        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            name: æ•°æ®é›†åç§°
            description: æ•°æ®é›†æè¿°
            data_type: æ•°æ®ç±»å‹
            
        Returns:
            æ•°æ®é›†ID
        """
        try:
            dataset_id = self.generate_dataset_id(data, name)
            
            # ä¿å­˜æ•°æ®æ–‡ä»¶
            file_name = f"{dataset_id}.pkl.gz"
            file_path = os.path.join(self.data_dir, 'processed_data', file_name)
            
            with gzip.open(file_path, 'wb') as f:
                pickle.dump(data, f)
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_size = os.path.getsize(file_path)
            checksum = self.calculate_file_checksum(file_path)
            
            # è¯„ä¼°æ•°æ®è´¨é‡
            quality_assessment = self.assess_data_quality(data, dataset_id)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            current_time = datetime.now().isoformat()
            
            cursor.execute('''
                INSERT INTO datasets 
                (dataset_id, name, description, created_time, updated_time, data_type, 
                 sample_count, feature_count, quality_score, file_path, file_size, checksum)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                dataset_id, name, description, current_time, current_time, data_type,
                len(data), len(data.columns), quality_assessment['overall_score'],
                file_path, file_size, checksum
            ))
            
            conn.commit()
            conn.close()
            
            logger.info(f"æ•°æ®é›†ä¿å­˜æˆåŠŸ: {dataset_id}, æ ·æœ¬æ•°: {len(data)}, è´¨é‡è¯„åˆ†: {quality_assessment['overall_score']:.3f}")
            
            return dataset_id
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®é›†å¤±è´¥: {str(e)}")
            return ""
    
    def load_dataset(self, dataset_id: str) -> Optional[pd.DataFrame]:
        """
        åŠ è½½æ•°æ®é›†
        
        Args:
            dataset_id: æ•°æ®é›†ID
            
        Returns:
            æ•°æ®é›†DataFrame
        """
        try:
            # å…ˆæ£€æŸ¥ç¼“å­˜
            if dataset_id in self.cached_data:
                logger.info(f"ä»ç¼“å­˜åŠ è½½æ•°æ®é›†: {dataset_id}")
                return self.cached_data[dataset_id]
            
            # ä»æ•°æ®åº“è·å–æ–‡ä»¶è·¯å¾„
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT file_path, checksum FROM datasets WHERE dataset_id = ?', (dataset_id,))
            result = cursor.fetchone()
            conn.close()
            
            if not result:
                logger.warning(f"æ•°æ®é›†ä¸å­˜åœ¨: {dataset_id}")
                return None
            
            file_path, expected_checksum = result
            
            # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
            if expected_checksum:
                actual_checksum = self.calculate_file_checksum(file_path)
                if actual_checksum != expected_checksum:
                    logger.error(f"æ•°æ®é›†æ–‡ä»¶å·²æŸå: {dataset_id}")
                    return None
            
            # åŠ è½½æ•°æ®
            with gzip.open(file_path, 'rb') as f:
                data = pickle.load(f)
            
            # ç¼“å­˜ç®¡ç†
            if len(self.cached_data) >= self.cache_size_limit:
                # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                oldest_key = next(iter(self.cached_data))
                del self.cached_data[oldest_key]
            
            self.cached_data[dataset_id] = data
            
            logger.info(f"æ•°æ®é›†åŠ è½½æˆåŠŸ: {dataset_id}, æ ·æœ¬æ•°: {len(data)}")
            return data
            
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
            return None
    
    def assess_data_quality(self, data: pd.DataFrame, dataset_id: str) -> Dict[str, Any]:
        """
        è¯„ä¼°æ•°æ®è´¨é‡
        
        Args:
            data: è¦è¯„ä¼°çš„æ•°æ®
            dataset_id: æ•°æ®é›†ID
            
        Returns:
            è´¨é‡è¯„ä¼°ç»“æœ
        """
        try:
            assessment = {
                'completeness_score': 0.0,
                'consistency_score': 0.0,
                'accuracy_score': 0.0,
                'freshness_score': 0.0,
                'overall_score': 0.0,
                'anomalies_detected': 0,
                'quality_issues': [],
                'recommendations': []
            }
            
            # 1. å®Œæ•´æ€§è¯„ä¼°
            total_cells = data.size
            missing_cells = data.isnull().sum().sum()
            completeness = 1 - (missing_cells / total_cells) if total_cells > 0 else 0
            assessment['completeness_score'] = completeness
            
            if completeness < self.quality_config['completeness_threshold']:
                assessment['quality_issues'].append(f"æ•°æ®å®Œæ•´æ€§è¾ƒä½: {completeness:.2%}")
                assessment['recommendations'].append("å»ºè®®è¡¥å……ç¼ºå¤±æ•°æ®æˆ–ä½¿ç”¨æ’å€¼æ–¹æ³•")
            
            # 2. ä¸€è‡´æ€§è¯„ä¼°
            consistency_score = 1.0  # é»˜è®¤ä¸€è‡´æ€§è‰¯å¥½
            
            # æ£€æŸ¥æ•°æ®ç±»å‹ä¸€è‡´æ€§
            for column in data.columns:
                if data[column].dtype == 'object':
                    # æ£€æŸ¥å­—ç¬¦ä¸²åˆ—çš„æ ¼å¼ä¸€è‡´æ€§
                    unique_formats = data[column].dropna().apply(lambda x: type(x).__name__).unique()
                    if len(unique_formats) > 1:
                        consistency_score -= 0.1
                        assessment['quality_issues'].append(f"åˆ— {column} æ•°æ®ç±»å‹ä¸ä¸€è‡´")
            
            assessment['consistency_score'] = max(0, consistency_score)
            
            # 3. å‡†ç¡®æ€§è¯„ä¼°ï¼ˆåŸºäºç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ï¼‰
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            accuracy_score = 1.0
            total_anomalies = 0
            
            if len(numeric_columns) > 0:
                # æ•°å€¼åˆ—å¼‚å¸¸æ£€æµ‹
                numeric_data = data[numeric_columns].fillna(data[numeric_columns].mean())
                
                if len(numeric_data) > 10:  # éœ€è¦è¶³å¤Ÿçš„æ ·æœ¬è¿›è¡Œå¼‚å¸¸æ£€æµ‹
                    try:
                        anomaly_labels = self.anomaly_detector.fit_predict(numeric_data)
                        anomaly_count = np.sum(anomaly_labels == -1)
                        total_anomalies = anomaly_count
                        
                        anomaly_rate = anomaly_count / len(numeric_data)
                        accuracy_score = max(0, 1 - anomaly_rate * 2)  # å¼‚å¸¸ç‡å½±å“å‡†ç¡®æ€§
                        
                        if anomaly_rate > self.quality_config['outlier_contamination']:
                            assessment['quality_issues'].append(f"æ£€æµ‹åˆ° {anomaly_count} ä¸ªå¼‚å¸¸æ ·æœ¬ ({anomaly_rate:.1%})")
                            assessment['recommendations'].append("å»ºè®®å®¡æŸ¥å¼‚å¸¸æ•°æ®å¹¶è€ƒè™‘æ¸…ç†")
                    
                    except Exception as e:
                        logger.warning(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")
            
            assessment['accuracy_score'] = accuracy_score
            assessment['anomalies_detected'] = total_anomalies
            
            # 4. æ–°é²œåº¦è¯„ä¼°
            freshness_score = 1.0  # æ–°æ•°æ®é»˜è®¤æ–°é²œåº¦æ»¡åˆ†
            
            # å¦‚æœæ•°æ®åŒ…å«æ—¶é—´æˆ³ï¼Œæ£€æŸ¥æ•°æ®çš„æ–°é²œåº¦
            time_columns = []
            for col in data.columns:
                if 'time' in col.lower() or 'date' in col.lower() or 'æ—¶é—´' in col:
                    time_columns.append(col)
            
            if time_columns:
                try:
                    latest_time_col = time_columns[0]
                    if data[latest_time_col].dtype == 'object':
                        latest_date = pd.to_datetime(data[latest_time_col]).max()
                    else:
                        latest_date = data[latest_time_col].max()
                    
                    if pd.isna(latest_date):
                        freshness_score = 0.5
                    else:
                        days_old = (datetime.now() - pd.to_datetime(latest_date)).days
                        freshness_score = max(0, 1 - days_old / self.quality_config['freshness_days'])
                        
                        if days_old > self.quality_config['freshness_days']:
                            assessment['quality_issues'].append(f"æ•°æ®è¾ƒæ—§ï¼Œæœ€æ–°æ•°æ®è·ä»Š {days_old} å¤©")
                            assessment['recommendations'].append("å»ºè®®æ›´æ–°æ•°æ®æˆ–æ”¶é›†æ›´æ–°çš„æ•°æ®")
                
                except Exception as e:
                    logger.warning(f"æ–°é²œåº¦è¯„ä¼°å¤±è´¥: {str(e)}")
            
            assessment['freshness_score'] = freshness_score
            
            # 5. è®¡ç®—ç»¼åˆè¯„åˆ†
            weights = {
                'completeness': 0.3,
                'consistency': 0.2,
                'accuracy': 0.3,
                'freshness': 0.2
            }
            
            overall_score = (
                assessment['completeness_score'] * weights['completeness'] +
                assessment['consistency_score'] * weights['consistency'] +
                assessment['accuracy_score'] * weights['accuracy'] +
                assessment['freshness_score'] * weights['freshness']
            )
            
            assessment['overall_score'] = overall_score
            
            # ä¿å­˜è´¨é‡è¯„ä¼°ç»“æœåˆ°æ•°æ®åº“
            self.save_quality_assessment(dataset_id, assessment)
            
            logger.info(f"æ•°æ®è´¨é‡è¯„ä¼°å®Œæˆ: {dataset_id}, ç»¼åˆè¯„åˆ†: {overall_score:.3f}")
            
            return assessment
            
        except Exception as e:
            logger.error(f"æ•°æ®è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}")
            return {
                'completeness_score': 0.0,
                'consistency_score': 0.0,
                'accuracy_score': 0.0,
                'freshness_score': 0.0,
                'overall_score': 0.0,
                'anomalies_detected': 0,
                'quality_issues': ['è´¨é‡è¯„ä¼°ç³»ç»Ÿæ•…éšœ'],
                'recommendations': ['è¯·æ£€æŸ¥æ•°æ®è´¨é‡è¯„ä¼°ç³»ç»Ÿ']
            }
    
    def save_quality_assessment(self, dataset_id: str, assessment: Dict[str, Any]):
        """ä¿å­˜è´¨é‡è¯„ä¼°ç»“æœ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO quality_assessments 
                (dataset_id, assessment_time, completeness_score, consistency_score, 
                 accuracy_score, freshness_score, overall_score, anomalies_detected, quality_report)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                dataset_id,
                datetime.now().isoformat(),
                assessment['completeness_score'],
                assessment['consistency_score'],
                assessment['accuracy_score'],
                assessment['freshness_score'],
                assessment['overall_score'],
                assessment['anomalies_detected'],
                json.dumps(assessment, ensure_ascii=False)
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ä¿å­˜è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}")
    
    def detect_duplicate_datasets(self, similarity_threshold: float = None) -> List[Tuple[str, str, float]]:
        """
        æ£€æµ‹é‡å¤æ•°æ®é›†
        
        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            é‡å¤æ•°æ®é›†å¯¹åˆ—è¡¨ [(dataset_id1, dataset_id2, similarity), ...]
        """
        try:
            if similarity_threshold is None:
                similarity_threshold = self.quality_config['duplicate_threshold']
            
            # è·å–æ‰€æœ‰æ•°æ®é›†
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT dataset_id, sample_count, feature_count FROM datasets')
            datasets_info = cursor.fetchall()
            conn.close()
            
            duplicates = []
            
            # æ¯”è¾ƒæ•°æ®é›†ç›¸ä¼¼åº¦
            for i, (id1, count1, features1) in enumerate(datasets_info):
                for j, (id2, count2, features2) in enumerate(datasets_info[i+1:], i+1):
                    try:
                        # åŠ è½½æ•°æ®é›†è¿›è¡Œæ¯”è¾ƒ
                        data1 = self.load_dataset(id1)
                        data2 = self.load_dataset(id2)
                        
                        if data1 is not None and data2 is not None:
                            # è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦
                            structure_similarity = self.calculate_structure_similarity(data1, data2)
                            
                            if structure_similarity > similarity_threshold:
                                # è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦
                                content_similarity = self.calculate_content_similarity(data1, data2)
                                overall_similarity = (structure_similarity + content_similarity) / 2
                                
                                if overall_similarity > similarity_threshold:
                                    duplicates.append((id1, id2, overall_similarity))
                                    
                    except Exception as e:
                        logger.warning(f"æ¯”è¾ƒæ•°æ®é›†å¤±è´¥ {id1} vs {id2}: {str(e)}")
            
            if duplicates:
                logger.info(f"æ£€æµ‹åˆ° {len(duplicates)} å¯¹é‡å¤æ•°æ®é›†")
            
            return duplicates
            
        except Exception as e:
            logger.error(f"æ£€æµ‹é‡å¤æ•°æ®é›†å¤±è´¥: {str(e)}")
            return []
    
    def calculate_structure_similarity(self, data1: pd.DataFrame, data2: pd.DataFrame) -> float:
        """è®¡ç®—æ•°æ®é›†ç»“æ„ç›¸ä¼¼åº¦"""
        try:
            # æ¯”è¾ƒåˆ—å
            cols1 = set(data1.columns)
            cols2 = set(data2.columns)
            
            common_cols = cols1.intersection(cols2)
            all_cols = cols1.union(cols2)
            
            column_similarity = len(common_cols) / len(all_cols) if all_cols else 0
            
            # æ¯”è¾ƒæ•°æ®ç±»å‹
            dtype_similarity = 1.0
            for col in common_cols:
                if data1[col].dtype != data2[col].dtype:
                    dtype_similarity -= 0.1
            
            dtype_similarity = max(0, dtype_similarity)
            
            # æ¯”è¾ƒå½¢çŠ¶ç›¸ä¼¼åº¦
            shape_similarity = 1 - abs(len(data1) - len(data2)) / max(len(data1), len(data2))
            
            # ç»¼åˆç»“æ„ç›¸ä¼¼åº¦
            structure_similarity = (column_similarity * 0.5 + dtype_similarity * 0.3 + shape_similarity * 0.2)
            
            return structure_similarity
            
        except Exception as e:
            logger.error(f"è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
            return 0.0
    
    def calculate_content_similarity(self, data1: pd.DataFrame, data2: pd.DataFrame) -> float:
        """è®¡ç®—æ•°æ®é›†å†…å®¹ç›¸ä¼¼åº¦"""
        try:
            # æ‰¾åˆ°å…±åŒçš„æ•°å€¼åˆ—
            numeric_cols1 = set(data1.select_dtypes(include=[np.number]).columns)
            numeric_cols2 = set(data2.select_dtypes(include=[np.number]).columns)
            common_numeric_cols = list(numeric_cols1.intersection(numeric_cols2))
            
            if not common_numeric_cols:
                return 0.0
            
            # è®¡ç®—ç»Ÿè®¡ç‰¹å¾ç›¸ä¼¼åº¦
            similarities = []
            
            for col in common_numeric_cols:
                try:
                    # åŸºæœ¬ç»Ÿè®¡é‡
                    stats1 = data1[col].describe()
                    stats2 = data2[col].describe()
                    
                    # è®¡ç®—ç»Ÿè®¡é‡ç›¸ä¼¼åº¦
                    stat_names = ['mean', 'std', 'min', 'max']
                    stat_similarities = []
                    
                    for stat in stat_names:
                        if stat in stats1 and stat in stats2:
                            val1, val2 = stats1[stat], stats2[stat]
                            if val1 != 0 or val2 != 0:
                                stat_sim = 1 - abs(val1 - val2) / (abs(val1) + abs(val2))
                                stat_similarities.append(stat_sim)
                    
                    if stat_similarities:
                        similarities.append(np.mean(stat_similarities))
                
                except Exception as e:
                    logger.warning(f"è®¡ç®—åˆ— {col} ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
            
            if similarities:
                return np.mean(similarities)
            else:
                return 0.0
                
        except Exception as e:
            logger.error(f"è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
            return 0.0
    
    def cleanup_old_data(self, days_threshold: int = 90):
        """
        æ¸…ç†è¿‡æœŸæ•°æ®
        
        Args:
            days_threshold: ä¿ç•™å¤©æ•°é˜ˆå€¼
        """
        try:
            cutoff_date = datetime.now() - timedelta(days=days_threshold)
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # è·å–è¿‡æœŸæ•°æ®é›†
            cursor.execute('''
                SELECT dataset_id, file_path FROM datasets 
                WHERE created_time < ?
            ''', (cutoff_date.isoformat(),))
            
            old_datasets = cursor.fetchall()
            
            cleaned_count = 0
            for dataset_id, file_path in old_datasets:
                try:
                    # åˆ é™¤æ–‡ä»¶
                    if os.path.exists(file_path):
                        os.remove(file_path)
                    
                    # åˆ é™¤æ•°æ®åº“è®°å½•
                    cursor.execute('DELETE FROM datasets WHERE dataset_id = ?', (dataset_id,))
                    cursor.execute('DELETE FROM quality_assessments WHERE dataset_id = ?', (dataset_id,))
                    cursor.execute('DELETE FROM learning_history WHERE dataset_id = ?', (dataset_id,))
                    cursor.execute('DELETE FROM anomaly_data WHERE dataset_id = ?', (dataset_id,))
                    
                    # æ¸…ç†ç¼“å­˜
                    if dataset_id in self.cached_data:
                        del self.cached_data[dataset_id]
                    
                    cleaned_count += 1
                    
                except Exception as e:
                    logger.error(f"æ¸…ç†æ•°æ®é›†å¤±è´¥ {dataset_id}: {str(e)}")
            
            conn.commit()
            conn.close()
            
            logger.info(f"æ•°æ®æ¸…ç†å®Œæˆï¼Œæ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸæ•°æ®é›†")
            
        except Exception as e:
            logger.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {str(e)}")
    
    def get_data_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åŸºç¡€ç»Ÿè®¡
            cursor.execute('SELECT COUNT(*) FROM datasets')
            total_datasets = cursor.fetchone()[0]
            
            cursor.execute('SELECT SUM(sample_count) FROM datasets')
            total_samples = cursor.fetchone()[0] or 0
            
            cursor.execute('SELECT AVG(quality_score) FROM datasets')
            avg_quality = cursor.fetchone()[0] or 0
            
            # æ•°æ®ç±»å‹åˆ†å¸ƒ
            cursor.execute('SELECT data_type, COUNT(*) FROM datasets GROUP BY data_type')
            type_distribution = dict(cursor.fetchall())
            
            # è´¨é‡åˆ†å¸ƒ
            cursor.execute('''
                SELECT 
                    CASE 
                        WHEN quality_score >= 0.8 THEN 'excellent'
                        WHEN quality_score >= 0.6 THEN 'good'
                        WHEN quality_score >= 0.4 THEN 'fair'
                        ELSE 'poor'
                    END as quality_level,
                    COUNT(*) as count
                FROM datasets 
                GROUP BY quality_level
            ''')
            quality_distribution = dict(cursor.fetchall())
            
            # æœ€è¿‘æ´»åŠ¨
            week_ago = (datetime.now() - timedelta(days=7)).isoformat()
            cursor.execute('SELECT COUNT(*) FROM datasets WHERE created_time > ?', (week_ago,))
            recent_datasets = cursor.fetchone()[0]
            
            conn.close()
            
            statistics = {
                'total_datasets': total_datasets,
                'total_samples': total_samples,
                'average_quality_score': round(avg_quality, 3),
                'data_type_distribution': type_distribution,
                'quality_distribution': quality_distribution,
                'recent_datasets_7days': recent_datasets,
                'cache_size': len(self.cached_data),
                'storage_directory': self.data_dir
            }
            
            return statistics
            
        except Exception as e:
            logger.error(f"è·å–æ•°æ®ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {}
    
    def export_data_report(self) -> str:
        """å¯¼å‡ºæ•°æ®ç®¡ç†æŠ¥å‘Š"""
        try:
            stats = self.get_data_statistics()
            
            report = f"""
# å­¦ä¹ æ•°æ®ç®¡ç†æŠ¥å‘Š

## æ•°æ®æ¦‚è§ˆ
- æ•°æ®é›†æ€»æ•°: {stats.get('total_datasets', 0)}
- æ ·æœ¬æ€»æ•°: {stats.get('total_samples', 0):,}
- å¹³å‡è´¨é‡è¯„åˆ†: {stats.get('average_quality_score', 0):.3f}/1.0
- è¿‘7å¤©æ–°å¢æ•°æ®é›†: {stats.get('recent_datasets_7days', 0)}

## æ•°æ®ç±»å‹åˆ†å¸ƒ
"""
            
            for data_type, count in stats.get('data_type_distribution', {}).items():
                report += f"- {data_type}: {count} ä¸ªæ•°æ®é›†\n"
            
            report += f"""
## æ•°æ®è´¨é‡åˆ†å¸ƒ
"""
            
            for quality_level, count in stats.get('quality_distribution', {}).items():
                report += f"- {quality_level}: {count} ä¸ªæ•°æ®é›†\n"
            
            report += f"""
## å­˜å‚¨ä¿¡æ¯
- å­˜å‚¨ç›®å½•: {stats.get('storage_directory', 'N/A')}
- ç¼“å­˜æ•°æ®é›†: {stats.get('cache_size', 0)} ä¸ª

## æ•°æ®è´¨é‡å»ºè®®
"""
            
            # åŸºäºç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆå»ºè®®
            avg_quality = stats.get('average_quality_score', 0)
            if avg_quality < 0.6:
                report += "- âš ï¸ æ•´ä½“æ•°æ®è´¨é‡åä½ï¼Œå»ºè®®åŠ å¼ºæ•°æ®æ¸…ç†å’ŒéªŒè¯\n"
            elif avg_quality < 0.8:
                report += "- ğŸ’¡ æ•°æ®è´¨é‡ä¸­ç­‰ï¼Œå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ•°æ®æ”¶é›†æµç¨‹\n"
            else:
                report += "- âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰æ ‡å‡†\n"
            
            poor_count = stats.get('quality_distribution', {}).get('poor', 0)
            if poor_count > 0:
                report += f"- ğŸ”§ å‘ç° {poor_count} ä¸ªä½è´¨é‡æ•°æ®é›†ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†\n"
            
            report += f"""
æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
            
            # ä¿å­˜æŠ¥å‘Š
            report_path = os.path.join(self.data_dir, 'quality_reports', 
                                     f"data_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            
            logger.info(f"æ•°æ®ç®¡ç†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            return report_path
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ•°æ®æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return ""


def main():
    """æµ‹è¯•å­¦ä¹ æ•°æ®ç®¡ç†ç³»ç»Ÿ"""
    # åˆ›å»ºæ•°æ®ç®¡ç†å™¨
    manager = LearningDataManager()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = pd.DataFrame({
        'feature1': np.random.normal(0, 1, 100),
        'feature2': np.random.normal(5, 2, 100),
        'feature3': ['A'] * 50 + ['B'] * 50,
        'target': np.random.random(100),
        'timestamp': pd.date_range('2024-01-01', periods=100, freq='D')
    })
    
    # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
    test_data.loc[10:15, 'feature1'] = np.nan
    test_data.loc[95:99, 'feature2'] = 100  # å¼‚å¸¸å€¼
    
    # ä¿å­˜æ•°æ®é›†
    print("ä¿å­˜æµ‹è¯•æ•°æ®é›†...")
    dataset_id = manager.save_dataset(test_data, "æµ‹è¯•æ•°æ®é›†", "ç”¨äºæµ‹è¯•æ•°æ®ç®¡ç†ç³»ç»Ÿ", "training")
    print(f"æ•°æ®é›†ID: {dataset_id}")
    
    # åŠ è½½æ•°æ®é›†
    print("åŠ è½½æ•°æ®é›†...")
    loaded_data = manager.load_dataset(dataset_id)
    print(f"åŠ è½½æˆåŠŸï¼Œæ ·æœ¬æ•°: {len(loaded_data) if loaded_data is not None else 0}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    print("è·å–æ•°æ®ç»Ÿè®¡...")
    stats = manager.get_data_statistics()
    print("ç»Ÿè®¡ä¿¡æ¯:", json.dumps(stats, indent=2, ensure_ascii=False))
    
    # å¯¼å‡ºæŠ¥å‘Š
    print("å¯¼å‡ºæ•°æ®æŠ¥å‘Š...")
    report_path = manager.export_data_report()
    print(f"æŠ¥å‘Šè·¯å¾„: {report_path}")


if __name__ == "__main__":
    main()