# åƒä¸‡çº§æ•°æ®å¤„ç†å®Œæ•´æ–¹æ¡ˆ

## ğŸ¯ ç›®æ ‡

å®ç° 1200ä¸‡+ è®¢å•æ•°æ®çš„é«˜æ•ˆæŸ¥è¯¢å’Œåº”ç”¨ï¼ŒæŸ¥è¯¢æ—¶é—´ä» 5åˆ†é’Ÿ é™ä½åˆ° 10ç§’ ä»¥å†…ã€‚

---

## ğŸ“Š å½“å‰çŠ¶æ€åˆ†æ

### ç°çŠ¶
- **æ•°æ®é‡**: 1200ä¸‡æ¡è®¢å•
- **æŸ¥è¯¢æ—¶é—´**: 2-5 åˆ†é’Ÿï¼ˆæ— ä¼˜åŒ–ï¼‰
- **å†…å­˜å ç”¨**: 20-40 GB
- **ç”¨æˆ·ä½“éªŒ**: å¾ˆå·®

### ç“¶é¢ˆ
1. **æ•°æ®åº“æŸ¥è¯¢æ…¢** - å…¨è¡¨æ‰«æ
2. **æ•°æ®ä¼ è¾“æ…¢** - 1200ä¸‡æ¡æ•°æ®ä¼ è¾“
3. **å†…å­˜å ç”¨é«˜** - ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®
4. **æ— ç¼“å­˜æœºåˆ¶** - æ¯æ¬¡éƒ½é‡æ–°æŸ¥è¯¢

---

## ğŸš€ å®Œæ•´è§£å†³æ–¹æ¡ˆ

### é˜¶æ®µ1: ç«‹å³ä¼˜åŒ–ï¼ˆä»Šå¤©ï¼Œ1å°æ—¶ï¼‰â­â­â­

#### 1.1 åˆ›å»ºæ•°æ®åº“ç´¢å¼•

**ç›®æ ‡**: æŸ¥è¯¢é€Ÿåº¦æå‡ 10-100 å€

```bash
# æ‰§è¡Œç´¢å¼•åˆ›å»ºè„šæœ¬
cd O2O-Analysis
python database/create_indexes.py
```

**åˆ›å»ºçš„ç´¢å¼•**:
```sql
-- å¤åˆç´¢å¼•ï¼ˆæœ€é‡è¦ï¼‰
CREATE INDEX idx_orders_store_date ON orders(store_name, date);
CREATE INDEX idx_orders_date_store ON orders(date, store_name);

-- å•åˆ—ç´¢å¼•
CREATE INDEX idx_orders_store_name ON orders(store_name);
CREATE INDEX idx_orders_date ON orders(date);
CREATE INDEX idx_orders_barcode ON orders(barcode);

-- é™åºç´¢å¼•ï¼ˆæœ€æ–°æ•°æ®æŸ¥è¯¢ï¼‰
CREATE INDEX idx_orders_date_desc ON orders(date DESC);
```

**é¢„æœŸæ•ˆæœ**:
- æ— ç´¢å¼•: 5-10 åˆ†é’Ÿ
- æœ‰ç´¢å¼•: 30-120 ç§’

#### 1.2 å¯ç”¨ Redis ç¼“å­˜

**ç›®æ ‡**: é‡å¤æŸ¥è¯¢é€Ÿåº¦æå‡ 40 å€

```bash
# å¯åŠ¨ Redis
.\å¯åŠ¨Redis.ps1

# éªŒè¯ Redis
python æµ‹è¯•Redisè‡ªåŠ¨å¯åŠ¨.py
```

**é¢„æœŸæ•ˆæœ**:
- é¦–æ¬¡æŸ¥è¯¢: 30-120 ç§’
- äºŒæ¬¡æŸ¥è¯¢: < 5 ç§’

#### 1.3 ä¼˜åŒ– Product JOIN

**é—®é¢˜**: JOIN æ“ä½œåœ¨å¤§è¡¨ä¸Šå¾ˆæ…¢

**è§£å†³æ–¹æ¡ˆA: ç¡®ä¿ Product è¡¨æœ‰ç´¢å¼•**
```sql
CREATE INDEX idx_product_barcode ON products(barcode);
```

**è§£å†³æ–¹æ¡ˆB: å»¶è¿Ÿ JOINï¼ˆæ¨èï¼‰**
```python
# å…ˆæŸ¥è¯¢ Order è¡¨
orders = db.query(Order).filter(...).all()

# æ‰¹é‡æŸ¥è¯¢ Product è¡¨
barcodes = list(set([o.barcode for o in orders]))
products = db.query(Product).filter(Product.barcode.in_(barcodes)).all()
product_map = {p.barcode: p for p in products}

# åœ¨å†…å­˜ä¸­å…³è”
for order in orders:
    order.store_code = product_map.get(order.barcode, {}).get('store_code', '')
```

---

### é˜¶æ®µ2: çŸ­æœŸä¼˜åŒ–ï¼ˆæœ¬å‘¨ï¼Œ4å°æ—¶ï¼‰â­â­â­

#### 2.1 å®æ–½æµå¼æŸ¥è¯¢

**ç›®æ ‡**: é¿å…å†…å­˜æº¢å‡ºï¼Œæ”¯æŒè¶…å¤§æ•°æ®é‡

**å®ç°**: ä¿®æ”¹ `database/data_source_manager.py`

```python
def load_from_database_streaming(self, 
                                 store_name: str = None,
                                 start_date: datetime = None,
                                 end_date: datetime = None,
                                 batch_size: int = 10000):
    """
    æµå¼åŠ è½½æ•°æ®ï¼Œé¿å…å†…å­˜æº¢å‡º
    
    Args:
        batch_size: æ¯æ‰¹åŠ è½½çš„è¡Œæ•°ï¼Œé»˜è®¤ 10000
    """
    print(f"[Database] æµå¼åŠ è½½æ•°æ®...")
    
    db = next(get_db())
    
    # æ„å»ºæŸ¥è¯¢
    query = db.query(Order)
    
    if store_name:
        query = query.filter(Order.store_name == store_name)
    if start_date:
        query = query.filter(Order.date >= start_date)
    if end_date:
        query = query.filter(Order.date <= end_date)
    
    # æµå¼åŠ è½½
    all_data = []
    offset = 0
    
    while True:
        print(f"[Database] åŠ è½½æ‰¹æ¬¡ {offset // batch_size + 1}...")
        
        # åˆ†æ‰¹æŸ¥è¯¢
        batch = query.limit(batch_size).offset(offset).all()
        
        if not batch:
            break
        
        all_data.extend(batch)
        offset += batch_size
        
        print(f"[Database] å·²åŠ è½½ {len(all_data):,} æ¡è®°å½•")
        
        # å†…å­˜ä¿æŠ¤ï¼šè¶…è¿‡ 100 ä¸‡æ¡åœæ­¢
        if len(all_data) >= 1000000:
            print(f"âš ï¸ å·²åŠ è½½ 100 ä¸‡æ¡ï¼Œåœæ­¢åŠ è½½")
            break
    
    print(f"[Database] âœ… æµå¼åŠ è½½å®Œæˆï¼Œå…± {len(all_data):,} æ¡è®°å½•")
    
    # è½¬æ¢ä¸º DataFrame
    return self._convert_to_dataframe(all_data)
```

**é¢„æœŸæ•ˆæœ**:
- å†…å­˜å ç”¨: é™ä½ 50-80%
- æ”¯æŒæ•°æ®é‡: æ— é™åˆ¶ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
- ç”¨æˆ·ä½“éªŒ: å¯ä»¥çœ‹åˆ°è¿›åº¦

#### 2.2 å®æ–½æ™ºèƒ½åˆ†é¡µ

**ç›®æ ‡**: å‰ç«¯åªæ˜¾ç¤ºå¿…è¦çš„æ•°æ®

**å®ç°**: ä¿®æ”¹ `database/data_source_manager.py`

```python
def load_from_database_paginated(self,
                                 page: int = 1,
                                 page_size: int = 10000,
                                 store_name: str = None,
                                 start_date: datetime = None,
                                 end_date: datetime = None):
    """
    åˆ†é¡µåŠ è½½æ•°æ®
    
    Args:
        page: é¡µç ï¼Œä» 1 å¼€å§‹
        page_size: æ¯é¡µè¡Œæ•°ï¼Œé»˜è®¤ 10000
    """
    print(f"[Database] åˆ†é¡µåŠ è½½æ•°æ® (ç¬¬ {page} é¡µ)...")
    
    db = next(get_db())
    
    # æ„å»ºæŸ¥è¯¢
    query = db.query(Order)
    
    if store_name:
        query = query.filter(Order.store_name == store_name)
    if start_date:
        query = query.filter(Order.date >= start_date)
    if end_date:
        query = query.filter(Order.date <= end_date)
    
    # è·å–æ€»æ•°ï¼ˆä½¿ç”¨å¿«é€Ÿä¼°ç®—ï¼‰
    total_count = query.count()
    total_pages = (total_count + page_size - 1) // page_size
    
    # åˆ†é¡µæŸ¥è¯¢
    offset = (page - 1) * page_size
    results = query.limit(page_size).offset(offset).all()
    
    print(f"[Database] âœ… åŠ è½½å®Œæˆ: ç¬¬ {page}/{total_pages} é¡µï¼Œ{len(results):,} æ¡è®°å½•")
    
    return {
        'data': self._convert_to_dataframe(results),
        'page': page,
        'page_size': page_size,
        'total_count': total_count,
        'total_pages': total_pages
    }
```

**é¢„æœŸæ•ˆæœ**:
- é¦–å±åŠ è½½: < 5 ç§’
- ç¿»é¡µåŠ è½½: < 2 ç§’
- å†…å­˜å ç”¨: é™ä½ 90%+

#### 2.3 å®æ–½æŸ¥è¯¢ä¼˜åŒ–å™¨

**ç›®æ ‡**: æ ¹æ®æ•°æ®é‡è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥

```python
def load_from_database_smart(self, **kwargs):
    """
    æ™ºèƒ½åŠ è½½ï¼šæ ¹æ®æ•°æ®é‡è‡ªåŠ¨é€‰æ‹©ç­–ç•¥
    """
    db = next(get_db())
    
    # æ„å»ºæŸ¥è¯¢
    query = self._build_query(db, **kwargs)
    
    # å¿«é€Ÿä¼°ç®—æ•°æ®é‡
    estimated_count = self._estimate_count(query)
    
    print(f"[Database] é¢„ä¼°æ•°æ®é‡: {estimated_count:,} æ¡")
    
    # æ ¹æ®æ•°æ®é‡é€‰æ‹©ç­–ç•¥
    if estimated_count < 10000:
        # å°æ•°æ®é‡ï¼šå…¨é‡åŠ è½½
        print(f"[Database] ç­–ç•¥: å…¨é‡åŠ è½½")
        return self.load_from_database(**kwargs)
    
    elif estimated_count < 100000:
        # ä¸­ç­‰æ•°æ®é‡ï¼šæµå¼åŠ è½½
        print(f"[Database] ç­–ç•¥: æµå¼åŠ è½½")
        return self.load_from_database_streaming(**kwargs)
    
    else:
        # å¤§æ•°æ®é‡ï¼šåˆ†é¡µåŠ è½½
        print(f"[Database] ç­–ç•¥: åˆ†é¡µåŠ è½½")
        return self.load_from_database_paginated(**kwargs)
```

---

### é˜¶æ®µ3: ä¸­æœŸä¼˜åŒ–ï¼ˆä¸‹å‘¨ï¼Œ8å°æ—¶ï¼‰â­â­

#### 3.1 æ•°æ®åˆ†åŒº

**ç›®æ ‡**: æŸ¥è¯¢åªæ‰«æç›¸å…³åˆ†åŒºï¼Œé€Ÿåº¦æå‡ 10 å€

**å®ç°**: æŒ‰æœˆä»½åˆ†åŒº

```sql
-- 1. åˆ›å»ºåˆ†åŒºè¡¨
CREATE TABLE orders_partitioned (
    LIKE orders INCLUDING ALL
) PARTITION BY RANGE (date);

-- 2. åˆ›å»ºåˆ†åŒº
CREATE TABLE orders_2024_01 PARTITION OF orders_partitioned
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE orders_2024_02 PARTITION OF orders_partitioned
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- ... ä¸ºæ¯ä¸ªæœˆåˆ›å»ºåˆ†åŒº

-- 3. è¿ç§»æ•°æ®
INSERT INTO orders_partitioned SELECT * FROM orders;

-- 4. é‡å‘½åè¡¨
ALTER TABLE orders RENAME TO orders_old;
ALTER TABLE orders_partitioned RENAME TO orders;
```

**é¢„æœŸæ•ˆæœ**:
- æŸ¥è¯¢å•æœˆæ•°æ®: é€Ÿåº¦æå‡ 10-20 å€
- æŸ¥è¯¢è·¨æœˆæ•°æ®: åªæ‰«æç›¸å…³åˆ†åŒº
- ç»´æŠ¤æˆæœ¬: æ¯æœˆåˆ›å»ºæ–°åˆ†åŒº

#### 3.2 ç‰©åŒ–è§†å›¾

**ç›®æ ‡**: é¢„è®¡ç®—å¸¸ç”¨èšåˆï¼ŒæŸ¥è¯¢é€Ÿåº¦æå‡ 100 å€

**å®ç°**: åˆ›å»ºæ—¥åº¦æ±‡æ€»è§†å›¾

```sql
-- åˆ›å»ºç‰©åŒ–è§†å›¾
CREATE MATERIALIZED VIEW daily_summary AS
SELECT 
    date,
    store_name,
    COUNT(*) as order_count,
    SUM(amount) as total_amount,
    SUM(quantity) as total_quantity,
    AVG(price) as avg_price
FROM orders
GROUP BY date, store_name;

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_daily_summary_date ON daily_summary(date);
CREATE INDEX idx_daily_summary_store ON daily_summary(store_name);

-- å®šæœŸåˆ·æ–°ï¼ˆæ¯å¤©å‡Œæ™¨ï¼‰
REFRESH MATERIALIZED VIEW daily_summary;
```

**ä½¿ç”¨**:
```python
# æŸ¥è¯¢æ—¥åº¦æ±‡æ€»ï¼ˆéå¸¸å¿«ï¼‰
summary = db.query(DailySummary).filter(
    DailySummary.date >= start_date,
    DailySummary.date <= end_date
).all()
```

**é¢„æœŸæ•ˆæœ**:
- æ±‡æ€»æŸ¥è¯¢: ä» 60ç§’ é™ä½åˆ° < 1ç§’
- é€‚ç”¨åœºæ™¯: æŠ¥è¡¨ã€è¶‹åŠ¿åˆ†æ
- é™åˆ¶: åªèƒ½æŸ¥è¯¢é¢„å®šä¹‰çš„èšåˆ

#### 3.3 å¼‚æ­¥æŸ¥è¯¢

**ç›®æ ‡**: å¤§æŸ¥è¯¢ä¸é˜»å¡ç•Œé¢

**å®ç°**: ä½¿ç”¨åå°ä»»åŠ¡

```python
from background_tasks import BackgroundTaskManager

task_manager = BackgroundTaskManager()

def async_load_data(store_name, start_date, end_date):
    """å¼‚æ­¥åŠ è½½æ•°æ®"""
    
    # åˆ›å»ºä»»åŠ¡
    task_id = task_manager.create_task(
        name="æ•°æ®åŠ è½½",
        description=f"åŠ è½½ {store_name} ä» {start_date} åˆ° {end_date} çš„æ•°æ®"
    )
    
    try:
        # æ‰§è¡ŒæŸ¥è¯¢
        data = load_from_database(store_name, start_date, end_date)
        
        # ç¼“å­˜ç»“æœ
        cache_key = f"data:{store_name}:{start_date}:{end_date}"
        redis.set(cache_key, data, ex=3600)
        
        # æ ‡è®°å®Œæˆ
        task_manager.complete_task(task_id, result=cache_key)
        
    except Exception as e:
        task_manager.fail_task(task_id, error=str(e))
    
    return task_id
```

**å‰ç«¯ä½¿ç”¨**:
```python
# æäº¤ä»»åŠ¡
task_id = async_load_data(store_name, start_date, end_date)

# æ˜¾ç¤ºè¿›åº¦
while not task_manager.is_complete(task_id):
    progress = task_manager.get_progress(task_id)
    print(f"è¿›åº¦: {progress}%")
    time.sleep(1)

# è·å–ç»“æœ
cache_key = task_manager.get_result(task_id)
data = redis.get(cache_key)
```

---

### é˜¶æ®µ4: é•¿æœŸä¼˜åŒ–ï¼ˆä¸‹æœˆï¼Œ16å°æ—¶ï¼‰â­

#### 4.1 è¯»å†™åˆ†ç¦»

**ç›®æ ‡**: æŸ¥è¯¢ä¸å½±å“å†™å…¥æ€§èƒ½

**æ¶æ„**:
```
ä¸»åº“ï¼ˆMasterï¼‰  â† å†™å…¥
    â†“ å¤åˆ¶
ä»åº“ï¼ˆSlaveï¼‰   â† æŸ¥è¯¢
```

**å®ç°**:
```python
# database/connection.py

# ä¸»åº“è¿æ¥ï¼ˆå†™å…¥ï¼‰
MASTER_DATABASE_URL = "postgresql://user:pass@master-host:5432/db"
master_engine = create_engine(MASTER_DATABASE_URL)

# ä»åº“è¿æ¥ï¼ˆæŸ¥è¯¢ï¼‰
SLAVE_DATABASE_URL = "postgresql://user:pass@slave-host:5432/db"
slave_engine = create_engine(SLAVE_DATABASE_URL)

def get_db_master():
    """è·å–ä¸»åº“è¿æ¥ï¼ˆå†™å…¥ï¼‰"""
    return SessionLocal(bind=master_engine)

def get_db_slave():
    """è·å–ä»åº“è¿æ¥ï¼ˆæŸ¥è¯¢ï¼‰"""
    return SessionLocal(bind=slave_engine)
```

**ä½¿ç”¨**:
```python
# å†™å…¥ä½¿ç”¨ä¸»åº“
db = get_db_master()
db.add(new_order)
db.commit()

# æŸ¥è¯¢ä½¿ç”¨ä»åº“
db = get_db_slave()
orders = db.query(Order).all()
```

#### 4.2 æ•°æ®å½’æ¡£

**ç›®æ ‡**: ä¸»è¡¨åªä¿ç•™çƒ­æ•°æ®ï¼Œæå‡æŸ¥è¯¢é€Ÿåº¦

**ç­–ç•¥**:
- ä¸»è¡¨: æœ€è¿‘ 1 å¹´æ•°æ®
- å½’æ¡£è¡¨: 1 å¹´ä»¥å‰æ•°æ®

**å®ç°**:
```sql
-- åˆ›å»ºå½’æ¡£è¡¨
CREATE TABLE orders_archive (LIKE orders INCLUDING ALL);

-- å½’æ¡£æ—§æ•°æ®ï¼ˆæ¯æœˆæ‰§è¡Œï¼‰
INSERT INTO orders_archive 
SELECT * FROM orders 
WHERE date < CURRENT_DATE - INTERVAL '1 year';

-- åˆ é™¤å·²å½’æ¡£æ•°æ®
DELETE FROM orders 
WHERE date < CURRENT_DATE - INTERVAL '1 year';
```

**æŸ¥è¯¢**:
```python
def load_data_with_archive(start_date, end_date):
    """æŸ¥è¯¢åŒ…å«å½’æ¡£æ•°æ®"""
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦æŸ¥è¯¢å½’æ¡£è¡¨
    one_year_ago = datetime.now() - timedelta(days=365)
    
    if start_date < one_year_ago:
        # æŸ¥è¯¢å½’æ¡£è¡¨
        archive_data = db.query(OrderArchive).filter(
            OrderArchive.date >= start_date,
            OrderArchive.date < one_year_ago
        ).all()
    else:
        archive_data = []
    
    # æŸ¥è¯¢ä¸»è¡¨
    main_data = db.query(Order).filter(
        Order.date >= max(start_date, one_year_ago),
        Order.date <= end_date
    ).all()
    
    # åˆå¹¶
    return archive_data + main_data
```

#### 4.3 æ•°æ®å¯¼å‡ºåŠŸèƒ½

**ç›®æ ‡**: å¤§æ•°æ®é‡æŸ¥è¯¢æ”¹ä¸ºå¼‚æ­¥å¯¼å‡º

**å®ç°**:
```python
def export_data_async(store_name, start_date, end_date, format='csv'):
    """å¼‚æ­¥å¯¼å‡ºæ•°æ®"""
    
    # åˆ›å»ºå¯¼å‡ºä»»åŠ¡
    task_id = str(uuid.uuid4())
    
    # åå°æ‰§è¡Œ
    def export_worker():
        # æŸ¥è¯¢æ•°æ®
        data = load_from_database(store_name, start_date, end_date)
        
        # å¯¼å‡ºæ–‡ä»¶
        filename = f"export_{task_id}.{format}"
        filepath = f"exports/{filename}"
        
        if format == 'csv':
            data.to_csv(filepath, index=False)
        elif format == 'excel':
            data.to_excel(filepath, index=False)
        
        # é€šçŸ¥ç”¨æˆ·
        notify_user(f"å¯¼å‡ºå®Œæˆ: {filename}")
    
    # å¯åŠ¨åå°çº¿ç¨‹
    thread = threading.Thread(target=export_worker)
    thread.start()
    
    return task_id
```

---

## ğŸ“‹ å®æ–½è®¡åˆ’

### ç¬¬1å¤©ï¼ˆä»Šå¤©ï¼‰
- [x] åˆ›å»ºæ•°æ®åº“ç´¢å¼•
- [x] å¯ç”¨ Redis ç¼“å­˜
- [x] ä¼˜åŒ– Product JOIN
- [ ] æµ‹è¯•éªŒè¯

**é¢„æœŸæ•ˆæœ**: æŸ¥è¯¢æ—¶é—´ä» 5åˆ†é’Ÿ é™ä½åˆ° 30-60ç§’

### ç¬¬2-3å¤©ï¼ˆæœ¬å‘¨ï¼‰
- [ ] å®æ–½æµå¼æŸ¥è¯¢
- [ ] å®æ–½æ™ºèƒ½åˆ†é¡µ
- [ ] å®æ–½æŸ¥è¯¢ä¼˜åŒ–å™¨
- [ ] æµ‹è¯•éªŒè¯

**é¢„æœŸæ•ˆæœ**: æŸ¥è¯¢æ—¶é—´é™ä½åˆ° 10-30ç§’ï¼Œæ”¯æŒè¶…å¤§æ•°æ®é‡

### ç¬¬4-5å¤©ï¼ˆä¸‹å‘¨ï¼‰
- [ ] æ•°æ®åˆ†åŒº
- [ ] ç‰©åŒ–è§†å›¾
- [ ] å¼‚æ­¥æŸ¥è¯¢
- [ ] æµ‹è¯•éªŒè¯

**é¢„æœŸæ•ˆæœ**: æŸ¥è¯¢æ—¶é—´é™ä½åˆ° 5-10ç§’ï¼Œç”¨æˆ·ä½“éªŒä¼˜ç§€

### ç¬¬6-10å¤©ï¼ˆä¸‹æœˆï¼‰
- [ ] è¯»å†™åˆ†ç¦»
- [ ] æ•°æ®å½’æ¡£
- [ ] æ•°æ®å¯¼å‡ºåŠŸèƒ½
- [ ] å…¨é¢æµ‹è¯•

**é¢„æœŸæ•ˆæœ**: ç³»ç»Ÿæ”¯æŒ 100+ å¹¶å‘ç”¨æˆ·ï¼ŒæŸ¥è¯¢æ—¶é—´ < 5ç§’

---

## ğŸ¯ æ€§èƒ½ç›®æ ‡

| é˜¶æ®µ | æŸ¥è¯¢æ—¶é—´ | å†…å­˜å ç”¨ | å¹¶å‘ç”¨æˆ· | çŠ¶æ€ |
|------|---------|---------|---------|------|
| å½“å‰ | 2-5 åˆ†é’Ÿ | 20-40 GB | 1-2 | âŒ ä¸å¯ç”¨ |
| é˜¶æ®µ1 | 30-60 ç§’ | 10-20 GB | 5-10 | âš ï¸ å¯ç”¨ |
| é˜¶æ®µ2 | 10-30 ç§’ | 2-5 GB | 10-20 | âœ… è‰¯å¥½ |
| é˜¶æ®µ3 | 5-10 ç§’ | 1-2 GB | 20-50 | âœ… ä¼˜ç§€ |
| é˜¶æ®µ4 | < 5 ç§’ | < 1 GB | 50-100+ | ğŸ‰ å“è¶Š |

---

## âœ… ç«‹å³è¡ŒåŠ¨

### ç»™åŒäº‹æ‰§è¡Œï¼ˆ5åˆ†é’Ÿï¼‰

```bash
# 1. åˆ›å»ºç´¢å¼•
cd O2O-Analysis
python database/create_indexes.py

# 2. å¯åŠ¨ Redis
.\å¯åŠ¨Redis.ps1

# 3. é‡å¯çœ‹æ¿
.\å¯åŠ¨çœ‹æ¿.ps1

# 4. æµ‹è¯•æŸ¥è¯¢
# é€‰æ‹©ä¸€ä¸ªé—¨åº—ï¼ŒæŸ¥è¯¢æœ€è¿‘ 30 å¤©
# è§‚å¯ŸæŸ¥è¯¢æ—¶é—´
```

**é¢„æœŸ**: æŸ¥è¯¢æ—¶é—´ä» 5åˆ†é’Ÿ é™ä½åˆ° 30-60ç§’

---

**æ–‡æ¡£ç‰ˆæœ¬**: V1.0  
**åˆ›å»ºæ—¶é—´**: 2025-12-11  
**é€‚ç”¨åœºæ™¯**: åƒä¸‡çº§æ•°æ®æŸ¥è¯¢å’Œåº”ç”¨  
**é¢„æœŸæ”¶ç›Š**: æŸ¥è¯¢é€Ÿåº¦æå‡ 10-100 å€  
