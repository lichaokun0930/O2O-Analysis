#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½é—¨åº—æ•°æ®å¯¼å…¥ç³»ç»Ÿ python æ™ºèƒ½å¯¼å…¥é—¨åº—æ•°æ®.py
- è‡ªåŠ¨è¯†åˆ«æ–°å¢æ•°æ®æ–‡ä»¶
- é¿å…é‡å¤å¯¼å…¥
- è‡ªåŠ¨æ•°æ®å®Œæ•´æ€§æ ¡éªŒ
"""

import pandas as pd
import os
import sys
import hashlib
import json
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from database.connection import SessionLocal, init_database
from database.models import Order
from çœŸå®æ•°æ®å¤„ç†å™¨ import RealDataProcessor

# å¯¼å…¥å†å²è®°å½•æ–‡ä»¶
IMPORT_HISTORY_FILE = "å­¦ä¹ æ•°æ®ä»“åº“/import_history.json"

class SmartImporter:
    """æ™ºèƒ½æ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(self):
        # âœ… ç¡®ä¿æ•°æ®åº“è¡¨å·²åˆ›å»º
        init_database()
        self.session = SessionLocal()
        self.processor = RealDataProcessor()  # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        self.import_history = self.load_import_history()
        self.validation_report = {
            'success': True,
            'warnings': [],
            'errors': [],
            'stats': {}
        }
    
    def load_import_history(self):
        """åŠ è½½å¯¼å…¥å†å²è®°å½•"""
        if os.path.exists(IMPORT_HISTORY_FILE):
            with open(IMPORT_HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def save_import_history(self):
        """ä¿å­˜å¯¼å…¥å†å²è®°å½•"""
        os.makedirs(os.path.dirname(IMPORT_HISTORY_FILE), exist_ok=True)
        with open(IMPORT_HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.import_history, f, ensure_ascii=False, indent=2)
    
    def get_file_hash(self, filepath):
        """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def get_file_signature(self, filepath):
        """è·å–æ–‡ä»¶ç­¾å(hash + ä¿®æ”¹æ—¶é—´)"""
        file_hash = self.get_file_hash(filepath)
        mod_time = os.path.getmtime(filepath)
        return {
            'hash': file_hash,
            'mod_time': mod_time,
            'size': os.path.getsize(filepath)
        }
    
    def scan_new_files(self):
        """æ‰«ææ–°å¢çš„Excelæ–‡ä»¶"""
        print("="*70)
        print(" ğŸ” æ‰«ææ–°å¢æ•°æ®æ–‡ä»¶")
        print("="*70)
        
        # æ‰«ææ‰€æœ‰Excelæ–‡ä»¶
        excel_files = []
        for pattern in ["å®é™…æ•°æ®/*.xlsx", "é—¨åº—æ•°æ®/*.xlsx", "é—¨åº—æ•°æ®/**/*.xlsx"]:
            excel_files.extend(Path(".").glob(pattern))
        
        # è¿‡æ»¤ä¸´æ—¶æ–‡ä»¶
        excel_files = [f for f in excel_files if not f.name.startswith('~$')]
        
        print(f"\næ‰¾åˆ° {len(excel_files)} ä¸ªExcelæ–‡ä»¶")
        
        # è¯†åˆ«æ–°æ–‡ä»¶
        new_files = []
        updated_files = []
        skipped_files = []
        
        for file_path in excel_files:
            file_str = str(file_path)
            signature = self.get_file_signature(file_str)
            
            if file_str not in self.import_history:
                # å…¨æ–°æ–‡ä»¶
                new_files.append((file_str, signature))
                print(f"  ğŸ†• æ–°æ–‡ä»¶: {file_path.name}")
            else:
                old_sig = self.import_history[file_str]
                if signature['hash'] != old_sig.get('hash'):
                    # æ–‡ä»¶å·²æ›´æ–°
                    updated_files.append((file_str, signature))
                    print(f"  ğŸ”„ å·²æ›´æ–°: {file_path.name}")
                else:
                    # å·²å¯¼å…¥,è·³è¿‡
                    skipped_files.append(file_str)
                    print(f"  â­ï¸  å·²å¯¼å…¥: {file_path.name}")
        
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"  æ–°æ–‡ä»¶: {len(new_files)}")
        print(f"  æ›´æ–°æ–‡ä»¶: {len(updated_files)}")
        print(f"  è·³è¿‡æ–‡ä»¶: {len(skipped_files)}")
        
        return new_files + updated_files
    
    def validate_excel_structure(self, df, file_name):
        """éªŒè¯Excelæ•°æ®ç»“æ„"""
        print(f"\n  ğŸ“‹ éªŒè¯æ•°æ®ç»“æ„: {file_name}")
        
        # å¿…éœ€å­—æ®µ (æ”¯æŒæ ‡å‡†åŒ–åçš„å­—æ®µå)
        # æ ¼å¼: (æ˜¾ç¤ºåç§°, [å€™é€‰å­—æ®µåˆ—è¡¨])
        required_checks = [
            ('è®¢å•ID', ['è®¢å•ID']),
            ('é—¨åº—åç§°', ['é—¨åº—åç§°']),
            ('å•†å“åç§°', ['å•†å“åç§°']),
            ('å•†å“å®å”®ä»·', ['å•†å“å®å”®ä»·']),
            ('é”€é‡', ['é”€é‡', 'æœˆå”®']),  # å…¼å®¹æ ‡å‡†åŒ–åçš„'æœˆå”®'
            ('ä¸€çº§åˆ†ç±»å', ['ä¸€çº§åˆ†ç±»å']),
            ('ä¸‹å•æ—¶é—´', ['ä¸‹å•æ—¶é—´', 'æ—¥æœŸ']) # å…¼å®¹æ ‡å‡†åŒ–åçš„'æ—¥æœŸ'
        ]
        
        # é‡è¦å­—æ®µ
        important_checks = [
            ('æˆæœ¬', ['æˆæœ¬', 'å•†å“é‡‡è´­æˆæœ¬']), # å…¼å®¹æ ‡å‡†åŒ–åçš„'å•†å“é‡‡è´­æˆæœ¬'
            ('åˆ©æ¶¦é¢', ['åˆ©æ¶¦é¢']),
            ('ç‰©æµé…é€è´¹', ['ç‰©æµé…é€è´¹']),
            ('å¹³å°ä½£é‡‘', ['å¹³å°ä½£é‡‘']),
            ('å‰©ä½™åº“å­˜', ['å‰©ä½™åº“å­˜', 'åº“å­˜']), # å…¼å®¹æ ‡å‡†åŒ–åçš„'åº“å­˜'
            ('æ¡ç ', ['æ¡ç ']),
            ('åº—å†…ç ', ['åº—å†…ç '])
        ]
        
        validation_ok = True
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for label, candidates in required_checks:
            if not any(field in df.columns for field in candidates):
                self.validation_report['errors'].append(
                    f"âŒ {file_name}: ç¼ºå°‘å¿…éœ€å­—æ®µ '{label}' (æ£€æŸ¥è¿‡: {candidates})"
                )
                validation_ok = False
        
        # æ£€æŸ¥é‡è¦å­—æ®µ
        for label, candidates in important_checks:
            if not any(field in df.columns for field in candidates):
                self.validation_report['warnings'].append(
                    f"âš ï¸  {file_name}: ç¼ºå°‘é‡è¦å­—æ®µ '{label}'"
                )
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        cost_field = next((f for f in ['æˆæœ¬', 'å•†å“é‡‡è´­æˆæœ¬'] if f in df.columns), None)
        if cost_field:
            non_zero_cost = len(df[df[cost_field] > 0])
            cost_ratio = non_zero_cost / len(df) * 100 if len(df) > 0 else 0
            
            if cost_ratio < 50:
                self.validation_report['warnings'].append(
                    f"âš ï¸  {file_name}: æˆæœ¬æ•°æ®è¾ƒå°‘({cost_ratio:.1f}%æœ‰æˆæœ¬)"
                )
            
            print(f"    âœ… æˆæœ¬å­—æ®µ({cost_field}): {non_zero_cost}/{len(df)} ({cost_ratio:.1f}%)")
        else:
            print(f"    âŒ æˆæœ¬å­—æ®µ: ä¸å­˜åœ¨")
        
        if 'è®¢å•ID' in df.columns:
            unique_orders = df['è®¢å•ID'].nunique()
            print(f"    âœ… è®¢å•æ•°é‡: {unique_orders:,}")
        
        return validation_ok
    
    def import_file(self, file_path, signature):
        """å¯¼å…¥å•ä¸ªæ–‡ä»¶"""
        print(f"\n{'='*70}")
        print(f" ğŸ“¥ å¯¼å…¥æ–‡ä»¶: {os.path.basename(file_path)}")
        print(f"{'='*70}")
        
        try:
            # 1. è¯»å–Excel
            print(f"\n1ï¸âƒ£ è¯»å–Excel...")
            df = pd.read_excel(file_path)
            print(f"   æ€»è¡Œæ•°: {len(df):,}")
            
            # 1.5 æ ‡å‡†åŒ–å­—æ®µå (ä½¿ç”¨çœŸå®æ•°æ®å¤„ç†å™¨)
            print(f"   ğŸ”„ æ ‡å‡†åŒ–å­—æ®µå...")
            df = self.processor.standardize_sales_data(df)
            
            # 2. éªŒè¯æ•°æ®ç»“æ„
            if not self.validate_excel_structure(df, os.path.basename(file_path)):
                return False
            
            # âŒ 2025-11-22: ç¦ç”¨è€—æè¿‡æ»¤,ä¿ç•™çœŸå®æˆæœ¬æ•°æ®
            # åŸå› : è€—æ(è´­ç‰©è¢‹)æ˜¯è®¢å•æˆæœ¬çš„ä¸€éƒ¨åˆ†,å‰”é™¤ä¼šå¯¼è‡´åˆ©æ¶¦è™šé«˜
            # ä¸çœ‹æ¿ä¸Šä¼ é€»è¾‘ä¿æŒä¸€è‡´ (2025-11-18å·²ä¿®æ”¹)
            # if 'ä¸€çº§åˆ†ç±»å' in df.columns:
            #     original_len = len(df)
            #     df = df[~df['ä¸€çº§åˆ†ç±»å'].isin(['è€—æ'])]
            #     filtered_count = original_len - len(df)
            #     if filtered_count > 0:
            #         print(f"\n2ï¸âƒ£ è¿‡æ»¤æ•°æ®: ç§»é™¤ {filtered_count:,} æ¡è€—æè®°å½•")
            print(f"\n2ï¸âƒ£ âœ… ä¿ç•™è€—ææ•°æ® (åŒ…å«è´­ç‰©è¢‹ç­‰æˆæœ¬)")
            
            # 3. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥é—¨åº—æ•°æ®
            if 'é—¨åº—åç§°' in df.columns:
                store_name = df['é—¨åº—åç§°'].iloc[0]
                existing = self.session.query(Order).filter(
                    Order.store_name == store_name
                ).first()
                
                if existing:
                    print(f"\nâš ï¸  æ£€æµ‹åˆ°é—¨åº— '{store_name}' å·²å­˜åœ¨æ•°æ®")
                    print("   ğŸ”„ è‡ªåŠ¨è¦†ç›–æ¨¡å¼: æ­£åœ¨åˆ é™¤æ—§æ•°æ®...")
                    
                    # åˆ é™¤æ—§æ•°æ®
                    self.session.query(Order).filter(
                        Order.store_name == store_name
                    ).delete()
                    self.session.commit()
            
            # 4. å¯¼å…¥æ•°æ®ï¼ˆæ‰¹é‡æ’å…¥ä¼˜åŒ–ï¼‰
            print(f"\n3ï¸âƒ£ å¯¼å…¥æ•°æ®ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰...")
            success_count = 0
            error_count = 0
            field_errors = {}  # è®°å½•å­—æ®µé”™è¯¯
            batch_size = 5000  # æ‰¹é‡å¤§å°
            batch_orders = []
            
            start_time = datetime.now()
            
            for idx, row in df.iterrows():
                try:
                    order_data = self.map_row_to_order(row)
                    batch_orders.append(order_data)
                    success_count += 1
                    
                    # æ¯batch_sizeæ¡æ‰¹é‡æ’å…¥ä¸€æ¬¡
                    if len(batch_orders) >= batch_size:
                        try:
                            self.session.bulk_insert_mappings(Order, batch_orders)
                            self.session.commit()
                            batch_orders = []
                        except Exception as batch_error:
                            self.session.rollback()
                            print(f"\n   âš ï¸ æ‰¹é‡æ’å…¥å¤±è´¥ï¼ˆå¯èƒ½æœ‰é‡å¤è®¢å•IDï¼‰ï¼Œå°è¯•é€æ¡æ’å…¥...")
                            # é€æ¡æ’å…¥ï¼Œè·³è¿‡é‡å¤çš„
                            for order in batch_orders:
                                try:
                                    self.session.execute(
                                        Order.__table__.insert().values(**order)
                                    )
                                    self.session.commit()
                                except:
                                    self.session.rollback()
                                    success_count -= 1
                                    error_count += 1
                            batch_orders = []
                        
                        # è®¡ç®—è¿›åº¦å’Œé¢„ä¼°æ—¶é—´
                        elapsed = (datetime.now() - start_time).total_seconds()
                        speed = success_count / elapsed if elapsed > 0 else 0
                        remaining = (len(df) - success_count) / speed if speed > 0 else 0
                        
                        print(f"   è¿›åº¦: {success_count:,}/{len(df):,} ({success_count/len(df)*100:.1f}%) | "
                              f"é€Ÿåº¦: {speed:.0f}è¡Œ/ç§’ | "
                              f"é¢„è®¡å‰©ä½™: {int(remaining)}ç§’", end='\r')
                
                except Exception as e:
                    error_count += 1
                    error_msg = str(e)
                    field_errors[error_msg] = field_errors.get(error_msg, 0) + 1
                    
                    if error_count <= 3:
                        print(f"\n   âš ï¸  ç¬¬{idx+1}è¡Œå¤±è´¥: {e}")
            
            # æ’å…¥å‰©ä½™æ•°æ®
            if batch_orders:
                try:
                    self.session.bulk_insert_mappings(Order, batch_orders)
                    self.session.commit()
                except Exception as e:
                    self.session.rollback()
                    print(f"\n   âš ï¸ æœ€åä¸€æ‰¹æ•°æ®æ’å…¥å¤±è´¥: {e}")
                    error_count += len(batch_orders)
            
            total_time = (datetime.now() - start_time).total_seconds()
            print(f"\n   â±ï¸  æ€»è€—æ—¶: {total_time:.1f}ç§’ | å¹³å‡é€Ÿåº¦: {success_count/total_time if total_time > 0 else 0:.0f}è¡Œ/ç§’")
            
            print(f"\n\n4ï¸âƒ£ å¯¼å…¥ç»“æœ:")
            print(f"   âœ… æˆåŠŸ: {success_count:,}/{len(df):,} ({success_count/len(df)*100:.1f}%)")
            if error_count > 0:
                print(f"   âŒ å¤±è´¥: {error_count:,}")
                print(f"\n   å¤±è´¥åŸå› ç»Ÿè®¡:")
                for error, count in field_errors.items():
                    print(f"     â€¢ {error}: {count}æ¬¡")
            
            # 6. æ•°æ®å®Œæ•´æ€§æ ¡éªŒ
            self.validate_imported_data(file_path, df, success_count)
            
            # 7. æ›´æ–°å¯¼å…¥å†å²
            self.import_history[file_path] = {
                **signature,
                'import_time': datetime.now().isoformat(),
                'rows_imported': success_count,
                'rows_failed': error_count
            }
            
            return True
            
        except Exception as e:
            self.session.rollback()
            self.validation_report['errors'].append(
                f"âŒ æ–‡ä»¶å¯¼å…¥å¤±è´¥ {os.path.basename(file_path)}: {e}"
            )
            print(f"\nâŒ å¯¼å…¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def map_row_to_order(self, row):
        """æ˜ å°„Excelè¡Œåˆ°Orderå¯¹è±¡"""
        # âœ… æ™ºèƒ½è¯†åˆ«æ—¥æœŸå­—æ®µ
        order_date = None
        for date_field in ['ä¸‹å•æ—¶é—´', 'æ—¥æœŸ', 'è®¢å•æ—¶é—´', 'æ—¶é—´', 'date', 'order_date']:
            if date_field in row.index and pd.notna(row.get(date_field)):
                try:
                    order_date = pd.to_datetime(row.get(date_field))
                    break
                except:
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¶é—´å¹¶è®°å½•è­¦å‘Š
        if order_date is None:
            order_date = datetime.now()
            if not hasattr(self, '_date_warning_shown'):
                print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆæ—¥æœŸå­—æ®µï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
                self._date_warning_shown = True
        
        return {
            'order_id': str(row.get('è®¢å•ID', '')),
            'order_number': str(row.get('è®¢å•ç¼–å·', '')),  # âœ… æ–°å¢è®¢å•ç¼–å·å­—æ®µæ˜ å°„
            'date': order_date,
            'store_name': str(row.get('é—¨åº—åç§°', '')),
            # âœ… ç§»é™¤äº†store_idå­—æ®µ(Orderæ¨¡å‹ä¸­ä¸å­˜åœ¨)
            'product_name': str(row.get('å•†å“åç§°', '')),
            'barcode': str(row.get('æ¡ç ', '')),
            # âœ… æ·»åŠ åº—å†…ç å­—æ®µæ˜ å°„
            'store_code': str(row.get('åº—å†…ç ', '')) if pd.notna(row.get('åº—å†…ç ')) else '',
            'price': float(row.get('å•†å“å®å”®ä»·', 0)),
            'original_price': float(row.get('å•†å“åŸä»·', 0)),
            'quantity': int(row.get('é”€é‡', row.get('æœˆå”®', 0))), # å…¼å®¹æ ‡å‡†åŒ–åçš„'æœˆå”®'
            'cost': float(row.get('æˆæœ¬', row.get('å•†å“é‡‡è´­æˆæœ¬', 0))) if pd.notna(row.get('æˆæœ¬', row.get('å•†å“é‡‡è´­æˆæœ¬'))) else 0.0,
            'profit': float(row.get('åˆ©æ¶¦é¢', 0)) if pd.notna(row.get('åˆ©æ¶¦é¢')) else 0.0,
            'category_level1': str(row.get('ä¸€çº§åˆ†ç±»å', '')),
            'category_level3': str(row.get('ä¸‰çº§åˆ†ç±»å', '')),
            'barcode': str(row.get('æ¡ç ', '')),
            # âœ… æ·»åŠ å‰©ä½™åº“å­˜å­—æ®µæ˜ å°„
            'remaining_stock': float(row.get('å‰©ä½™åº“å­˜', row.get('åº“å­˜', 0))) if pd.notna(row.get('å‰©ä½™åº“å­˜', row.get('åº“å­˜'))) else 0.0,
            'delivery_fee': float(row.get('ç‰©æµé…é€è´¹', 0)) if pd.notna(row.get('ç‰©æµé…é€è´¹')) else 0.0,
            'commission': float(row.get('å¹³å°ä½£é‡‘', 0)) if pd.notna(row.get('å¹³å°ä½£é‡‘')) else 0.0,
            'platform_service_fee': float(row.get('å¹³å°æœåŠ¡è´¹', 0)) if pd.notna(row.get('å¹³å°æœåŠ¡è´¹')) else 0.0,  # ä¿®å¤:æ·»åŠ å¹³å°æœåŠ¡è´¹å­—æ®µæ˜ å°„
            'user_paid_delivery_fee': float(row.get('ç”¨æˆ·æ”¯ä»˜é…é€è´¹', 0)) if pd.notna(row.get('ç”¨æˆ·æ”¯ä»˜é…é€è´¹')) else 0.0,
            'delivery_discount': float(row.get('é…é€è´¹å‡å…é‡‘é¢', 0)) if pd.notna(row.get('é…é€è´¹å‡å…é‡‘é¢')) else 0.0,
            'full_reduction': float(row.get('æ»¡å‡é‡‘é¢', 0)) if pd.notna(row.get('æ»¡å‡é‡‘é¢')) else 0.0,
            'product_discount': float(row.get('å•†å“å‡å…é‡‘é¢', 0)) if pd.notna(row.get('å•†å“å‡å…é‡‘é¢')) else 0.0,
            'merchant_voucher': float(row.get('å•†å®¶ä»£é‡‘åˆ¸', 0)) if pd.notna(row.get('å•†å®¶ä»£é‡‘åˆ¸')) else 0.0,
            'merchant_share': float(row.get('å•†å®¶æ‰¿æ‹…éƒ¨åˆ†åˆ¸', 0)) if pd.notna(row.get('å•†å®¶æ‰¿æ‹…éƒ¨åˆ†åˆ¸')) else 0.0,
            'packaging_fee': float(row.get('æ‰“åŒ…è¢‹é‡‘é¢', 0)) if pd.notna(row.get('æ‰“åŒ…è¢‹é‡‘é¢')) else 0.0,
            # âœ… æ–°å¢è¥é”€ç»´åº¦å­—æ®µ
            'gift_amount': float(row.get('æ»¡èµ é‡‘é¢', 0)) if pd.notna(row.get('æ»¡èµ é‡‘é¢')) else 0.0,
            'other_merchant_discount': float(row.get('å•†å®¶å…¶ä»–ä¼˜æƒ ', 0)) if pd.notna(row.get('å•†å®¶å…¶ä»–ä¼˜æƒ ')) else 0.0,
            'new_customer_discount': float(row.get('æ–°å®¢å‡å…é‡‘é¢', 0)) if pd.notna(row.get('æ–°å®¢å‡å…é‡‘é¢')) else 0.0,
            # âœ… æ–°å¢åˆ©æ¶¦ç»´åº¦å­—æ®µ
            'corporate_rebate': float(row.get('ä¼å®¢åè¿”', 0)) if pd.notna(row.get('ä¼å®¢åè¿”')) else 0.0,
            # âœ… é…é€å¹³å°å­—æ®µ
            'delivery_platform': str(row.get('é…é€å¹³å°', '')),
            # âœ… æ¢å¤delivery_distanceå’Œcityå­—æ®µæ˜ å°„ (Orderæ¨¡å‹å·²æ”¯æŒ)
            'delivery_distance': float(row.get('é…é€è·ç¦»', row.get('distance', row.get('è·ç¦»', 0)))) if pd.notna(row.get('é…é€è·ç¦»', row.get('distance', row.get('è·ç¦»')))) else 0.0,
            'city': str(row.get('åŸå¸‚', '')),
            'store_id': str(row.get('é—¨åº—ID', '')),
            'store_franchise_type': int(row.get('é—¨åº—åŠ ç›Ÿç±»å‹', 0)) if pd.notna(row.get('é—¨åº—åŠ ç›Ÿç±»å‹')) else None,
            
            'address': str(row.get('æ”¶è´§åœ°å€', '')),
            'channel': str(row.get('æ¸ é“', '')),
            'actual_price': float(row.get('å®æ”¶ä»·æ ¼', 0)) if pd.notna(row.get('å®æ”¶ä»·æ ¼')) else 0.0,
            # âœ… ä¿®å¤: å­˜å‚¨"é¢„è®¡è®¢å•æ”¶å…¥"è€Œä¸æ˜¯"è®¢å•é›¶å”®é¢"(ä¸migrate.pyä¿æŒä¸€è‡´)
            'amount': float(row.get('é¢„è®¡è®¢å•æ”¶å…¥', row.get('è®¢å•é›¶å”®é¢', 0))) if pd.notna(row.get('é¢„è®¡è®¢å•æ”¶å…¥', row.get('è®¢å•é›¶å”®é¢', 0))) else 0.0,
        }
    
    def validate_imported_data(self, file_path, df_source, imported_count):
        """éªŒè¯å¯¼å…¥æ•°æ®çš„å®Œæ•´æ€§"""
        print(f"\n5ï¸âƒ£ æ•°æ®å®Œæ•´æ€§æ ¡éªŒ...")
        
        file_name = os.path.basename(file_path)
        
        # 1. æ£€æŸ¥å¯¼å…¥æ•°é‡
        expected_count = len(df_source)
        if imported_count < expected_count:
            loss_ratio = (expected_count - imported_count) / expected_count * 100
            self.validation_report['warnings'].append(
                f"âš ï¸  {file_name}: æ•°æ®ä¸¢å¤± {expected_count - imported_count} æ¡ ({loss_ratio:.1f}%)"
            )
            print(f"   âš ï¸  å¯¼å…¥ç‡: {imported_count}/{expected_count} ({imported_count/expected_count*100:.1f}%)")
        else:
            print(f"   âœ… å¯¼å…¥ç‡: 100%")
        
        # 2. è·å–æ•°æ®åº“ä¸­çš„æ•°æ®
        if 'é—¨åº—åç§°' in df_source.columns:
            store_name = df_source['é—¨åº—åç§°'].iloc[0]
            db_orders = self.session.query(Order).filter(
                Order.store_name == store_name
            ).all()
            
            print(f"   æ•°æ®åº“è®°å½•æ•°: {len(db_orders):,}")
            
            # 3. éªŒè¯å…³é”®å­—æ®µ
            validation_fields = {
                'æˆæœ¬': ('cost', 'sum'),
                'åˆ©æ¶¦é¢': ('profit', 'sum'),
                'å•†å“å®å”®ä»·': ('price', 'sum'),
                'ç‰©æµé…é€è´¹': ('delivery_fee', 'sum'),
                'å¹³å°ä½£é‡‘': ('commission', 'sum'),
                'å¹³å°æœåŠ¡è´¹': ('platform_service_fee', 'sum'),  # æ·»åŠ å¹³å°æœåŠ¡è´¹éªŒè¯
            }
            
            all_fields_ok = True
            
            for excel_field, (db_field, agg_method) in validation_fields.items():
                if excel_field in df_source.columns:
                    # Excelå€¼
                    if agg_method == 'sum':
                        excel_value = df_source[excel_field].sum()
                    else:
                        excel_value = df_source[excel_field].mean()
                    
                    # æ•°æ®åº“å€¼
                    db_values = [getattr(o, db_field, 0) or 0 for o in db_orders]
                    if agg_method == 'sum':
                        db_value = sum(db_values)
                    else:
                        db_value = sum(db_values) / len(db_values) if db_values else 0
                    
                    # æ¯”è¾ƒ(å…è®¸0.01%è¯¯å·®)
                    if excel_value > 0:
                        diff_ratio = abs(db_value - excel_value) / excel_value * 100
                        if diff_ratio > 0.01:
                            all_fields_ok = False
                            self.validation_report['errors'].append(
                                f"âŒ {file_name}: {excel_field}å­—æ®µä¸åŒ¹é… "
                                f"(Excel:Â¥{excel_value:,.2f} vs DB:Â¥{db_value:,.2f}, å·®å¼‚{diff_ratio:.2f}%)"
                            )
                            print(f"   âŒ {excel_field}: Excel=Â¥{excel_value:,.2f}, DB=Â¥{db_value:,.2f} (å·®å¼‚{diff_ratio:.2f}%)")
                        else:
                            print(f"   âœ… {excel_field}: Â¥{db_value:,.2f}")
                    elif db_value == 0:
                        print(f"   âœ… {excel_field}: Â¥0.00")
                    else:
                        all_fields_ok = False
                        self.validation_report['warnings'].append(
                            f"âš ï¸  {file_name}: {excel_field}å­—æ®µå¼‚å¸¸ (Excelæ— æ•°æ®ä½†DBæœ‰Â¥{db_value:,.2f})"
                        )
                        print(f"   âš ï¸  {excel_field}: Excelæ— æ•°æ®, DB=Â¥{db_value:,.2f}")
            
            # 4. ç‰¹åˆ«æ£€æŸ¥æˆæœ¬å­—æ®µ(è¿™æ˜¯ä¹‹å‰çš„é—®é¢˜)
            if 'æˆæœ¬' in df_source.columns:
                source_cost_sum = df_source['æˆæœ¬'].sum()
                db_cost_sum = sum(o.cost or 0 for o in db_orders)
                
                if source_cost_sum > 0 and db_cost_sum == 0:
                    all_fields_ok = False
                    self.validation_report['errors'].append(
                        f"âŒ {file_name}: æˆæœ¬å­—æ®µå¯¼å…¥å¤±è´¥! Excelæœ‰Â¥{source_cost_sum:,.2f}ä½†æ•°æ®åº“ä¸ºÂ¥0"
                    )
                    print(f"\n   ğŸš¨ ä¸¥é‡è­¦å‘Š: æˆæœ¬å­—æ®µå¯¼å…¥å¤±è´¥!")
                    print(f"      Excelæˆæœ¬æ€»é¢: Â¥{source_cost_sum:,.2f}")
                    print(f"      æ•°æ®åº“æˆæœ¬æ€»é¢: Â¥{db_cost_sum:,.2f}")
            
            if all_fields_ok:
                print(f"\n   âœ… æ‰€æœ‰å­—æ®µæ ¡éªŒé€šè¿‡!")
            else:
                self.validation_report['success'] = False
                print(f"\n   âŒ éƒ¨åˆ†å­—æ®µæ ¡éªŒå¤±è´¥,è¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯!")
    
    def print_final_report(self):
        """æ‰“å°æœ€ç»ˆæŠ¥å‘Š"""
        print(f"\n{'='*70}")
        print(f" ğŸ“Š å¯¼å…¥å®Œæˆ - æœ€ç»ˆæŠ¥å‘Š")
        print(f"{'='*70}")
        
        if self.validation_report['success'] and not self.validation_report['errors']:
            print(f"\nâœ… æ‰€æœ‰æ•°æ®å¯¼å…¥æˆåŠŸ,æœªå‘ç°é”™è¯¯!\n")
        else:
            if self.validation_report['errors']:
                print(f"\nâŒ å‘ç° {len(self.validation_report['errors'])} ä¸ªé”™è¯¯:")
                for error in self.validation_report['errors']:
                    print(f"   {error}")
            
            if self.validation_report['warnings']:
                print(f"\nâš ï¸  å‘ç° {len(self.validation_report['warnings'])} ä¸ªè­¦å‘Š:")
                for warning in self.validation_report['warnings']:
                    print(f"   {warning}")
        
        print(f"\n{'='*70}\n")
    
    def run(self):
        """æ‰§è¡Œæ™ºèƒ½å¯¼å…¥"""
        try:
            # 1. æ‰«ææ–°æ–‡ä»¶
            new_files = self.scan_new_files()
            
            if not new_files:
                print(f"\nâœ… æ²¡æœ‰æ–°æ•°æ®éœ€è¦å¯¼å…¥!")
                return
            
            # 2. ç¡®è®¤å¯¼å…¥
            print(f"\nå‡†å¤‡å¯¼å…¥ {len(new_files)} ä¸ªæ–‡ä»¶")
            print("ğŸš€ è‡ªåŠ¨å¼€å§‹å¯¼å…¥...")
            
            # 3. é€ä¸ªå¯¼å…¥
            success_files = []
            failed_files = []
            
            for file_path, signature in new_files:
                if self.import_file(file_path, signature):
                    success_files.append(file_path)
                else:
                    failed_files.append(file_path)
            
            # 4. ä¿å­˜å¯¼å…¥å†å²
            self.save_import_history()
            
            # 5. æ‰“å°æœ€ç»ˆæŠ¥å‘Š
            self.print_final_report()
            
            print(f"å¯¼å…¥ç»Ÿè®¡:")
            print(f"  æˆåŠŸ: {len(success_files)}")
            print(f"  å¤±è´¥: {len(failed_files)}")
            
        finally:
            self.session.close()

def main():
    """ä¸»å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                  â•‘
â•‘           ğŸš€ æ™ºèƒ½é—¨åº—æ•°æ®å¯¼å…¥ç³»ç»Ÿ v2.0                           â•‘
â•‘                                                                  â•‘
â•‘  åŠŸèƒ½:                                                           â•‘
â•‘    âœ… è‡ªåŠ¨è¯†åˆ«æ–°å¢æ•°æ®æ–‡ä»¶                                       â•‘
â•‘    âœ… é¿å…é‡å¤å¯¼å…¥è€æ•°æ®                                         â•‘
â•‘    âœ… è‡ªåŠ¨æ•°æ®å®Œæ•´æ€§æ ¡éªŒ                                         â•‘
â•‘    âœ… å¯¼å…¥é—®é¢˜å®æ—¶æŠ¥è­¦                                           â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    importer = SmartImporter()
    importer.run()

if __name__ == "__main__":
    main()
