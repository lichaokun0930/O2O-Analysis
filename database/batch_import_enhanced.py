# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ‰¹é‡æ•°æ®å¯¼å…¥å·¥å…· v2.0

åŠŸèƒ½ç‰¹æ€§ï¼š
- æ”¯æŒå¢é‡å¯¼å…¥ï¼ˆåªå¯¼å…¥æ–°æ•°æ®ï¼‰
- æ”¯æŒå…¨é‡æ›¿æ¢ï¼ˆåˆ é™¤æ—§æ•°æ®åå¯¼å…¥ï¼‰
- è‡ªåŠ¨è¯†åˆ«é—¨åº—åç§°
- è¯¦ç»†çš„å¯¼å…¥æŠ¥å‘Š
- é”™è¯¯å¤„ç†å’Œå›æ»š
- å¯¼å…¥å†å²è®°å½•

ä½¿ç”¨æ–¹å¼ï¼š
    python -m database.batch_import_enhanced --path ./å®é™…æ•°æ® --mode incremental
"""

import sys
import os
from pathlib import Path
import pandas as pd
from datetime import datetime
import glob
import hashlib
import argparse
from typing import List, Dict, Any, Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from database.connection import SessionLocal, init_database
from database.models import Order, DataUploadHistory
from sqlalchemy import func, text

# å°è¯•å¯¼å…¥æ•°æ®å¤„ç†å™¨
try:
    from çœŸå®æ•°æ®å¤„ç†å™¨ import RealDataProcessor
    DATA_PROCESSOR_AVAILABLE = True
except ImportError:
    DATA_PROCESSOR_AVAILABLE = False
    print("âš ï¸ çœŸå®æ•°æ®å¤„ç†å™¨æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨åŸºç¡€å­—æ®µæ˜ å°„")


class BatchDataImporterEnhanced:
    """å¢å¼ºç‰ˆæ‰¹é‡æ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(self, data_dir: str, mode: str = "incremental"):
        """
        åˆå§‹åŒ–å¯¼å…¥å™¨
        
        Args:
            data_dir: æ•°æ®æ–‡ä»¶ç›®å½•
            mode: å¯¼å…¥æ¨¡å¼ - incremental(å¢é‡) / replace(æ›¿æ¢)
        """
        self.data_dir = data_dir
        self.mode = mode
        self.processor = RealDataProcessor() if DATA_PROCESSOR_AVAILABLE else None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'files_total': 0,
            'files_success': 0,
            'files_failed': 0,
            'files_skipped': 0,
            'orders_inserted': 0,
            'orders_updated': 0,
            'orders_skipped': 0,
            'errors': []
        }
        
        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        init_database()
        
        # âœ… éªŒè¯é¢„èšåˆè¡¨ç»“æ„ï¼ˆé˜²æ­¢åŒæ­¥å¤±è´¥ï¼‰
        self._validate_schema()
    
    def _validate_schema(self):
        """éªŒè¯é¢„èšåˆè¡¨ç»“æ„ï¼Œè‡ªåŠ¨ä¿®å¤ç¼ºå¤±å­—æ®µ"""
        try:
            from backend.app.services.schema_validator import SchemaValidator
            success, messages = SchemaValidator.validate_and_fix_all()
            for msg in messages:
                print(f"   {msg}")
            if not success:
                print("âš ï¸ é¢„èšåˆè¡¨ç»“æ„å­˜åœ¨é—®é¢˜ï¼ŒåŒæ­¥å¯èƒ½å¤±è´¥")
        except ImportError:
            # éªŒè¯å™¨ä¸å­˜åœ¨æ—¶è·³è¿‡
            pass
        except Exception as e:
            print(f"âš ï¸ è¡¨ç»“æ„éªŒè¯å¤±è´¥: {e}")
    
    def find_excel_files(self) -> List[str]:
        """æŸ¥æ‰¾æ‰€æœ‰ Excel æ–‡ä»¶"""
        patterns = ['*.xlsx', '*.xls']
        files = []
        
        for pattern in patterns:
            files.extend(glob.glob(f"{self.data_dir}/**/{pattern}", recursive=True))
        
        # è¿‡æ»¤ä¸´æ—¶æ–‡ä»¶ï¼ˆä»¥ ~$ å¼€å¤´ï¼‰
        files = [f for f in files if not os.path.basename(f).startswith('~$')]
        
        return sorted(files)
    
    def calculate_file_hash(self, filepath: str) -> str:
        """è®¡ç®—æ–‡ä»¶ MD5 å“ˆå¸Œ"""
        hash_md5 = hashlib.md5()
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def check_file_imported(self, filepath: str, file_hash: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¯¼å…¥è¿‡"""
        session = SessionLocal()
        try:
            existing = session.query(DataUploadHistory).filter(
                DataUploadHistory.file_hash == file_hash
            ).first()
            
            if existing:
                # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦çœŸçš„æœ‰æ•°æ®
                order_count = session.query(func.count(Order.id)).scalar() or 0
                
                if order_count == 0:
                    # æ•°æ®åº“ä¸ºç©ºä½†æœ‰å¯¼å…¥å†å²ï¼Œè¯´æ˜æ•°æ®è¢«åˆ é™¤äº†ï¼Œè‡ªåŠ¨æ¸…ç†å†å²è®°å½•
                    print(f"   âš ï¸ æ£€æµ‹åˆ°æ•°æ®åº“ä¸ºç©ºä½†å­˜åœ¨å¯¼å…¥å†å²ï¼Œè‡ªåŠ¨æ¸…ç†...")
                    session.query(DataUploadHistory).delete()
                    session.commit()
                    print(f"   âœ… å·²è‡ªåŠ¨æ¸…ç†å¯¼å…¥å†å²è®°å½•")
                    return False
                
                return True
            return False
        finally:
            session.close()
    
    def extract_store_name(self, df: pd.DataFrame, filename: str) -> str:
        """ä»æ•°æ®æˆ–æ–‡ä»¶åä¸­æå–é—¨åº—åç§°"""
        # ä¼˜å…ˆä»æ•°æ®ä¸­è·å–
        if 'é—¨åº—åç§°' in df.columns:
            store_names = df['é—¨åº—åç§°'].dropna().unique()
            if len(store_names) > 0:
                return str(store_names[0])
        
        # ä»æ–‡ä»¶åæå–ï¼ˆå¦‚æœåŒ…å«é—¨åº—åç§°ï¼‰
        # ä¾‹å¦‚ï¼šæƒ å®œé€‰-æ³°å·æ³°å…´åº—_2025-01.xlsx
        basename = os.path.basename(filename)
        if '_' in basename:
            potential_store = basename.split('_')[0]
            if len(potential_store) > 2:
                return potential_store
        
        return "æœªçŸ¥é—¨åº—"
    
    def standardize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ– DataFrame å­—æ®µ"""
        if self.processor:
            return self.processor.standardize_sales_data(df)
        
        # åŸºç¡€å­—æ®µæ˜ å°„
        column_mapping = {
            'è®¢å•å·': 'è®¢å•ID',
            'è®¢å•ç¼–å·': 'è®¢å•ID',
            'ä¸‹å•æ—¥æœŸ': 'ä¸‹å•æ—¶é—´',
            'è®¢å•æ—¶é—´': 'ä¸‹å•æ—¶é—´',
            'é‡‡é›†æ—¶é—´': 'ä¸‹å•æ—¶é—´',
            'å“å': 'å•†å“åç§°',
            'å•†å“': 'å•†å“åç§°',
            'å®ä»˜é‡‘é¢': 'å®æ”¶ä»·æ ¼',
            'å®ä»˜': 'å®æ”¶ä»·æ ¼',
        }
        
        df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})
        return df
    
    def import_orders(self, df: pd.DataFrame, store_names: List[str]) -> Tuple[int, int, int]:
        """
        å¯¼å…¥è®¢å•æ•°æ®ï¼ˆæ”¯æŒå¤šé—¨åº—èšåˆè¡¨ï¼‰
        
        Args:
            df: è®¢å•æ•°æ® DataFrame
            store_names: é—¨åº—åç§°åˆ—è¡¨
        
        Returns:
            (inserted, updated, skipped)
        
        æ³¨æ„ï¼š
        - åŒä¸€è®¢å•ä¸­åŒä¸€å•†å“å¯èƒ½å‡ºç°å¤šæ¬¡ï¼ˆé¡¾å®¢è´­ä¹°å¤šä»½ï¼‰ï¼Œè¿™äº›éƒ½æ˜¯æœ‰æ•ˆæ•°æ®
        - ä¸å†ä½¿ç”¨ è®¢å•ID+å•†å“åç§° å»é‡ï¼Œæ”¹ä¸ºå…¨é‡å¯¼å…¥
        - å¢é‡æ¨¡å¼ä¸‹ï¼Œåªæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¯¼å…¥è¿‡ï¼Œä¸æ£€æŸ¥å•æ¡è®°å½•
        """
        session = SessionLocal()
        inserted = 0
        updated = 0
        skipped = 0
        
        try:
            # æ›¿æ¢æ¨¡å¼ï¼šå…ˆåˆ é™¤æ‰€æœ‰æ¶‰åŠé—¨åº—çš„æ—§æ•°æ®
            if self.mode == "replace":
                total_deleted = 0
                for store_name in store_names:
                    deleted = session.query(Order).filter(Order.store_name == store_name).delete()
                    if deleted > 0:
                        print(f"   ğŸ—‘ï¸ åˆ é™¤ {store_name}: {deleted:,} æ¡")
                        total_deleted += deleted
                session.commit()
                if total_deleted > 0:
                    print(f"   ğŸ—‘ï¸ åˆ é™¤æ—§æ•°æ®æ€»è®¡: {total_deleted:,} æ¡")
            
            # è·å–æ—¥æœŸåˆ—
            date_col = None
            for col in ['æ—¥æœŸ', 'ä¸‹å•æ—¶é—´', 'é‡‡é›†æ—¶é—´', 'date']:
                if col in df.columns:
                    date_col = col
                    break
            
            if not date_col:
                raise ValueError("æœªæ‰¾åˆ°æ—¥æœŸåˆ—")
            
            # æ‰¹é‡å¤„ç†
            batch_size = 5000
            total_rows = len(df)
            
            for batch_start in range(0, total_rows, batch_size):
                batch_end = min(batch_start + batch_size, total_rows)
                batch_df = df.iloc[batch_start:batch_end]
                
                orders_to_insert = []
                
                for _, row in batch_df.iterrows():
                    order_id = str(row.get('è®¢å•ID', ''))
                    if not order_id or order_id == 'nan':
                        skipped += 1
                        continue
                    
                    # æ³¨æ„ï¼šä¸å†è¿›è¡Œå•æ¡è®°å½•å»é‡
                    # åŸå› ï¼šåŒä¸€è®¢å•ä¸­åŒä¸€å•†å“å¯èƒ½å‡ºç°å¤šæ¬¡ï¼ˆå¦‚é¡¾å®¢ä¹°äº†2ä»½åŒæ ·çš„å•†å“ï¼‰
                    # è¿™äº›éƒ½æ˜¯æœ‰æ•ˆçš„é”€å”®æ•°æ®ï¼Œä¸åº”è¯¥è¢«è·³è¿‡
                    # å¢é‡æ¨¡å¼çš„å»é‡é€šè¿‡æ–‡ä»¶å“ˆå¸Œæ£€æŸ¥å®ç°ï¼ˆcheck_file_importedï¼‰
                    
                    # è§£ææ—¥æœŸ
                    order_date = None
                    if pd.notna(row.get(date_col)):
                        try:
                            order_date = pd.to_datetime(row[date_col])
                            if pd.isna(order_date):
                                order_date = None
                        except:
                            order_date = None
                    
                    if order_date is None:
                        skipped += 1
                        continue
                    
                    # è¾…åŠ©å‡½æ•°
                    def safe_float(val, default=0):
                        if pd.isna(val): return default
                        try: return float(val)
                        except: return default
                    
                    def safe_int(val, default=0):
                        if pd.isna(val): return default
                        try: return int(val)
                        except: return default
                    
                    def safe_str(val, default=''):
                        if pd.isna(val): return default
                        return str(val)
                    
                    # åˆ›å»ºè®¢å•å¯¹è±¡ï¼ˆä»æ¯è¡Œæ•°æ®è·å–é—¨åº—åç§°ï¼Œæ”¯æŒå¤šé—¨åº—èšåˆè¡¨ï¼‰
                    row_store_name = safe_str(row.get('é—¨åº—åç§°', ''))
                    if not row_store_name:
                        row_store_name = store_names[0] if store_names else 'æœªçŸ¥é—¨åº—'
                    
                    order = Order(
                        order_id=order_id,
                        order_number=safe_str(row.get('è®¢å•ç¼–å·', '')),
                        store_name=row_store_name,
                        product_name=safe_str(row.get('å•†å“åç§°', '')),
                        date=order_date,
                        channel=safe_str(row.get('æ¸ é“', '')),
                        address=safe_str(row.get('æ”¶è´§åœ°å€', '')),
                        
                        # åˆ†ç±»
                        category_level1=safe_str(row.get('ä¸€çº§åˆ†ç±»å', row.get('ä¸€çº§åˆ†ç±»', ''))),
                        category_level3=safe_str(row.get('ä¸‰çº§åˆ†ç±»å', row.get('ä¸‰çº§åˆ†ç±»', ''))),
                        
                        # ä»·æ ¼å’Œæˆæœ¬
                        price=safe_float(row.get('å•†å“å®å”®ä»·', 0)),
                        original_price=safe_float(row.get('å•†å“åŸä»·', 0)),
                        cost=safe_float(row.get('å•†å“é‡‡è´­æˆæœ¬', row.get('æˆæœ¬', 0))),
                        actual_price=safe_float(row.get('å®æ”¶ä»·æ ¼', 0)),
                        
                        # é”€é‡å’Œé‡‘é¢
                        quantity=safe_int(row.get('é”€é‡', row.get('æœˆå”®', 1)), 1),
                        stock=safe_int(row.get('åº“å­˜', 0)),
                        remaining_stock=safe_float(row.get('å‰©ä½™åº“å­˜', row.get('åº“å­˜', 0))),
                        amount=safe_float(row.get('é¢„è®¡è®¢å•æ”¶å…¥', row.get('è®¢å•é›¶å”®é¢', row.get('é”€å”®é¢', 0)))),
                        profit=safe_float(row.get('åˆ©æ¶¦é¢', row.get('å®é™…åˆ©æ¶¦', row.get('åˆ©æ¶¦', 0)))),
                        
                        # è´¹ç”¨
                        delivery_fee=safe_float(row.get('ç‰©æµé…é€è´¹', 0)),
                        commission=safe_float(row.get('å¹³å°ä½£é‡‘', 0)),
                        platform_service_fee=safe_float(row.get('å¹³å°æœåŠ¡è´¹', row.get('å¹³å°ä½£é‡‘', 0))),
                        
                        # è¥é”€æ´»åŠ¨è´¹ç”¨
                        user_paid_delivery_fee=safe_float(row.get('ç”¨æˆ·æ”¯ä»˜é…é€è´¹', 0)),
                        delivery_discount=safe_float(row.get('é…é€è´¹å‡å…é‡‘é¢', 0)),
                        full_reduction=safe_float(row.get('æ»¡å‡é‡‘é¢', 0)),
                        product_discount=safe_float(row.get('å•†å“å‡å…é‡‘é¢', 0)),
                        merchant_voucher=safe_float(row.get('å•†å®¶ä»£é‡‘åˆ¸', 0)),
                        merchant_share=safe_float(row.get('å•†å®¶æ‰¿æ‹…éƒ¨åˆ†åˆ¸', 0)),
                        packaging_fee=safe_float(row.get('æ‰“åŒ…è¢‹é‡‘é¢', 0)),
                        gift_amount=safe_float(row.get('æ»¡èµ é‡‘é¢', 0)),
                        other_merchant_discount=safe_float(row.get('å•†å®¶å…¶ä»–ä¼˜æƒ ', 0)),
                        new_customer_discount=safe_float(row.get('æ–°å®¢å‡å…é‡‘é¢', 0)),
                        
                        # åˆ©æ¶¦è¡¥å¿é¡¹
                        corporate_rebate=safe_float(row.get('ä¼å®¢åè¿”', 0)),
                        
                        # é…é€ä¿¡æ¯
                        delivery_distance=safe_float(row.get('é…é€è·ç¦»', 0)),
                        delivery_platform=safe_str(row.get('é…é€å¹³å°', '')),
                        
                        # é—¨åº—ä¿¡æ¯
                        store_id=safe_str(row.get('é—¨åº—ID', '')),
                        city=safe_str(row.get('åŸå¸‚åç§°', row.get('åŸå¸‚', ''))),
                        
                        # æ¡ç 
                        barcode=safe_str(row.get('æ¡ç ', '')),
                        store_code=safe_str(row.get('åº—å†…ç ', '')),
                    )
                    orders_to_insert.append(order)
                    inserted += 1
                
                # æ‰¹é‡æ’å…¥
                if orders_to_insert:
                    session.bulk_save_objects(orders_to_insert)
                    session.commit()
                
                # æ˜¾ç¤ºè¿›åº¦
                progress = min(batch_end, total_rows)
                print(f"   ğŸ“Š è¿›åº¦: {progress:,}/{total_rows:,} ({progress*100//total_rows}%)", end='\r')
            
            print()  # æ¢è¡Œ
            return inserted, updated, skipped
            
        except Exception as e:
            session.rollback()
            raise e
        finally:
            session.close()
    
    def log_upload_history(self, filename: str, file_hash: str, file_size: int,
                          rows_imported: int, success: bool, error_msg: str = None):
        """è®°å½•ä¸Šä¼ å†å²"""
        session = SessionLocal()
        try:
            history = DataUploadHistory(
                file_name=os.path.basename(filename),
                file_size=file_size,
                file_hash=file_hash,
                rows_imported=rows_imported,
                rows_failed=0 if success else rows_imported,
                success=success,
                error_log=error_msg,
                uploaded_at=datetime.now()
            )
            session.add(history)
            session.commit()
        except Exception as e:
            print(f"   âš ï¸ è®°å½•ä¸Šä¼ å†å²å¤±è´¥: {e}")
            session.rollback()
        finally:
            session.close()
    
    def import_file(self, filepath: str) -> bool:
        """å¯¼å…¥å•ä¸ªæ–‡ä»¶"""
        filename = os.path.basename(filepath)
        file_size = os.path.getsize(filepath)
        
        print(f"\n{'='*60}")
        print(f"ğŸ“„ {filename}")
        print(f"{'='*60}")
        
        try:
            # 1. è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            file_hash = self.calculate_file_hash(filepath)
            
            # 2. æ£€æŸ¥æ˜¯å¦å·²å¯¼å…¥ï¼ˆå¢é‡æ¨¡å¼ï¼‰
            if self.mode == "incremental" and self.check_file_imported(filepath, file_hash):
                print(f"   â­ï¸ æ–‡ä»¶å·²å¯¼å…¥è¿‡ï¼Œè·³è¿‡")
                self.stats['files_skipped'] += 1
                return True
            
            # 3. åŠ è½½ Excel
            print(f"   ğŸ“– åŠ è½½æ–‡ä»¶...")
            df = pd.read_excel(filepath)
            original_rows = len(df)
            print(f"   ğŸ“Š åŸå§‹æ•°æ®: {original_rows:,} è¡Œ")
            
            # 4. æ ‡å‡†åŒ–å­—æ®µ
            print(f"   ğŸ”§ æ ‡å‡†åŒ–å­—æ®µ...")
            df = self.standardize_dataframe(df)
            
            # 5. ä¸šåŠ¡è¿‡æ»¤ï¼ˆæ’é™¤è€—æç­‰ï¼‰
            if 'ä¸€çº§åˆ†ç±»å' in df.columns:
                df = df[df['ä¸€çº§åˆ†ç±»å'] != 'è€—æ'].copy()
            filtered_rows = len(df)
            if filtered_rows < original_rows:
                print(f"   ğŸ” è¿‡æ»¤å: {filtered_rows:,} è¡Œ")
            
            # 6. æå–é—¨åº—åç§°åˆ—è¡¨ï¼ˆæ”¯æŒå¤šé—¨åº—èšåˆè¡¨ï¼‰
            if 'é—¨åº—åç§°' in df.columns:
                store_names = df['é—¨åº—åç§°'].dropna().unique().tolist()
            else:
                store_names = [self.extract_store_name(df, filepath)]
            
            if len(store_names) == 1:
                print(f"   ğŸª é—¨åº—: {store_names[0]}")
            else:
                print(f"   ğŸª é—¨åº—: {len(store_names)} ä¸ª (èšåˆè¡¨)")
                for s in store_names[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                    print(f"      â€¢ {s}")
                if len(store_names) > 5:
                    print(f"      ... è¿˜æœ‰ {len(store_names) - 5} ä¸ª")
            
            # 7. å¯¼å…¥æ•°æ®
            print(f"   ğŸ’¾ å¯¼å…¥æ•°æ®åº“...")
            inserted, updated, skipped = self.import_orders(df, store_names)
            
            # 8. è®°å½•å†å²
            self.log_upload_history(filepath, file_hash, file_size, inserted, True)
            
            # 9. æ›´æ–°ç»Ÿè®¡
            self.stats['files_success'] += 1
            self.stats['orders_inserted'] += inserted
            self.stats['orders_updated'] += updated
            self.stats['orders_skipped'] += skipped
            
            # 10. è®°å½•éœ€è¦æ›´æ–°é¢„èšåˆè¡¨çš„é—¨åº—
            if not hasattr(self, 'stores_to_sync'):
                self.stores_to_sync = set()
            self.stores_to_sync.update(store_names)
            
            print(f"   âœ… å®Œæˆ: æ–°å¢ {inserted:,}, è·³è¿‡ {skipped:,}")
            return True
            
        except Exception as e:
            error_msg = str(e)
            print(f"   âŒ å¤±è´¥: {error_msg[:100]}")
            self.stats['files_failed'] += 1
            self.stats['errors'].append({'file': filename, 'error': error_msg})
            self.log_upload_history(filepath, '', file_size, 0, False, error_msg)
            return False
    
    def run(self):
        """æ‰§è¡Œæ‰¹é‡å¯¼å…¥"""
        print("\n" + "="*60)
        print("ğŸ“¦ æ‰¹é‡æ•°æ®å¯¼å…¥å·¥å…· v2.0")
        print("="*60)
        
        # æŸ¥æ‰¾æ–‡ä»¶
        files = self.find_excel_files()
        self.stats['files_total'] = len(files)
        
        if not files:
            print(f"\nâŒ æœªæ‰¾åˆ° Excel æ–‡ä»¶: {self.data_dir}")
            return
        
        print(f"\nğŸ“‚ æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"ğŸ“Š å‘ç° {len(files)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“‹ å¯¼å…¥æ¨¡å¼: {self.mode}")
        
        # é€ä¸ªå¯¼å…¥
        for filepath in files:
            self.import_file(filepath)
        
        # âœ… è‡ªåŠ¨æ›´æ–°é¢„èšåˆè¡¨
        if hasattr(self, 'stores_to_sync') and self.stores_to_sync:
            print("\n" + "="*60)
            print("ğŸ”„ è‡ªåŠ¨æ›´æ–°é¢„èšåˆè¡¨")
            print("="*60)
            try:
                from backend.app.services.aggregation_sync_service import AggregationSyncService
                AggregationSyncService.sync_store_data(list(self.stores_to_sync), async_mode=False)
            except ImportError:
                # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œå°è¯•ç›´æ¥æ‰§è¡ŒSQL
                print("âš ï¸ æ— æ³•å¯¼å…¥åŒæ­¥æœåŠ¡ï¼Œå°è¯•ç›´æ¥æ›´æ–°...")
                self._sync_aggregation_tables(list(self.stores_to_sync))
        
        # âœ… å¯¼å…¥å®Œæˆåæ‰§è¡Œä¸€è‡´æ€§æ£€æŸ¥ï¼ˆç¡®ä¿æ‰€æœ‰é—¨åº—æ•°æ®éƒ½å·²åŒæ­¥ï¼‰
        self._run_consistency_check()
        
        # æ‰“å°ç»Ÿè®¡
        self.print_summary()
    
    def _run_consistency_check(self):
        """å¯¼å…¥å®Œæˆåæ‰§è¡Œä¸€è‡´æ€§æ£€æŸ¥ï¼Œç¡®ä¿æ‰€æœ‰é—¨åº—æ•°æ®éƒ½å·²åŒæ­¥"""
        print("\n" + "="*60)
        print("ğŸ” æ‰§è¡Œé¢„èšåˆè¡¨ä¸€è‡´æ€§æ£€æŸ¥")
        print("="*60)
        
        try:
            from backend.app.services.aggregation_consistency_service import aggregation_consistency_service
            result = aggregation_consistency_service.check_and_repair()
            
            check = result.get("check_result", {})
            repair = result.get("repair_result")
            
            if check.get("consistent"):
                print(f"âœ… é¢„èšåˆè¡¨ä¸€è‡´: {len(check.get('order_stores', []))} ä¸ªé—¨åº—")
            elif repair:
                synced = repair.get("synced_stores", [])
                failed = repair.get("failed_stores", [])
                if synced:
                    print(f"âœ… å·²ä¿®å¤: åŒæ­¥ {len(synced)} ä¸ªé—¨åº—")
                if failed:
                    print(f"âš ï¸ åŒæ­¥å¤±è´¥: {len(failed)} ä¸ªé—¨åº—")
                    for f in failed[:3]:
                        print(f"   - {f['store']}: {f['error'][:50]}")
            else:
                print("âš ï¸ å­˜åœ¨ä¸ä¸€è‡´ä½†æ— éœ€ä¿®å¤")
                
        except ImportError as e:
            print(f"âš ï¸ æ— æ³•å¯¼å…¥ä¸€è‡´æ€§æ£€æŸ¥æœåŠ¡: {e}")
        except Exception as e:
            print(f"âŒ ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
    
    def _sync_aggregation_tables(self, store_names: list):
        """å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥æ‰§è¡ŒSQLæ›´æ–°é¢„èšåˆè¡¨"""
        if not store_names:
            return
        
        session = SessionLocal()
        store_list = "', '".join(store_names)
        
        try:
            # 1. åˆ é™¤æ—§æ•°æ®
            tables = ['store_daily_summary', 'store_hourly_summary', 'category_daily_summary', 
                     'delivery_summary', 'product_daily_summary']
            for table in tables:
                try:
                    result = session.execute(
                        text(f"DELETE FROM {table} WHERE store_name IN ('{store_list}')")
                    )
                    if result.rowcount > 0:
                        print(f"   ğŸ—‘ï¸ {table}: åˆ é™¤ {result.rowcount} æ¡")
                except:
                    pass
            session.commit()
            
            # 2. è°ƒç”¨å…¨é‡é‡å»ºè„šæœ¬ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            import subprocess
            result = subprocess.run(
                ['python', 'å…¨çœ‹æ¿æ€§èƒ½ä¼˜åŒ–å®æ–½.py'],
                cwd=str(PROJECT_ROOT),
                capture_output=True,
                text=True,
                timeout=300
            )
            if result.returncode == 0:
                print("   âœ… é¢„èšåˆè¡¨æ›´æ–°å®Œæˆ")
            else:
                print(f"   âš ï¸ é¢„èšåˆè¡¨æ›´æ–°å¯èƒ½å¤±è´¥: {result.stderr[:200]}")
        except Exception as e:
            print(f"   âŒ é¢„èšåˆè¡¨æ›´æ–°å¤±è´¥: {e}")
        finally:
            session.close()
    
    def print_summary(self):
        """æ‰“å°å¯¼å…¥ç»Ÿè®¡"""
        print("\n" + "="*60)
        print("ğŸ“Š å¯¼å…¥ç»Ÿè®¡")
        print("="*60)
        print(f"æ–‡ä»¶æ€»æ•°: {self.stats['files_total']}")
        print(f"  âœ… æˆåŠŸ: {self.stats['files_success']}")
        print(f"  â­ï¸ è·³è¿‡: {self.stats['files_skipped']}")
        print(f"  âŒ å¤±è´¥: {self.stats['files_failed']}")
        print(f"\nè®¢å•ç»Ÿè®¡:")
        print(f"  ğŸ“¥ æ–°å¢: {self.stats['orders_inserted']:,}")
        print(f"  ğŸ”„ æ›´æ–°: {self.stats['orders_updated']:,}")
        print(f"  â­ï¸ è·³è¿‡: {self.stats['orders_skipped']:,}")
        
        if self.stats['errors']:
            print(f"\nâŒ é”™è¯¯è¯¦æƒ…:")
            for err in self.stats['errors'][:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                print(f"  - {err['file']}: {err['error'][:50]}")
        
        # æ˜¾ç¤ºæ•°æ®åº“æ€»é‡
        session = SessionLocal()
        try:
            total_orders = session.query(func.count(Order.id)).scalar()
            total_stores = session.query(func.count(func.distinct(Order.store_name))).scalar()
            print(f"\nğŸ“ˆ æ•°æ®åº“æ€»é‡:")
            print(f"  è®¢å•: {total_orders:,}")
            print(f"  é—¨åº—: {total_stores}")
        finally:
            session.close()


def main():
    parser = argparse.ArgumentParser(description='æ‰¹é‡å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“')
    parser.add_argument('--path', '-p', default='./å®é™…æ•°æ®', help='æ•°æ®æ–‡ä»¶ç›®å½•')
    parser.add_argument('--mode', '-m', choices=['incremental', 'replace'], 
                       default='incremental', help='å¯¼å…¥æ¨¡å¼')
    
    args = parser.parse_args()
    
    importer = BatchDataImporterEnhanced(
        data_dir=args.path,
        mode=args.mode
    )
    importer.run()


if __name__ == "__main__":
    main()
