# -*- coding: utf-8 -*-
"""
åå°ä»»åŠ¡æ¨¡å— - V8.1 ä¼ä¸šçº§æ€§èƒ½ä¼˜åŒ–

åŠŸèƒ½:
- å®šæ—¶é¢„è®¡ç®—è¯Šæ–­æ•°æ®
- å®šæ—¶é¢„è®¡ç®—å•†å“è¯„åˆ†æ•°æ®
- å°†ç»“æœç¼“å­˜åˆ°Redis

è®¾è®¡ç†å¿µ:
- ç”¨æˆ·è®¿é—®æ—¶ç›´æ¥è¯»ç¼“å­˜(<1ç§’)
- åå°æ¯5åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡
- é¿å…é˜»å¡ç”¨æˆ·è¯·æ±‚

ä½œè€…: AI Assistant
ç‰ˆæœ¬: V8.1
æ—¥æœŸ: 2025-12-11
"""

from apscheduler.schedulers.background import BackgroundScheduler
from datetime import datetime
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
APP_DIR = Path(__file__).resolve().parent
sys.path.insert(0, str(APP_DIR))

# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
_scheduler = None


def update_diagnosis_cache():
    """
    æ›´æ–°è¯Šæ–­æ•°æ®ç¼“å­˜ - V8.4åˆ†å±‚ç¼“å­˜+æ™ºèƒ½é¢„çƒ­
    
    æ‰§è¡Œæµç¨‹:
    1. ä»æ•°æ®åº“åŠ è½½æ•°æ®
    2. åˆ†æçƒ­ç‚¹é—¨åº—ï¼ˆåŸºäºè®¿é—®æ—¥å¿—ï¼‰
    3. ä¼˜å…ˆé¢„çƒ­çƒ­ç‚¹é—¨åº—ï¼ˆå¹¶è¡Œï¼‰
    4. åå°æ¸è¿›å¼é¢„çƒ­å…¶ä»–é—¨åº—
    
    ç­–ç•¥:
    - ä½¿ç”¨åˆ†å±‚ç¼“å­˜æ¶æ„
    - çƒ­ç‚¹é—¨åº—ä¼˜å…ˆï¼ˆ80/20åŸåˆ™ï¼‰
    - å¹¶è¡Œé¢„çƒ­ï¼ˆå¤šçº¿ç¨‹ï¼‰
    - å‹ç¼©å­˜å‚¨ï¼ˆèŠ‚çœå†…å­˜ï¼‰
    """
    try:
        print(f"\n{'='*80}")
        print(f"[åå°ä»»åŠ¡] V8.4åˆ†å±‚ç¼“å­˜æ™ºèƒ½é¢„çƒ­ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
        
        import time
        import pandas as pd
        from concurrent.futures import ThreadPoolExecutor, as_completed
        start_time = time.time()
        
        # å¯¼å…¥åˆ†å±‚ç¼“å­˜ç®¡ç†å™¨
        from hierarchical_cache_manager import get_hierarchical_cache
        hierarchical_cache = get_hierarchical_cache()
        
        if not hierarchical_cache.enabled:
            print("[åå°ä»»åŠ¡] âš ï¸ åˆ†å±‚ç¼“å­˜æœªå¯ç”¨")
            return
        
        # è·å–å…¨å±€æ•°æ®
        try:
            from æ™ºèƒ½é—¨åº—çœ‹æ¿_Dashç‰ˆ import GLOBAL_DATA
            if GLOBAL_DATA is None or GLOBAL_DATA.empty:
                print("[åå°ä»»åŠ¡] âš ï¸ GLOBAL_DATAä¸ºç©ºï¼Œè·³è¿‡æ›´æ–°")
                return
            
            df = GLOBAL_DATA.copy()
            print(f"[åå°ä»»åŠ¡] æ•°æ®è¡Œæ•°: {len(df):,}")
            
        except Exception as e:
            print(f"[åå°ä»»åŠ¡] âŒ è·å–GLOBAL_DATAå¤±è´¥: {e}")
            return
        
        # å¯¼å…¥è¯Šæ–­è®¡ç®—å‡½æ•°
        from components.today_must_do.diagnosis_analysis import get_diagnosis_summary
        
        # è·å–æ—¥æœŸèŒƒå›´
        date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in df.columns else 'ä¸‹å•æ—¶é—´'
        dates = pd.to_datetime(df[date_col])
        date_range = (dates.min().strftime('%Y-%m-%d'), dates.max().strftime('%Y-%m-%d'))
        
        cached_count = 0
        
        # ===== é˜¶æ®µ1: é¢„çƒ­å…¨å±€æ•°æ® =====
        print(f"\n[é˜¶æ®µ1] é¢„çƒ­å…¨å±€æ•°æ®...")
        try:
            diagnosis = get_diagnosis_summary(df)
            hierarchical_cache.cache_diagnosis(
                store_ids=[],  # ç©ºåˆ—è¡¨è¡¨ç¤ºå…¨å±€
                date_range=date_range,
                diagnosis=diagnosis,
                ttl=3600
            )
            cached_count += 1
            print(f"[é˜¶æ®µ1] âœ… å…¨å±€æ•°æ®å·²ç¼“å­˜")
        except Exception as e:
            print(f"[é˜¶æ®µ1] âš ï¸ å…¨å±€æ•°æ®ç¼“å­˜å¤±è´¥: {e}")
        
        # ===== é˜¶æ®µ2: åˆ†æçƒ­ç‚¹é—¨åº— =====
        if 'é—¨åº—åç§°' not in df.columns:
            print("[åå°ä»»åŠ¡] âš ï¸ æ— é—¨åº—å­—æ®µï¼Œè·³è¿‡é—¨åº—é¢„çƒ­")
            return
        
        all_stores = df['é—¨åº—åç§°'].unique().tolist()
        total_stores = len(all_stores)
        print(f"\n[é˜¶æ®µ2] åˆ†æçƒ­ç‚¹é—¨åº—ï¼ˆæ€»æ•°: {total_stores}ï¼‰...")
        
        # åŸºäºè®¿é—®æ—¥å¿—åˆ†æçƒ­ç‚¹
        hot_stores = hierarchical_cache.analyze_hot_stores(top_n=max(1, total_stores // 5))
        
        # ç¡®ä¿çƒ­ç‚¹é—¨åº—åœ¨all_storesä¸­
        hot_stores = [s for s in hot_stores if s in all_stores]
        
        # å¦‚æœæ²¡æœ‰è®¿é—®æ—¥å¿—ï¼Œé»˜è®¤é¢„çƒ­å‰20%
        if not hot_stores:
            hot_count = max(1, total_stores // 5)
            hot_stores = all_stores[:hot_count]
            print(f"[é˜¶æ®µ2] æ— è®¿é—®æ—¥å¿—ï¼Œé»˜è®¤é¢„çƒ­å‰{hot_count}ä¸ªé—¨åº—")
        else:
            print(f"[é˜¶æ®µ2] è¯†åˆ«çƒ­ç‚¹é—¨åº—: {len(hot_stores)}ä¸ª")
        
        # å†·é—¨åº— = å…¨éƒ¨ - çƒ­ç‚¹
        cold_stores = [s for s in all_stores if s not in hot_stores]
        
        # ===== é˜¶æ®µ3: å¹¶è¡Œé¢„çƒ­çƒ­ç‚¹é—¨åº— =====
        print(f"\n[é˜¶æ®µ3] å¹¶è¡Œé¢„çƒ­çƒ­ç‚¹é—¨åº—ï¼ˆ{len(hot_stores)}ä¸ªï¼‰...")
        
        def warmup_single_store(store_name):
            """é¢„çƒ­å•ä¸ªé—¨åº—"""
            try:
                store_df = df[df['é—¨åº—åç§°'] == store_name]
                diagnosis = get_diagnosis_summary(store_df)
                hierarchical_cache.cache_diagnosis(
                    store_ids=[store_name],
                    date_range=date_range,
                    diagnosis=diagnosis,
                    ttl=3600
                )
                return (store_name, True, None)
            except Exception as e:
                return (store_name, False, str(e))
        
        # å¹¶è¡Œé¢„çƒ­ï¼ˆæœ€å¤š5ä¸ªçº¿ç¨‹ï¼‰
        hot_success = 0
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = {executor.submit(warmup_single_store, store): store for store in hot_stores}
            
            for idx, future in enumerate(as_completed(futures), 1):
                store_name, success, error = future.result()
                if success:
                    hot_success += 1
                    cached_count += 1
                    print(f"[é˜¶æ®µ3] âœ… [{idx}/{len(hot_stores)}] {store_name}")
                else:
                    print(f"[é˜¶æ®µ3] âš ï¸ [{idx}/{len(hot_stores)}] {store_name} å¤±è´¥: {error}")
        
        print(f"[é˜¶æ®µ3] çƒ­ç‚¹é—¨åº—é¢„çƒ­å®Œæˆ: {hot_success}/{len(hot_stores)}")
        
        # ===== é˜¶æ®µ4: æ¸è¿›å¼é¢„çƒ­å†·é—¨åº—ï¼ˆé™åˆ¶æ•°é‡ï¼Œé¿å…è¶…æ—¶ï¼‰=====
        if cold_stores:
            # åªé¢„çƒ­éƒ¨åˆ†å†·é—¨åº—ï¼ˆæœ€å¤š20ä¸ªï¼‰ï¼Œé¿å…ä»»åŠ¡è¶…æ—¶
            cold_to_warmup = cold_stores[:20]
            print(f"\n[é˜¶æ®µ4] æ¸è¿›å¼é¢„çƒ­å†·é—¨åº—ï¼ˆ{len(cold_to_warmup)}/{len(cold_stores)}ï¼‰...")
            
            cold_success = 0
            for idx, store_name in enumerate(cold_to_warmup, 1):
                try:
                    store_df = df[df['é—¨åº—åç§°'] == store_name]
                    diagnosis = get_diagnosis_summary(store_df)
                    hierarchical_cache.cache_diagnosis(
                        store_ids=[store_name],
                        date_range=date_range,
                        diagnosis=diagnosis,
                        ttl=3600
                    )
                    cold_success += 1
                    cached_count += 1
                    print(f"[é˜¶æ®µ4] âœ… [{idx}/{len(cold_to_warmup)}] {store_name}")
                except Exception as e:
                    print(f"[é˜¶æ®µ4] âš ï¸ [{idx}/{len(cold_to_warmup)}] {store_name} å¤±è´¥: {e}")
            
            print(f"[é˜¶æ®µ4] å†·é—¨åº—é¢„çƒ­å®Œæˆ: {cold_success}/{len(cold_to_warmup)}")
            
            if len(cold_stores) > 20:
                print(f"[é˜¶æ®µ4] å‰©ä½™{len(cold_stores) - 20}ä¸ªé—¨åº—å°†æŒ‰éœ€ç¼“å­˜ï¼ˆé¦–æ¬¡è®¿é—®æ—¶ï¼‰")
        
        # ===== æ€»ç»“ =====
        elapsed = time.time() - start_time
        stats = hierarchical_cache.get_stats()
        
        print(f"\n{'='*80}")
        print(f"[åå°ä»»åŠ¡] âœ… V8.4æ™ºèƒ½é¢„çƒ­å®Œæˆ")
        print(f"[åå°ä»»åŠ¡] æ€»è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"[åå°ä»»åŠ¡] å·²ç¼“å­˜: {cached_count} ä¸ªæ•°æ®é›†")
        print(f"[åå°ä»»åŠ¡] å†…å­˜ä½¿ç”¨: {stats.get('used_memory_mb', 0):.1f}MB / {stats.get('max_memory_mb', 0):.1f}MB")
        print(f"[åå°ä»»åŠ¡] ç¼“å­˜å‘½ä¸­ç‡: {stats.get('hit_rate', 0):.1f}%")
        print(f"[åå°ä»»åŠ¡] ä¸‹æ¬¡æ›´æ–°: {datetime.now().strftime('%H:%M:%S')} + 5åˆ†é’Ÿ")
        print(f"{'='*80}\n")
        
    except Exception as e:
        print(f"[åå°ä»»åŠ¡] âŒ æ›´æ–°è¯Šæ–­æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def update_product_scores_cache():
    """
    æ›´æ–°å•†å“è¯„åˆ†æ•°æ®ç¼“å­˜
    
    æ‰§è¡Œæµç¨‹:
    1. ä»æ•°æ®åº“åŠ è½½æ•°æ®
    2. è®¡ç®—å•†å“è¯„åˆ†
    3. å­˜å…¥Redisç¼“å­˜
    """
    try:
        print(f"\n{'='*80}")
        print(f"[åå°ä»»åŠ¡] å¼€å§‹æ›´æ–°å•†å“è¯„åˆ†ç¼“å­˜ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
        
        import time
        start_time = time.time()
        
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from redis_cache_manager import REDIS_CACHE_MANAGER
        
        # è·å–å…¨å±€æ•°æ®
        try:
            from æ™ºèƒ½é—¨åº—çœ‹æ¿_Dashç‰ˆ import GLOBAL_DATA
            if GLOBAL_DATA is None or GLOBAL_DATA.empty:
                print("[åå°ä»»åŠ¡] âš ï¸ GLOBAL_DATAä¸ºç©ºï¼Œè·³è¿‡æ›´æ–°")
                return
            
            df = GLOBAL_DATA.copy()
            print(f"[åå°ä»»åŠ¡] æ•°æ®è¡Œæ•°: {len(df)}")
            
        except Exception as e:
            print(f"[åå°ä»»åŠ¡] âŒ è·å–GLOBAL_DATAå¤±è´¥: {e}")
            return
        
        # è®¡ç®—å•†å“è¯„åˆ†ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥è°ƒç”¨å…·ä½“çš„è¯„åˆ†å‡½æ•°ï¼‰
        # ç”±äºå•†å“è¯„åˆ†è®¡ç®—è¾ƒå¤æ‚ï¼Œæš‚æ—¶è·³è¿‡
        print(f"[åå°ä»»åŠ¡] â„¹ï¸ å•†å“è¯„åˆ†ç¼“å­˜æ›´æ–°å·²è·³è¿‡ï¼ˆå¾…å®ç°ï¼‰")
        
        elapsed = time.time() - start_time
        print(f"[åå°ä»»åŠ¡] è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"{'='*80}\n")
        
    except Exception as e:
        print(f"[åå°ä»»åŠ¡] âŒ æ›´æ–°å•†å“è¯„åˆ†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def start_background_tasks():
    """
    å¯åŠ¨åå°ä»»åŠ¡è°ƒåº¦å™¨
    
    ä»»åŠ¡åˆ—è¡¨:
    1. æ¯5åˆ†é’Ÿæ›´æ–°è¯Šæ–­æ•°æ®ç¼“å­˜
    2. æ¯10åˆ†é’Ÿæ›´æ–°å•†å“è¯„åˆ†ç¼“å­˜
    
    Returns:
        BackgroundScheduler: è°ƒåº¦å™¨å®ä¾‹
    """
    global _scheduler
    
    if _scheduler is not None:
        print("[åå°ä»»åŠ¡] âš ï¸ è°ƒåº¦å™¨å·²ç»åœ¨è¿è¡Œ")
        return _scheduler
    
    print(f"\n{'='*80}")
    print("[åå°ä»»åŠ¡] ğŸš€ å¯åŠ¨åå°ä»»åŠ¡è°ƒåº¦å™¨...")
    print(f"{'='*80}")
    
    _scheduler = BackgroundScheduler()
    
    # ä»»åŠ¡1: æ›´æ–°è¯Šæ–­æ•°æ®ç¼“å­˜ï¼ˆæ¯5åˆ†é’Ÿï¼‰
    _scheduler.add_job(
        update_diagnosis_cache,
        'interval',
        minutes=5,
        id='update_diagnosis',
        name='æ›´æ–°è¯Šæ–­æ•°æ®ç¼“å­˜',
        max_instances=1,  # åŒæ—¶åªè¿è¡Œä¸€ä¸ªå®ä¾‹
        coalesce=True,    # å¦‚æœé”™è¿‡äº†æ‰§è¡Œæ—¶é—´ï¼Œåªæ‰§è¡Œä¸€æ¬¡
        replace_existing=True
    )
    print("[åå°ä»»åŠ¡] âœ… å·²æ·»åŠ ä»»åŠ¡: æ›´æ–°è¯Šæ–­æ•°æ®ç¼“å­˜ (æ¯5åˆ†é’Ÿ)")
    
    # ä»»åŠ¡2: æ›´æ–°å•†å“è¯„åˆ†ç¼“å­˜ï¼ˆæ¯10åˆ†é’Ÿï¼‰
    _scheduler.add_job(
        update_product_scores_cache,
        'interval',
        minutes=10,
        id='update_product_scores',
        name='æ›´æ–°å•†å“è¯„åˆ†ç¼“å­˜',
        max_instances=1,
        coalesce=True,
        replace_existing=True
    )
    print("[åå°ä»»åŠ¡] âœ… å·²æ·»åŠ ä»»åŠ¡: æ›´æ–°å•†å“è¯„åˆ†ç¼“å­˜ (æ¯10åˆ†é’Ÿ)")
    
    # å¯åŠ¨è°ƒåº¦å™¨
    _scheduler.start()
    print("[åå°ä»»åŠ¡] âœ… è°ƒåº¦å™¨å·²å¯åŠ¨")
    
    # ç«‹å³æ‰§è¡Œä¸€æ¬¡ï¼ˆé¢„çƒ­ç¼“å­˜ï¼‰
    print("[åå°ä»»åŠ¡] ğŸ”¥ ç«‹å³æ‰§è¡Œä¸€æ¬¡é¢„çƒ­ç¼“å­˜...")
    try:
        update_diagnosis_cache()
    except Exception as e:
        print(f"[åå°ä»»åŠ¡] âš ï¸ é¢„çƒ­å¤±è´¥: {e}")
    
    print(f"{'='*80}\n")
    
    return _scheduler


def stop_background_tasks():
    """
    åœæ­¢åå°ä»»åŠ¡è°ƒåº¦å™¨
    """
    global _scheduler
    
    if _scheduler is None:
        print("[åå°ä»»åŠ¡] âš ï¸ è°ƒåº¦å™¨æœªè¿è¡Œ")
        return
    
    print("[åå°ä»»åŠ¡] ğŸ›‘ åœæ­¢åå°ä»»åŠ¡è°ƒåº¦å™¨...")
    _scheduler.shutdown(wait=False)
    _scheduler = None
    print("[åå°ä»»åŠ¡] âœ… è°ƒåº¦å™¨å·²åœæ­¢")


def get_scheduler_status():
    """
    è·å–è°ƒåº¦å™¨çŠ¶æ€
    
    Returns:
        dict: è°ƒåº¦å™¨çŠ¶æ€ä¿¡æ¯
    """
    global _scheduler
    
    if _scheduler is None:
        return {
            'running': False,
            'jobs': []
        }
    
    jobs = []
    for job in _scheduler.get_jobs():
        jobs.append({
            'id': job.id,
            'name': job.name,
            'next_run': job.next_run_time.strftime('%Y-%m-%d %H:%M:%S') if job.next_run_time else None
        })
    
    return {
        'running': _scheduler.running,
        'jobs': jobs
    }


# å¯¼å‡º
__all__ = [
    'start_background_tasks',
    'stop_background_tasks',
    'get_scheduler_status',
    'update_diagnosis_cache',
    'update_product_scores_cache'
]
