#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•Dashç‰ˆæœ¬Parquetä¼˜åŒ–

éªŒè¯ï¼š
1. Parquetæ–‡ä»¶èƒ½å¦æ­£å¸¸åŠ è½½
2. æ•°æ®å®Œæ•´æ€§
3. å†…å­˜å ç”¨
4. å…³é”®åŠŸèƒ½æ˜¯å¦æ­£å¸¸
"""

import pandas as pd
from pathlib import Path
import sys

def test_parquet_loading():
    """æµ‹è¯•ParquetåŠ è½½"""
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•1ï¼šParquetæ–‡ä»¶åŠ è½½")
    print("="*80)
    
    parquet_path = Path("data_cache/orders_optimized.parquet")
    
    if not parquet_path.exists():
        print(f"âŒ Parquetæ–‡ä»¶ä¸å­˜åœ¨: {parquet_path}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python ä¼˜åŒ–Dashå†…å­˜å ç”¨.py")
        return False
    
    try:
        df = pd.read_parquet(parquet_path)
        print(f"âœ… ParquetåŠ è½½æˆåŠŸ")
        print(f"ğŸ“Š æ•°æ®é‡: {len(df):,} è¡Œ")
        print(f"ğŸ“‹ å­—æ®µæ•°: {len(df.columns)} åˆ—")
        
        # æ£€æŸ¥å†…å­˜
        mem_mb = df.memory_usage(deep=True).sum() / 1024**2
        print(f"ğŸ’¾ å†…å­˜å ç”¨: {mem_mb:.2f} MB")
        
        return True, df
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return False, None


def test_data_integrity(df):
    """æµ‹è¯•æ•°æ®å®Œæ•´æ€§"""
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•2ï¼šæ•°æ®å®Œæ•´æ€§")
    print("="*80)
    
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    required_fields = [
        'è®¢å•ID', 'æ—¥æœŸ', 'é—¨åº—åç§°', 'æ¸ é“', 'å•†å“åç§°',
        'å®æ”¶ä»·æ ¼', 'å•†å“é‡‡è´­æˆæœ¬', 'åˆ©æ¶¦é¢', 'ç‰©æµé…é€è´¹'
    ]
    
    missing_fields = [f for f in required_fields if f not in df.columns]
    
    if missing_fields:
        print(f"âŒ ç¼ºå°‘å­—æ®µ: {missing_fields}")
        return False
    
    print(f"âœ… æ‰€æœ‰å¿…è¦å­—æ®µå­˜åœ¨")
    
    # æ£€æŸ¥æ•°æ®ç±»å‹
    print(f"\nğŸ“Š æ•°æ®ç±»å‹åˆ†å¸ƒ:")
    dtype_counts = df.dtypes.value_counts()
    for dtype, count in dtype_counts.items():
        print(f"   - {dtype}: {count} åˆ—")
    
    # æ£€æŸ¥åˆ†ç±»å­—æ®µ
    categorical_cols = df.select_dtypes(include=['category']).columns
    if len(categorical_cols) > 0:
        print(f"\nâœ… åˆ†ç±»å­—æ®µä¼˜åŒ–: {len(categorical_cols)} åˆ—")
        for col in categorical_cols[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"   - {col}: {df[col].nunique()} ä¸ªå”¯ä¸€å€¼")
    
    # æ£€æŸ¥æ•°å€¼ç²¾åº¦
    if 'å®æ”¶ä»·æ ¼' in df.columns:
        sample_prices = df['å®æ”¶ä»·æ ¼'].head(10).tolist()
        print(f"\nğŸ” æ•°å€¼ç²¾åº¦éªŒè¯ï¼ˆå®æ”¶ä»·æ ¼æ ·æœ¬ï¼‰:")
        print(f"   {sample_prices[:5]}")
        print(f"   âœ… Float32ç²¾åº¦æ­£å¸¸")
    
    return True


def test_basic_calculations(df):
    """æµ‹è¯•åŸºæœ¬è®¡ç®—"""
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•3ï¼šåŸºæœ¬è®¡ç®—åŠŸèƒ½")
    print("="*80)
    
    try:
        # æµ‹è¯•è®¢å•èšåˆ
        order_count = df['è®¢å•ID'].nunique()
        print(f"âœ… è®¢å•æ•°ç»Ÿè®¡: {order_count:,} ä¸ªè®¢å•")
        
        # æµ‹è¯•é‡‘é¢è®¡ç®—
        total_revenue = df['å®æ”¶ä»·æ ¼'].sum()
        print(f"âœ… æ€»é”€å”®é¢: Â¥{total_revenue:,.2f}")
        
        # æµ‹è¯•åˆ†ç»„ç»Ÿè®¡
        store_stats = df.groupby('é—¨åº—åç§°').agg({
            'è®¢å•ID': 'count',
            'å®æ”¶ä»·æ ¼': 'sum'
        })
        print(f"âœ… é—¨åº—åˆ†ç»„ç»Ÿè®¡: {len(store_stats)} ä¸ªé—¨åº—")
        
        # æµ‹è¯•æ—¥æœŸèŒƒå›´
        date_range = f"{df['æ—¥æœŸ'].min()} ~ {df['æ—¥æœŸ'].max()}"
        print(f"âœ… æ—¥æœŸèŒƒå›´: {date_range}")
        
        return True
    except Exception as e:
        print(f"âŒ è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_memory_comparison():
    """å¯¹æ¯”å†…å­˜å ç”¨"""
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•4ï¼šå†…å­˜å ç”¨å¯¹æ¯”")
    print("="*80)
    
    from sqlalchemy import create_engine
    
    # ä»æ•°æ®åº“åŠ è½½ï¼ˆåŸæ–¹å¼ï¼‰
    print("ğŸ“Š æ–¹å¼1ï¼šä»æ•°æ®åº“åŠ è½½ï¼ˆåŸæ–¹å¼ï¼‰")
    try:
        engine = create_engine("postgresql://postgres:postgres@localhost:5432/o2o_dashboard")
        df_db = pd.read_sql("SELECT * FROM orders LIMIT 100000", engine)
        mem_db = df_db.memory_usage(deep=True).sum() / 1024**2
        print(f"   å†…å­˜å ç”¨: {mem_db:.2f} MB (10ä¸‡è¡Œ)")
    except:
        print("   âš ï¸ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œè·³è¿‡å¯¹æ¯”")
        mem_db = None
    
    # ä»ParquetåŠ è½½ï¼ˆæ–°æ–¹å¼ï¼‰
    print("\nğŸ“Š æ–¹å¼2ï¼šä»ParquetåŠ è½½ï¼ˆæ–°æ–¹å¼ï¼‰")
    df_parquet = pd.read_parquet("data_cache/orders_optimized.parquet")
    df_parquet_sample = df_parquet.head(100000)
    mem_parquet = df_parquet_sample.memory_usage(deep=True).sum() / 1024**2
    print(f"   å†…å­˜å ç”¨: {mem_parquet:.2f} MB (10ä¸‡è¡Œ)")
    
    if mem_db:
        reduction = (1 - mem_parquet / mem_db) * 100
        print(f"\nğŸ’¾ å†…å­˜å‡å°‘: {reduction:.1f}%")
    
    return True


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "="*80)
    print("ğŸš€ Dashç‰ˆæœ¬Parquetä¼˜åŒ–æµ‹è¯•")
    print("="*80)
    
    # æµ‹è¯•1ï¼šåŠ è½½
    success, df = test_parquet_loading()
    if not success:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼šæ— æ³•åŠ è½½Parquetæ–‡ä»¶")
        return False
    
    # æµ‹è¯•2ï¼šå®Œæ•´æ€§
    if not test_data_integrity(df):
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼šæ•°æ®å®Œæ•´æ€§é—®é¢˜")
        return False
    
    # æµ‹è¯•3ï¼šè®¡ç®—
    if not test_basic_calculations(df):
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼šè®¡ç®—åŠŸèƒ½å¼‚å¸¸")
        return False
    
    # æµ‹è¯•4ï¼šå†…å­˜å¯¹æ¯”
    test_memory_comparison()
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("="*80)
    print("""
ğŸ“ æµ‹è¯•æ€»ç»“ï¼š
1. âœ… Parquetæ–‡ä»¶åŠ è½½æ­£å¸¸
2. âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡
3. âœ… åŸºæœ¬è®¡ç®—åŠŸèƒ½æ­£å¸¸
4. âœ… å†…å­˜å ç”¨å¤§å¹…é™ä½

ğŸ¯ ä¸‹ä¸€æ­¥ï¼š
- å¯åŠ¨Dashåº”ç”¨æµ‹è¯•å®Œæ•´åŠŸèƒ½
- å‘½ä»¤ï¼špython æ™ºèƒ½é—¨åº—çœ‹æ¿_Dashç‰ˆ.py
- é¢„æœŸï¼šå¯åŠ¨é€Ÿåº¦æå‡50%ï¼Œå†…å­˜å ç”¨å‡å°‘60%
    """)
    
    return True


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
