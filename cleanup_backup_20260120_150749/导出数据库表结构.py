"""
æ™ºèƒ½æ•°æ®åº“è¡¨ç»“æ„å¯¼å‡ºå·¥å…· V1.0

åŠŸèƒ½:
1. ä»PostgreSQLæ•°æ®åº“è¯»å–å®Œæ•´è¡¨ç»“æ„
2. ç”Ÿæˆå¤šç§æ ¼å¼çš„å»ºè¡¨è¯­å¥(PostgreSQL/MySQL/SQLite/SQL Server)
3. å¯¼å‡ºæ•°æ®å­—å…¸(Excelæ ¼å¼)
4. ç”ŸæˆMarkdownæ–‡æ¡£
5. åŒ…å«ç¤ºä¾‹æ•°æ®å’Œéƒ¨ç½²æŒ‡å—

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-12-09
"""

import os
import sys
from datetime import datetime
from pathlib import Path
import pandas as pd

# æ£€æŸ¥ä¾èµ–
try:
    from sqlalchemy import create_engine, inspect, text, MetaData
    from sqlalchemy.schema import CreateTable
    import openpyxl
    # æ£€æŸ¥psycopg2
    try:
        import psycopg2
        DRIVER = 'psycopg2'
    except ImportError:
        try:
            import pg8000
            DRIVER = 'pg8000'
        except ImportError:
            print("âŒ ç¼ºå°‘PostgreSQLé©±åŠ¨!")
            print("è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¹‹ä¸€:")
            print("  pip install psycopg2-binary")
            print("  æˆ–")
            print("  pip install pg8000")
            sys.exit(1)
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
    print("è¯·è¿è¡Œ: pip install sqlalchemy openpyxl")
    sys.exit(1)

# æ•°æ®åº“è¿æ¥é…ç½®
# ğŸ’¡ æç¤º: è¿™äº›é…ç½®åº”è¯¥ä¸.envæ–‡ä»¶ä¸­çš„DATABASE_URLä¿æŒä¸€è‡´
DATABASE_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'o2o_dashboard',
    'user': 'postgres',
    'password': '308352588'  # âœ… ä¸.envä¸­çš„å®é™…å¯†ç ä¸€è‡´
}

# å¯¼å‡ºé…ç½®
OUTPUT_DIR = Path("æ•°æ®åº“è¡¨ç»“æ„å¯¼å‡º")
TABLES_TO_EXPORT = ['orders', 'products', 'stores']  # å¯ä»¥æŒ‡å®šè¦å¯¼å‡ºçš„è¡¨,æˆ–ç•™ç©ºå¯¼å‡ºå…¨éƒ¨


class DatabaseSchemaExporter:
    """æ•°æ®åº“è¡¨ç»“æ„å¯¼å‡ºå™¨"""
    
    def __init__(self, db_config):
        """åˆå§‹åŒ–"""
        self.config = db_config
        self.engine = None
        self.inspector = None
        self.metadata = MetaData()
        
    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        import warnings
        warnings.filterwarnings('ignore')
        
        try:
            # æ ¹æ®å¯ç”¨é©±åŠ¨æ„å»ºè¿æ¥å­—ç¬¦ä¸²
            if DRIVER == 'psycopg2':
                conn_str = f"postgresql+psycopg2://{self.config['user']}:{self.config['password']}@{self.config['host']}:{self.config['port']}/{self.config['database']}?client_encoding=utf8"
                connect_args = {'options': '-c client_encoding=UTF8'}
            else:  # pg8000
                # pg8000åœ¨Windowsä¸Šéœ€è¦ç‰¹æ®Šå¤„ç†
                import pg8000.native
                # ç›´æ¥ä½¿ç”¨pg8000åŸç”Ÿè¿æ¥æµ‹è¯•
                try:
                    conn = pg8000.native.Connection(
                        user=self.config['user'],
                        password=self.config['password'],
                        host=self.config['host'],
                        port=self.config['port'],
                        database=self.config['database']
                    )
                    conn.close()
                except Exception as e:
                    # æ•è·å¹¶å¿½ç•¥ç¼–ç é”™è¯¯
                    if 'utf-8' not in str(e).lower():
                        raise
                
                conn_str = f"postgresql+pg8000://{self.config['user']}:{self.config['password']}@{self.config['host']}:{self.config['port']}/{self.config['database']}"
                connect_args = {}
            
            self.engine = create_engine(
                conn_str,
                connect_args=connect_args,
                pool_pre_ping=True,
                echo=False  # å…³é—­SQLæ—¥å¿—é¿å…ç¼–ç é—®é¢˜
            )
            self.inspector = inspect(self.engine)
            
            # æµ‹è¯•è¿æ¥
            with self.engine.connect() as conn:
                result = conn.execute(text("SELECT 1"))
                result.scalar()
                print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ (é©±åŠ¨: {DRIVER})")
            
            return True
        except Exception as e:
            # å¤„ç†å„ç§é”™è¯¯
            error_msg = str(e)
            
            # å¦‚æœæ˜¯UTF-8è§£ç é”™è¯¯,å°è¯•ç”¨GBKè§£ç 
            if 'utf-8' in error_msg.lower() and 'decode' in error_msg.lower():
                try:
                    if hasattr(e, 'args') and e.args:
                        for arg in e.args:
                            if isinstance(arg, bytes):
                                error_msg = arg.decode('gbk', errors='replace')
                                break
                except:
                    error_msg = "ç¼–ç é”™è¯¯ (å¯èƒ½PostgreSQLæœªå¯åŠ¨æˆ–é…ç½®é”™è¯¯)"
            
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {error_msg}")
            print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            
            # æä¾›æ›´è¯¦ç»†çš„è¯Šæ–­
            if 'Connection refused' in error_msg or 'could not connect' in error_msg.lower():
                print("   ğŸ’¡ PostgreSQLæœåŠ¡å¯èƒ½æœªå¯åŠ¨")
            elif 'authentication' in error_msg.lower() or 'password' in error_msg.lower():
                print("   ğŸ’¡ ç”¨æˆ·åæˆ–å¯†ç å¯èƒ½ä¸æ­£ç¡®")
            elif 'database' in error_msg.lower() and 'does not exist' in error_msg.lower():
                print("   ğŸ’¡ æ•°æ®åº“ä¸å­˜åœ¨")
            
            return False
    
    def get_all_tables(self):
        """è·å–æ‰€æœ‰è¡¨å"""
        return self.inspector.get_table_names()
    
    def get_table_info(self, table_name):
        """è·å–è¡¨çš„å®Œæ•´ä¿¡æ¯"""
        info = {
            'name': table_name,
            'columns': [],
            'primary_keys': [],
            'foreign_keys': [],
            'indexes': [],
            'row_count': 0,
            'sample_data': None
        }
        
        try:
            # è·å–åˆ—ä¿¡æ¯
            columns = self.inspector.get_columns(table_name)
            for col in columns:
                col_info = {
                    'name': col['name'],
                    'type': str(col['type']),
                    'nullable': col['nullable'],
                    'default': col.get('default'),
                    'comment': col.get('comment', '')
                }
                info['columns'].append(col_info)
            
            # è·å–ä¸»é”®
            pk = self.inspector.get_pk_constraint(table_name)
            info['primary_keys'] = pk.get('constrained_columns', [])
            
            # è·å–å¤–é”®
            fks = self.inspector.get_foreign_keys(table_name)
            for fk in fks:
                fk_info = {
                    'columns': fk['constrained_columns'],
                    'ref_table': fk['referred_table'],
                    'ref_columns': fk['referred_columns']
                }
                info['foreign_keys'].append(fk_info)
            
            # è·å–ç´¢å¼•
            indexes = self.inspector.get_indexes(table_name)
            for idx in indexes:
                idx_info = {
                    'name': idx['name'],
                    'columns': idx['column_names'],
                    'unique': idx['unique']
                }
                info['indexes'].append(idx_info)
            
            # è·å–è¡Œæ•°
            with self.engine.connect() as conn:
                result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                info['row_count'] = result.scalar()
                
                # è·å–ç¤ºä¾‹æ•°æ®(å‰5è¡Œ)
                result = conn.execute(text(f"SELECT * FROM {table_name} LIMIT 5"))
                info['sample_data'] = result.fetchall()
            
        except Exception as e:
            print(f"âš ï¸ è·å–è¡¨ {table_name} ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        
        return info
    
    def generate_postgresql_ddl(self, table_info):
        """ç”ŸæˆPostgreSQLå»ºè¡¨è¯­å¥"""
        lines = []
        lines.append(f"-- ============================================")
        lines.append(f"-- è¡¨å: {table_info['name']}")
        lines.append(f"-- è¡Œæ•°: {table_info['row_count']:,}")
        lines.append(f"-- ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"-- ============================================")
        lines.append("")
        lines.append(f"CREATE TABLE IF NOT EXISTS {table_info['name']} (")
        
        # åˆ—å®šä¹‰
        col_lines = []
        for col in table_info['columns']:
            col_def = f"    {col['name']} {col['type']}"
            if not col['nullable']:
                col_def += " NOT NULL"
            if col['default']:
                col_def += f" DEFAULT {col['default']}"
            col_lines.append(col_def)
        
        # ä¸»é”®
        if table_info['primary_keys']:
            pk_cols = ', '.join(table_info['primary_keys'])
            col_lines.append(f"    PRIMARY KEY ({pk_cols})")
        
        lines.append(',\n'.join(col_lines))
        lines.append(");")
        lines.append("")
        
        # å¤–é”®
        for fk in table_info['foreign_keys']:
            fk_cols = ', '.join(fk['columns'])
            ref_cols = ', '.join(fk['ref_columns'])
            lines.append(f"ALTER TABLE {table_info['name']} ADD FOREIGN KEY ({fk_cols}) REFERENCES {fk['ref_table']}({ref_cols});")
        
        # ç´¢å¼•
        for idx in table_info['indexes']:
            if idx['name'].startswith('pk_'):  # è·³è¿‡ä¸»é”®ç´¢å¼•
                continue
            idx_cols = ', '.join(idx['columns'])
            unique = "UNIQUE " if idx['unique'] else ""
            lines.append(f"CREATE {unique}INDEX {idx['name']} ON {table_info['name']} ({idx_cols});")
        
        # æ³¨é‡Š
        if any(col['comment'] for col in table_info['columns']):
            lines.append("")
            lines.append("-- åˆ—æ³¨é‡Š")
            for col in table_info['columns']:
                if col['comment']:
                    lines.append(f"COMMENT ON COLUMN {table_info['name']}.{col['name']} IS '{col['comment']}';")
        
        return '\n'.join(lines)
    
    def generate_mysql_ddl(self, table_info):
        """ç”ŸæˆMySQLå»ºè¡¨è¯­å¥"""
        lines = []
        lines.append(f"-- ============================================")
        lines.append(f"-- è¡¨å: {table_info['name']}")
        lines.append(f"-- è¡Œæ•°: {table_info['row_count']:,}")
        lines.append(f"-- ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"-- ============================================")
        lines.append("")
        lines.append(f"CREATE TABLE IF NOT EXISTS `{table_info['name']}` (")
        
        # åˆ—å®šä¹‰
        col_lines = []
        for col in table_info['columns']:
            # ç±»å‹è½¬æ¢
            mysql_type = self._convert_to_mysql_type(col['type'])
            col_def = f"    `{col['name']}` {mysql_type}"
            if not col['nullable']:
                col_def += " NOT NULL"
            if col['default']:
                col_def += f" DEFAULT {col['default']}"
            if col['comment']:
                col_def += f" COMMENT '{col['comment']}'"
            col_lines.append(col_def)
        
        # ä¸»é”®
        if table_info['primary_keys']:
            pk_cols = ', '.join([f"`{pk}`" for pk in table_info['primary_keys']])
            col_lines.append(f"    PRIMARY KEY ({pk_cols})")
        
        lines.append(',\n'.join(col_lines))
        lines.append(") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;")
        lines.append("")
        
        # ç´¢å¼•
        for idx in table_info['indexes']:
            if idx['name'].startswith('pk_'):
                continue
            idx_cols = ', '.join([f"`{col}`" for col in idx['columns']])
            unique = "UNIQUE " if idx['unique'] else ""
            lines.append(f"CREATE {unique}INDEX `{idx['name']}` ON `{table_info['name']}` ({idx_cols});")
        
        return '\n'.join(lines)
    
    def generate_sqlite_ddl(self, table_info):
        """ç”ŸæˆSQLiteå»ºè¡¨è¯­å¥"""
        lines = []
        lines.append(f"-- ============================================")
        lines.append(f"-- è¡¨å: {table_info['name']}")
        lines.append(f"-- è¡Œæ•°: {table_info['row_count']:,}")
        lines.append(f"-- ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"-- ============================================")
        lines.append("")
        lines.append(f"CREATE TABLE IF NOT EXISTS {table_info['name']} (")
        
        # åˆ—å®šä¹‰
        col_lines = []
        for col in table_info['columns']:
            # ç±»å‹è½¬æ¢
            sqlite_type = self._convert_to_sqlite_type(col['type'])
            col_def = f"    {col['name']} {sqlite_type}"
            if not col['nullable']:
                col_def += " NOT NULL"
            if col['default']:
                col_def += f" DEFAULT {col['default']}"
            col_lines.append(col_def)
        
        # ä¸»é”®
        if table_info['primary_keys']:
            pk_cols = ', '.join(table_info['primary_keys'])
            col_lines.append(f"    PRIMARY KEY ({pk_cols})")
        
        lines.append(',\n'.join(col_lines))
        lines.append(");")
        lines.append("")
        
        # ç´¢å¼•
        for idx in table_info['indexes']:
            if idx['name'].startswith('pk_'):
                continue
            idx_cols = ', '.join(idx['columns'])
            unique = "UNIQUE " if idx['unique'] else ""
            lines.append(f"CREATE {unique}INDEX {idx['name']} ON {table_info['name']} ({idx_cols});")
        
        return '\n'.join(lines)
    
    def _convert_to_mysql_type(self, pg_type):
        """PostgreSQLç±»å‹è½¬MySQLç±»å‹"""
        type_map = {
            'INTEGER': 'INT',
            'BIGINT': 'BIGINT',
            'VARCHAR': 'VARCHAR',
            'TEXT': 'TEXT',
            'TIMESTAMP': 'DATETIME',
            'BOOLEAN': 'TINYINT(1)',
            'NUMERIC': 'DECIMAL',
            'REAL': 'FLOAT',
            'DOUBLE PRECISION': 'DOUBLE',
            'DATE': 'DATE',
            'TIME': 'TIME',
            'JSON': 'JSON',
            'JSONB': 'JSON'
        }
        
        pg_type_upper = pg_type.upper()
        for pg, mysql in type_map.items():
            if pg_type_upper.startswith(pg):
                # ä¿ç•™é•¿åº¦ä¿¡æ¯
                if '(' in pg_type:
                    length = pg_type[pg_type.index('('):]
                    return mysql + length
                return mysql
        return pg_type  # æ— æ³•è½¬æ¢åˆ™ä¿æŒåŸæ ·
    
    def _convert_to_sqlite_type(self, pg_type):
        """PostgreSQLç±»å‹è½¬SQLiteç±»å‹"""
        type_map = {
            'INTEGER': 'INTEGER',
            'BIGINT': 'INTEGER',
            'VARCHAR': 'TEXT',
            'TEXT': 'TEXT',
            'TIMESTAMP': 'TEXT',
            'BOOLEAN': 'INTEGER',
            'NUMERIC': 'REAL',
            'REAL': 'REAL',
            'DOUBLE PRECISION': 'REAL',
            'DATE': 'TEXT',
            'TIME': 'TEXT',
            'JSON': 'TEXT',
            'JSONB': 'TEXT'
        }
        
        pg_type_upper = pg_type.upper()
        for pg, sqlite in type_map.items():
            if pg_type_upper.startswith(pg):
                return sqlite
        return 'TEXT'  # é»˜è®¤TEXT
    
    def generate_markdown_doc(self, table_info):
        """ç”ŸæˆMarkdownæ–‡æ¡£"""
        lines = []
        lines.append(f"# è¡¨: {table_info['name']}")
        lines.append("")
        lines.append(f"**æ•°æ®è¡Œæ•°**: {table_info['row_count']:,} è¡Œ")
        lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")
        
        # åˆ—ä¿¡æ¯è¡¨æ ¼
        lines.append("## åˆ—å®šä¹‰")
        lines.append("")
        lines.append("| åˆ—å | ç±»å‹ | å…è®¸NULL | é»˜è®¤å€¼ | è¯´æ˜ |")
        lines.append("|------|------|----------|--------|------|")
        for col in table_info['columns']:
            nullable = 'âœ…' if col['nullable'] else 'âŒ'
            default = col['default'] or '-'
            comment = col['comment'] or '-'
            lines.append(f"| {col['name']} | {col['type']} | {nullable} | {default} | {comment} |")
        lines.append("")
        
        # ä¸»é”®
        if table_info['primary_keys']:
            lines.append("## ä¸»é”®")
            lines.append("")
            lines.append(f"- `{', '.join(table_info['primary_keys'])}`")
            lines.append("")
        
        # å¤–é”®
        if table_info['foreign_keys']:
            lines.append("## å¤–é”®")
            lines.append("")
            for fk in table_info['foreign_keys']:
                fk_cols = ', '.join(fk['columns'])
                ref_cols = ', '.join(fk['ref_columns'])
                lines.append(f"- `{fk_cols}` â†’ `{fk['ref_table']}({ref_cols})`")
            lines.append("")
        
        # ç´¢å¼•
        if table_info['indexes']:
            lines.append("## ç´¢å¼•")
            lines.append("")
            lines.append("| ç´¢å¼•å | åˆ— | å”¯ä¸€ |")
            lines.append("|--------|----|----|")
            for idx in table_info['indexes']:
                unique = 'âœ…' if idx['unique'] else 'âŒ'
                cols = ', '.join(idx['columns'])
                lines.append(f"| {idx['name']} | {cols} | {unique} |")
            lines.append("")
        
        return '\n'.join(lines)
    
    def export_to_excel(self, all_tables_info, output_file):
        """å¯¼å‡ºåˆ°Excelæ•°æ®å­—å…¸"""
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # è¡¨æ¸…å•
            tables_summary = []
            for table_info in all_tables_info:
                tables_summary.append({
                    'è¡¨å': table_info['name'],
                    'åˆ—æ•°': len(table_info['columns']),
                    'è¡Œæ•°': table_info['row_count'],
                    'ä¸»é”®': ', '.join(table_info['primary_keys']) if table_info['primary_keys'] else '-',
                    'å¤–é”®æ•°': len(table_info['foreign_keys']),
                    'ç´¢å¼•æ•°': len(table_info['indexes'])
                })
            df_summary = pd.DataFrame(tables_summary)
            df_summary.to_excel(writer, sheet_name='è¡¨æ¸…å•', index=False)
            
            # æ¯ä¸ªè¡¨çš„è¯¦ç»†ä¿¡æ¯
            for table_info in all_tables_info:
                # åˆ—ä¿¡æ¯
                columns_data = []
                for col in table_info['columns']:
                    columns_data.append({
                        'åˆ—å': col['name'],
                        'æ•°æ®ç±»å‹': col['type'],
                        'å…è®¸NULL': 'æ˜¯' if col['nullable'] else 'å¦',
                        'é»˜è®¤å€¼': col['default'] or '',
                        'è¯´æ˜': col['comment'] or '',
                        'æ˜¯å¦ä¸»é”®': 'âœ“' if col['name'] in table_info['primary_keys'] else ''
                    })
                df_cols = pd.DataFrame(columns_data)
                
                # é™åˆ¶sheetåç§°é•¿åº¦
                sheet_name = table_info['name'][:31]
                df_cols.to_excel(writer, sheet_name=sheet_name, index=False)
        
        print(f"âœ… Excelæ•°æ®å­—å…¸å·²ç”Ÿæˆ: {output_file}")
    
    def export_all(self, tables=None):
        """å¯¼å‡ºæ‰€æœ‰è¡¨ç»“æ„"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        OUTPUT_DIR.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # è·å–è¦å¯¼å‡ºçš„è¡¨
        if tables:
            table_names = [t for t in tables if t in self.get_all_tables()]
        else:
            table_names = self.get_all_tables()
        
        if not table_names:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è¦å¯¼å‡ºçš„è¡¨")
            return
        
        print(f"ğŸ“‹ å‡†å¤‡å¯¼å‡º {len(table_names)} ä¸ªè¡¨...")
        print(f"   {', '.join(table_names)}")
        print("")
        
        # æ”¶é›†æ‰€æœ‰è¡¨ä¿¡æ¯
        all_tables_info = []
        for table_name in table_names:
            print(f"ğŸ” åˆ†æè¡¨: {table_name}...", end=' ')
            table_info = self.get_table_info(table_name)
            all_tables_info.append(table_info)
            print(f"âœ… ({table_info['row_count']:,} è¡Œ, {len(table_info['columns'])} åˆ—)")
        
        print("")
        
        # ç”Ÿæˆå„ç§æ ¼å¼çš„DDL
        for table_info in all_tables_info:
            table_name = table_info['name']
            
            # PostgreSQL
            pg_file = OUTPUT_DIR / f"{table_name}_postgresql.sql"
            with open(pg_file, 'w', encoding='utf-8') as f:
                f.write(self.generate_postgresql_ddl(table_info))
            print(f"âœ… PostgreSQL: {pg_file.name}")
            
            # MySQL
            mysql_file = OUTPUT_DIR / f"{table_name}_mysql.sql"
            with open(mysql_file, 'w', encoding='utf-8') as f:
                f.write(self.generate_mysql_ddl(table_info))
            print(f"âœ… MySQL: {mysql_file.name}")
            
            # SQLite
            sqlite_file = OUTPUT_DIR / f"{table_name}_sqlite.sql"
            with open(sqlite_file, 'w', encoding='utf-8') as f:
                f.write(self.generate_sqlite_ddl(table_info))
            print(f"âœ… SQLite: {sqlite_file.name}")
            
            # Markdown
            md_file = OUTPUT_DIR / f"{table_name}_æ–‡æ¡£.md"
            with open(md_file, 'w', encoding='utf-8') as f:
                f.write(self.generate_markdown_doc(table_info))
            print(f"âœ… æ–‡æ¡£: {md_file.name}")
            print("")
        
        # ç”Ÿæˆå®Œæ•´çš„SQLè„šæœ¬(æ‰€æœ‰è¡¨)
        for db_type in ['postgresql', 'mysql', 'sqlite']:
            full_file = OUTPUT_DIR / f"å®Œæ•´æ•°æ®åº“ç»“æ„_{db_type}.sql"
            with open(full_file, 'w', encoding='utf-8') as f:
                f.write(f"-- ============================================\n")
                f.write(f"-- å®Œæ•´æ•°æ®åº“ç»“æ„ ({db_type.upper()})\n")
                f.write(f"-- ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"-- åŒ…å«è¡¨: {', '.join(table_names)}\n")
                f.write(f"-- ============================================\n\n")
                
                for table_info in all_tables_info:
                    if db_type == 'postgresql':
                        f.write(self.generate_postgresql_ddl(table_info))
                    elif db_type == 'mysql':
                        f.write(self.generate_mysql_ddl(table_info))
                    else:
                        f.write(self.generate_sqlite_ddl(table_info))
                    f.write("\n\n")
            print(f"âœ… å®Œæ•´è„šæœ¬: {full_file.name}")
        
        # ç”ŸæˆExcelæ•°æ®å­—å…¸
        excel_file = OUTPUT_DIR / f"æ•°æ®å­—å…¸_{timestamp}.xlsx"
        self.export_to_excel(all_tables_info, excel_file)
        
        # ç”Ÿæˆéƒ¨ç½²æŒ‡å—
        guide_file = OUTPUT_DIR / "éƒ¨ç½²æŒ‡å—.md"
        self._generate_deployment_guide(guide_file, all_tables_info)
        
        print("")
        print("="*60)
        print(f"ğŸ‰ å¯¼å‡ºå®Œæˆ! æ–‡ä»¶ä¿å­˜åœ¨: {OUTPUT_DIR.absolute()}")
        print("="*60)
    
    def _generate_deployment_guide(self, output_file, all_tables_info):
        """ç”Ÿæˆéƒ¨ç½²æŒ‡å—"""
        lines = []
        lines.append("# æ•°æ®åº“éƒ¨ç½²æŒ‡å—")
        lines.append("")
        lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"**è¡¨æ•°é‡**: {len(all_tables_info)} ä¸ª")
        lines.append("")
        
        lines.append("## æ–‡ä»¶è¯´æ˜")
        lines.append("")
        lines.append("æœ¬æ¬¡å¯¼å‡ºåŒ…å«ä»¥ä¸‹æ–‡ä»¶:")
        lines.append("")
        lines.append("### SQLè„šæœ¬")
        lines.append("- `*_postgresql.sql` - PostgreSQLå»ºè¡¨è¯­å¥")
        lines.append("- `*_mysql.sql` - MySQLå»ºè¡¨è¯­å¥")
        lines.append("- `*_sqlite.sql` - SQLiteå»ºè¡¨è¯­å¥")
        lines.append("- `å®Œæ•´æ•°æ®åº“ç»“æ„_*.sql` - åŒ…å«æ‰€æœ‰è¡¨çš„å®Œæ•´è„šæœ¬")
        lines.append("")
        
        lines.append("### æ–‡æ¡£")
        lines.append("- `*_æ–‡æ¡£.md` - æ¯ä¸ªè¡¨çš„è¯¦ç»†æ–‡æ¡£")
        lines.append("- `æ•°æ®å­—å…¸_*.xlsx` - Excelæ ¼å¼çš„å®Œæ•´æ•°æ®å­—å…¸")
        lines.append("- `éƒ¨ç½²æŒ‡å—.md` - æœ¬æ–‡æ¡£")
        lines.append("")
        
        lines.append("## éƒ¨ç½²æ­¥éª¤")
        lines.append("")
        
        # PostgreSQL
        lines.append("### PostgreSQL")
        lines.append("```bash")
        lines.append("# è¿æ¥æ•°æ®åº“")
        lines.append("psql -U postgres -d your_database")
        lines.append("")
        lines.append("# æ‰§è¡Œè„šæœ¬")
        lines.append("\\i å®Œæ•´æ•°æ®åº“ç»“æ„_postgresql.sql")
        lines.append("```")
        lines.append("")
        
        # MySQL
        lines.append("### MySQL")
        lines.append("```bash")
        lines.append("# è¿æ¥æ•°æ®åº“")
        lines.append("mysql -u root -p your_database")
        lines.append("")
        lines.append("# æ‰§è¡Œè„šæœ¬")
        lines.append("source å®Œæ•´æ•°æ®åº“ç»“æ„_mysql.sql;")
        lines.append("```")
        lines.append("")
        
        # SQLite
        lines.append("### SQLite")
        lines.append("```bash")
        lines.append("# åˆ›å»ºå¹¶æ‰§è¡Œ")
        lines.append("sqlite3 your_database.db < å®Œæ•´æ•°æ®åº“ç»“æ„_sqlite.sql")
        lines.append("```")
        lines.append("")
        
        lines.append("## è¡¨æ¸…å•")
        lines.append("")
        for table_info in all_tables_info:
            lines.append(f"### {table_info['name']}")
            lines.append(f"- **æ•°æ®è¡Œæ•°**: {table_info['row_count']:,} è¡Œ")
            lines.append(f"- **åˆ—æ•°**: {len(table_info['columns'])} åˆ—")
            if table_info['primary_keys']:
                lines.append(f"- **ä¸»é”®**: `{', '.join(table_info['primary_keys'])}`")
            lines.append("")
        
        lines.append("## æ³¨æ„äº‹é¡¹")
        lines.append("")
        lines.append("1. **ç±»å‹å…¼å®¹æ€§**: ä¸åŒæ•°æ®åº“çš„æ•°æ®ç±»å‹å¯èƒ½æœ‰å·®å¼‚,è¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´")
        lines.append("2. **å­—ç¬¦ç¼–ç **: å»ºè®®ä½¿ç”¨UTF-8ç¼–ç ")
        lines.append("3. **æƒé™è®¾ç½®**: ç¡®ä¿æœ‰CREATE TABLEæƒé™")
        lines.append("4. **ç´¢å¼•ä¼˜åŒ–**: å¤§æ•°æ®é‡æ—¶å»ºè®®å…ˆå¯¼å…¥æ•°æ®å†åˆ›å»ºç´¢å¼•")
        lines.append("5. **å¤–é”®çº¦æŸ**: æ³¨æ„è¡¨åˆ›å»ºé¡ºåº,å…ˆåˆ›å»ºè¢«å¼•ç”¨çš„è¡¨")
        lines.append("")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        
        print(f"âœ… éƒ¨ç½²æŒ‡å—: {output_file.name}")


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("     æ™ºèƒ½æ•°æ®åº“è¡¨ç»“æ„å¯¼å‡ºå·¥å…· V1.0")
    print("="*60)
    print("")
    
    # åˆ›å»ºå¯¼å‡ºå™¨
    exporter = DatabaseSchemaExporter(DATABASE_CONFIG)
    
    # è¿æ¥æ•°æ®åº“
    print("ğŸ”Œ æ­£åœ¨è¿æ¥æ•°æ®åº“...")
    if not exporter.connect():
        print("")
        print("ğŸ’¡ æç¤º:")
        print("   1. è¯·ç¡®è®¤PostgreSQLæœåŠ¡å·²å¯åŠ¨")
        print("   2. æ£€æŸ¥æ•°æ®åº“é…ç½®æ˜¯å¦æ­£ç¡®(DATABASE_CONFIG)")
        print("   3. ç¡®è®¤ç”¨æˆ·åå’Œå¯†ç ")
        return
    
    print("")
    
    # æ˜¾ç¤ºå¯ç”¨çš„è¡¨
    all_tables = exporter.get_all_tables()
    print(f"ğŸ“‹ å‘ç° {len(all_tables)} ä¸ªè¡¨:")
    for i, table in enumerate(all_tables, 1):
        print(f"   {i}. {table}")
    print("")
    
    # é€‰æ‹©è¦å¯¼å‡ºçš„è¡¨
    if TABLES_TO_EXPORT:
        print(f"ğŸ“Œ å°†å¯¼å‡ºæŒ‡å®šçš„è¡¨: {', '.join(TABLES_TO_EXPORT)}")
        tables_to_export = TABLES_TO_EXPORT
    else:
        print("ğŸ“Œ å°†å¯¼å‡ºæ‰€æœ‰è¡¨")
        tables_to_export = None
    
    print("")
    input("æŒ‰å›è½¦é”®å¼€å§‹å¯¼å‡º...")
    print("")
    
    # æ‰§è¡Œå¯¼å‡º
    exporter.export_all(tables_to_export)
    
    print("")
    print("âœ¨ å¯¼å‡ºå®Œæˆ!ä½ å¯ä»¥å°†å¯¼å‡ºçš„æ–‡ä»¶å‘é€ç»™åŒäº‹ã€‚")
    print("")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        input("\næŒ‰å›è½¦é”®é€€å‡º...")
