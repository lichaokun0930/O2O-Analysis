# -*- coding: utf-8 -*-
"""
åˆ†å±‚ç¼“å­˜ç®¡ç†å™¨ - V8.4 ä¼ä¸šçº§æ‰©å±•

å››çº§ç¼“å­˜æ¶æ„:
Level 1: åŸå§‹æ•°æ®ç¼“å­˜ï¼ˆæŒ‰é—¨åº—åˆ†ç‰‡ï¼‰
Level 2: èšåˆæŒ‡æ ‡ç¼“å­˜ï¼ˆæŒ‰é—¨åº—+æ—¥æœŸï¼‰
Level 3: è¯Šæ–­ç»“æœç¼“å­˜ï¼ˆæŒ‰é—¨åº—ç»„åˆï¼‰
Level 4: çƒ­ç‚¹æ•°æ®ç¼“å­˜ï¼ˆLRUè‡ªåŠ¨ç®¡ç†ï¼‰

è®¾è®¡ç†å¿µ:
- åˆ†å±‚å­˜å‚¨ï¼Œå¢é‡è®¡ç®—
- æ™ºèƒ½é¢„çƒ­ï¼ŒæŒ‰éœ€åŠ è½½
- å‹ç¼©å­˜å‚¨ï¼ŒèŠ‚çœå†…å­˜
- çƒ­ç‚¹ä¼˜å…ˆï¼ŒLRUæ·˜æ±°

ä½œè€…: AI Assistant
ç‰ˆæœ¬: V8.4
æ—¥æœŸ: 2025-12-11
"""

import redis
import pickle
import gzip
import hashlib
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Any, Optional, Dict, List, Tuple
import logging
from collections import defaultdict

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class HierarchicalCacheManager:
    """åˆ†å±‚ç¼“å­˜ç®¡ç†å™¨ - æ”¯æŒ100+é—¨åº—ã€ç™¾ä¸‡çº§æ•°æ®"""
    
    # ç¼“å­˜å±‚çº§å®šä¹‰
    LEVEL_RAW_DATA = 1      # åŸå§‹æ•°æ®ï¼ˆæŒ‰é—¨åº—ï¼‰
    LEVEL_METRICS = 2       # èšåˆæŒ‡æ ‡ï¼ˆæŒ‰é—¨åº—+æ—¥æœŸï¼‰
    LEVEL_DIAGNOSIS = 3     # è¯Šæ–­ç»“æœï¼ˆæŒ‰é—¨åº—ç»„åˆï¼‰
    LEVEL_HOTSPOT = 4       # çƒ­ç‚¹æ•°æ®ï¼ˆLRUï¼‰
    
    def __init__(
        self,
        host: str = 'localhost',
        port: int = 6379,
        db: int = 0,
        password: Optional[str] = None,
        max_memory_mb: int = 1024,  # é»˜è®¤1GBï¼Œé€‚åˆ100å®¶é—¨åº—
        enable_compression: bool = True
    ):
        """
        åˆå§‹åŒ–åˆ†å±‚ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            host: RedisæœåŠ¡å™¨åœ°å€
            port: Redisç«¯å£
            db: æ•°æ®åº“ç¼–å·
            password: å¯†ç 
            max_memory_mb: æœ€å¤§å†…å­˜é™åˆ¶ï¼ˆMBï¼‰
            enable_compression: æ˜¯å¦å¯ç”¨å‹ç¼©
        """
        self.enable_compression = enable_compression
        self.enabled = False
        self.access_log = []  # è®¿é—®æ—¥å¿—ï¼ˆç”¨äºçƒ­ç‚¹åˆ†æï¼‰
        
        try:
            self.client = redis.Redis(
                host=host,
                port=port,
                db=db,
                password=password,
                decode_responses=False,
                socket_connect_timeout=5,
                socket_timeout=5,
                retry_on_timeout=True
            )
            
            # æµ‹è¯•è¿æ¥
            self.client.ping()
            self.enabled = True
            
            # é…ç½®å†…å­˜é™åˆ¶å’Œæ·˜æ±°ç­–ç•¥
            try:
                self.client.config_set('maxmemory', f'{max_memory_mb}mb')
                self.client.config_set('maxmemory-policy', 'allkeys-lru')
                logger.info(f"âœ… Redisé…ç½®æˆåŠŸ: maxmemory={max_memory_mb}MB, policy=allkeys-lru")
            except Exception as e:
                logger.warning(f"âš ï¸ Redisé…ç½®å¤±è´¥ï¼ˆå¯èƒ½éœ€è¦ç®¡ç†å‘˜æƒé™ï¼‰: {e}")
            
            logger.info(f"âœ… åˆ†å±‚ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ: {host}:{port}/{db}")
            
        except Exception as e:
            logger.error(f"âŒ Redisè¿æ¥å¤±è´¥: {e}")
            self.client = None
            self.enabled = False
    
    def _generate_key(self, level: int, **kwargs) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            level: ç¼“å­˜å±‚çº§
            **kwargs: é”®å‚æ•°
            
        Returns:
            æ ¼å¼åŒ–çš„ç¼“å­˜é”®
        """
        # å¯¹å‚æ•°æ’åºå¹¶åºåˆ—åŒ–
        params_str = json.dumps(kwargs, sort_keys=True, default=str)
        params_hash = hashlib.md5(params_str.encode()).hexdigest()[:12]
        
        level_names = {
            1: 'raw',
            2: 'metrics',
            3: 'diagnosis',
            4: 'hotspot'
        }
        level_name = level_names.get(level, 'unknown')
        
        return f"o2o:v8.4:{level_name}:{params_hash}"
    
    def _compress(self, data: bytes) -> bytes:
        """å‹ç¼©æ•°æ®"""
        if not self.enable_compression:
            return data
        return gzip.compress(data, compresslevel=6)
    
    def _decompress(self, data: bytes) -> bytes:
        """è§£å‹æ•°æ®"""
        if not self.enable_compression:
            return data
        try:
            return gzip.decompress(data)
        except:
            # å¯èƒ½æ˜¯æœªå‹ç¼©çš„æ—§æ•°æ®
            return data
    
    def _serialize(self, value: Any) -> bytes:
        """åºåˆ—åŒ–æ•°æ®"""
        if isinstance(value, pd.DataFrame):
            # DataFrameç‰¹æ®Šå¤„ç†ï¼ˆæ›´ç´§å‡‘ï¼‰
            return pickle.dumps({
                'type': 'dataframe',
                'data': value.to_dict('tight'),  # tightæ ¼å¼æ›´ç´§å‡‘
            }, protocol=pickle.HIGHEST_PROTOCOL)
        else:
            return pickle.dumps({
                'type': 'generic',
                'data': value
            }, protocol=pickle.HIGHEST_PROTOCOL)
    
    def _deserialize(self, data: bytes) -> Any:
        """ååºåˆ—åŒ–æ•°æ®"""
        obj = pickle.loads(data)
        if obj['type'] == 'dataframe':
            return pd.DataFrame.from_dict(obj['data'], orient='tight')
        else:
            return obj['data']
    
    # ========== Level 1: åŸå§‹æ•°æ®ç¼“å­˜ ==========
    
    def cache_raw_data(
        self,
        store_id: str,
        date_range: Tuple[str, str],
        data: pd.DataFrame,
        ttl: int = 86400  # 24å°æ—¶
    ) -> bool:
        """
        ç¼“å­˜åŸå§‹æ•°æ®ï¼ˆæŒ‰é—¨åº—åˆ†ç‰‡ï¼‰
        
        Args:
            store_id: é—¨åº—ID
            date_range: æ—¥æœŸèŒƒå›´ (start, end)
            data: åŸå§‹æ•°æ®
            ttl: è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        """
        if not self.enabled:
            return False
        
        try:
            key = self._generate_key(
                self.LEVEL_RAW_DATA,
                store_id=store_id,
                date_range=date_range
            )
            
            # åºåˆ—åŒ–å¹¶å‹ç¼©
            serialized = self._serialize(data)
            compressed = self._compress(serialized)
            
            # å­˜å‚¨
            self.client.setex(key, ttl, compressed)
            
            compression_ratio = len(compressed) / len(serialized) * 100
            logger.info(
                f"âœ… [L1] åŸå§‹æ•°æ®å·²ç¼“å­˜: store={store_id}, "
                f"size={len(compressed)/1024:.1f}KB, "
                f"compression={compression_ratio:.1f}%"
            )
            return True
            
        except Exception as e:
            logger.error(f"âŒ [L1] ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def get_raw_data(
        self,
        store_id: str,
        date_range: Tuple[str, str]
    ) -> Optional[pd.DataFrame]:
        """è·å–åŸå§‹æ•°æ®"""
        if not self.enabled:
            return None
        
        try:
            key = self._generate_key(
                self.LEVEL_RAW_DATA,
                store_id=store_id,
                date_range=date_range
            )
            
            compressed = self.client.get(key)
            if compressed is None:
                return None
            
            # è§£å‹å¹¶ååºåˆ—åŒ–
            serialized = self._decompress(compressed)
            data = self._deserialize(serialized)
            
            logger.info(f"âœ… [L1] åŸå§‹æ•°æ®å‘½ä¸­: store={store_id}")
            return data
            
        except Exception as e:
            logger.error(f"âŒ [L1] è¯»å–å¤±è´¥: {e}")
            return None
    
    # ========== Level 2: èšåˆæŒ‡æ ‡ç¼“å­˜ ==========
    
    def cache_metrics(
        self,
        store_id: str,
        date: str,
        metrics: Dict[str, Any],
        ttl: int = 21600  # 6å°æ—¶
    ) -> bool:
        """
        ç¼“å­˜èšåˆæŒ‡æ ‡ï¼ˆæŒ‰é—¨åº—+æ—¥æœŸï¼‰
        
        Args:
            store_id: é—¨åº—ID
            date: æ—¥æœŸ
            metrics: èšåˆæŒ‡æ ‡å­—å…¸
            ttl: è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        """
        if not self.enabled:
            return False
        
        try:
            key = self._generate_key(
                self.LEVEL_METRICS,
                store_id=store_id,
                date=date
            )
            
            # åºåˆ—åŒ–å¹¶å‹ç¼©
            serialized = self._serialize(metrics)
            compressed = self._compress(serialized)
            
            # å­˜å‚¨
            self.client.setex(key, ttl, compressed)
            
            logger.info(f"âœ… [L2] æŒ‡æ ‡å·²ç¼“å­˜: store={store_id}, date={date}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ [L2] ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def get_metrics(
        self,
        store_id: str,
        date: str
    ) -> Optional[Dict[str, Any]]:
        """è·å–èšåˆæŒ‡æ ‡"""
        if not self.enabled:
            return None
        
        try:
            key = self._generate_key(
                self.LEVEL_METRICS,
                store_id=store_id,
                date=date
            )
            
            compressed = self.client.get(key)
            if compressed is None:
                return None
            
            # è§£å‹å¹¶ååºåˆ—åŒ–
            serialized = self._decompress(compressed)
            metrics = self._deserialize(serialized)
            
            logger.info(f"âœ… [L2] æŒ‡æ ‡å‘½ä¸­: store={store_id}, date={date}")
            return metrics
            
        except Exception as e:
            logger.error(f"âŒ [L2] è¯»å–å¤±è´¥: {e}")
            return None
    
    def get_metrics_batch(
        self,
        store_ids: List[str],
        date: str
    ) -> Dict[str, Dict[str, Any]]:
        """æ‰¹é‡è·å–å¤šä¸ªé—¨åº—çš„æŒ‡æ ‡"""
        if not self.enabled:
            return {}
        
        results = {}
        for store_id in store_ids:
            metrics = self.get_metrics(store_id, date)
            if metrics:
                results[store_id] = metrics
        
        return results
    
    # ========== Level 3: è¯Šæ–­ç»“æœç¼“å­˜ ==========
    
    def cache_diagnosis(
        self,
        store_ids: List[str],
        date_range: Tuple[str, str],
        diagnosis: Dict[str, Any],
        ttl: int = 3600  # 1å°æ—¶
    ) -> bool:
        """
        ç¼“å­˜è¯Šæ–­ç»“æœï¼ˆæŒ‰é—¨åº—ç»„åˆï¼‰
        
        Args:
            store_ids: é—¨åº—IDåˆ—è¡¨
            date_range: æ—¥æœŸèŒƒå›´
            diagnosis: è¯Šæ–­ç»“æœ
            ttl: è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        """
        if not self.enabled:
            return False
        
        try:
            # é—¨åº—IDæ’åºï¼Œç¡®ä¿ç›¸åŒç»„åˆç”Ÿæˆç›¸åŒé”®
            sorted_stores = sorted(store_ids) if store_ids else ['all']
            
            key = self._generate_key(
                self.LEVEL_DIAGNOSIS,
                stores='_'.join(sorted_stores),
                date_range=date_range
            )
            
            # åºåˆ—åŒ–å¹¶å‹ç¼©
            serialized = self._serialize(diagnosis)
            compressed = self._compress(serialized)
            
            # å­˜å‚¨
            self.client.setex(key, ttl, compressed)
            
            logger.info(
                f"âœ… [L3] è¯Šæ–­ç»“æœå·²ç¼“å­˜: stores={len(sorted_stores)}, "
                f"size={len(compressed)/1024:.1f}KB"
            )
            return True
            
        except Exception as e:
            logger.error(f"âŒ [L3] ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def get_diagnosis(
        self,
        store_ids: List[str],
        date_range: Tuple[str, str]
    ) -> Optional[Dict[str, Any]]:
        """è·å–è¯Šæ–­ç»“æœ"""
        if not self.enabled:
            return None
        
        try:
            # è®°å½•è®¿é—®æ—¥å¿—ï¼ˆç”¨äºçƒ­ç‚¹åˆ†æï¼‰
            self._log_access(store_ids, date_range)
            
            sorted_stores = sorted(store_ids) if store_ids else ['all']
            
            key = self._generate_key(
                self.LEVEL_DIAGNOSIS,
                stores='_'.join(sorted_stores),
                date_range=date_range
            )
            
            compressed = self.client.get(key)
            if compressed is None:
                logger.debug(f"â­ï¸ [L3] è¯Šæ–­ç»“æœæœªå‘½ä¸­: stores={len(sorted_stores)}")
                return None
            
            # è§£å‹å¹¶ååºåˆ—åŒ–
            serialized = self._decompress(compressed)
            diagnosis = self._deserialize(serialized)
            
            logger.info(f"âœ… [L3] è¯Šæ–­ç»“æœå‘½ä¸­: stores={len(sorted_stores)}")
            return diagnosis
            
        except Exception as e:
            logger.error(f"âŒ [L3] è¯»å–å¤±è´¥: {e}")
            return None
    
    # ========== è®¿é—®æ—¥å¿—å’Œçƒ­ç‚¹åˆ†æ ==========
    
    def _log_access(self, store_ids: List[str], date_range: Tuple[str, str]):
        """è®°å½•è®¿é—®æ—¥å¿—"""
        self.access_log.append({
            'timestamp': datetime.now(),
            'store_ids': store_ids,
            'date_range': date_range
        })
        
        # åªä¿ç•™æœ€è¿‘1000æ¡
        if len(self.access_log) > 1000:
            self.access_log = self.access_log[-1000:]
    
    def analyze_hot_stores(self, top_n: int = 20) -> List[str]:
        """
        åˆ†æçƒ­ç‚¹é—¨åº—
        
        Args:
            top_n: è¿”å›TOP Nä¸ªçƒ­ç‚¹é—¨åº—
            
        Returns:
            çƒ­ç‚¹é—¨åº—IDåˆ—è¡¨
        """
        if not self.access_log:
            return []
        
        # ç»Ÿè®¡é—¨åº—è®¿é—®é¢‘ç‡
        store_count = defaultdict(int)
        for log in self.access_log:
            for store_id in log['store_ids']:
                store_count[store_id] += 1
        
        # æŒ‰è®¿é—®é¢‘ç‡æ’åº
        sorted_stores = sorted(
            store_count.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        hot_stores = [store_id for store_id, _ in sorted_stores[:top_n]]
        
        logger.info(f"ğŸ“Š çƒ­ç‚¹åˆ†æ: TOP{top_n}é—¨åº— = {hot_stores[:5]}...")
        return hot_stores
    
    # ========== ç¼“å­˜ç®¡ç† ==========
    
    def clear_level(self, level: int) -> int:
        """æ¸…ç©ºæŒ‡å®šå±‚çº§çš„ç¼“å­˜"""
        if not self.enabled:
            return 0
        
        try:
            level_names = {1: 'raw', 2: 'metrics', 3: 'diagnosis', 4: 'hotspot'}
            level_name = level_names.get(level, 'unknown')
            pattern = f"o2o:v8.4:{level_name}:*"
            
            keys = self.client.keys(pattern)
            if keys:
                deleted = self.client.delete(*keys)
                logger.info(f"ğŸ—‘ï¸ æ¸…ç©ºLevel {level}ç¼“å­˜: {deleted}ä¸ªé”®")
                return deleted
            return 0
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
            return 0
    
    def clear_all(self) -> bool:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        if not self.enabled:
            return False
        
        try:
            pattern = "o2o:v8.4:*"
            keys = self.client.keys(pattern)
            if keys:
                deleted = self.client.delete(*keys)
                logger.info(f"ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰ç¼“å­˜: {deleted}ä¸ªé”®")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enabled:
            return {'enabled': False}
        
        try:
            info = self.client.info('stats')
            memory = self.client.info('memory')
            
            # ç»Ÿè®¡å„å±‚çº§é”®æ•°é‡
            level_counts = {}
            for level, name in {1: 'raw', 2: 'metrics', 3: 'diagnosis', 4: 'hotspot'}.items():
                pattern = f"o2o:v8.4:{name}:*"
                keys = self.client.keys(pattern)
                level_counts[f'level_{level}'] = len(keys)
            
            return {
                'enabled': True,
                'total_keys': self.client.dbsize(),
                'used_memory_mb': round(memory.get('used_memory', 0) / 1024 / 1024, 2),
                'max_memory_mb': round(memory.get('maxmemory', 0) / 1024 / 1024, 2),
                'memory_usage_pct': round(
                    memory.get('used_memory', 0) / max(memory.get('maxmemory', 1), 1) * 100, 2
                ),
                'total_commands': info.get('total_commands_processed', 0),
                'hits': info.get('keyspace_hits', 0),
                'misses': info.get('keyspace_misses', 0),
                'hit_rate': round(
                    info.get('keyspace_hits', 0) / 
                    max(info.get('keyspace_hits', 0) + info.get('keyspace_misses', 0), 1) * 100,
                    2
                ),
                **level_counts
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {'enabled': False, 'error': str(e)}


# å…¨å±€å®ä¾‹ï¼ˆå•ä¾‹ï¼‰
_global_hierarchical_cache = None


def get_hierarchical_cache(**kwargs) -> HierarchicalCacheManager:
    """è·å–å…¨å±€åˆ†å±‚ç¼“å­˜ç®¡ç†å™¨å®ä¾‹"""
    global _global_hierarchical_cache
    
    if _global_hierarchical_cache is None:
        _global_hierarchical_cache = HierarchicalCacheManager(**kwargs)
    
    return _global_hierarchical_cache


# å¯¼å‡º
__all__ = [
    'HierarchicalCacheManager',
    'get_hierarchical_cache'
]


if __name__ == "__main__":
    print("=" * 80)
    print(" åˆ†å±‚ç¼“å­˜ç®¡ç†å™¨æµ‹è¯•")
    print("=" * 80)
    
    # åˆå§‹åŒ–
    cache = HierarchicalCacheManager(
        host='localhost',
        port=6379,
        max_memory_mb=512,
        enable_compression=True
    )
    
    if cache.enabled:
        print("\nâœ… ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•Level 3ç¼“å­˜
        print("\næµ‹è¯•Level 3ï¼ˆè¯Šæ–­ç»“æœç¼“å­˜ï¼‰:")
        test_diagnosis = {
            'overflow': {'count': 5, 'loss': 123.45},
            'delivery': {'count': 3, 'extra_cost': 67.89}
        }
        
        cache.cache_diagnosis(
            store_ids=['store_001', 'store_002'],
            date_range=('2025-12-01', '2025-12-11'),
            diagnosis=test_diagnosis
        )
        
        result = cache.get_diagnosis(
            store_ids=['store_001', 'store_002'],
            date_range=('2025-12-01', '2025-12-11')
        )
        
        print(f"ç¼“å­˜ç»“æœ: {result}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        print("\nç¼“å­˜ç»Ÿè®¡:")
        stats = cache.get_stats()
        for key, value in stats.items():
            print(f"  {key}: {value}")
    else:
        print("\nâš ï¸ Redisæœªå¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
