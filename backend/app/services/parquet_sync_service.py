# -*- coding: utf-8 -*-
"""
Parquet æ•°æ®åŒæ­¥æœåŠ¡

è´Ÿè´£å°† PostgreSQL æ•°æ®åŒæ­¥åˆ° Parquet æ–‡ä»¶
æ”¯æŒå®šæ—¶ä»»åŠ¡è‡ªåŠ¨åŒæ­¥å’Œæ‰‹åŠ¨è§¦å‘

å­˜å‚¨ç»“æ„:
data/
â”œâ”€â”€ raw/                          # åŸå§‹æ•°æ®ï¼ˆæŒ‰æ—¥æœŸåˆ†åŒºï¼‰
â”‚   â”œâ”€â”€ 2025/
â”‚   â”‚   â”œâ”€â”€ 12/
â”‚   â”‚   â”‚   â”œâ”€â”€ orders_20251201.parquet
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ 2026/
â”œâ”€â”€ aggregated/                   # é¢„èšåˆæ•°æ®
â”‚   â”œâ”€â”€ daily/
â”‚   â”‚   â”œâ”€â”€ kpi_daily.parquet
â”‚   â”‚   â”œâ”€â”€ channel_daily.parquet
â”‚   â”‚   â””â”€â”€ category_daily.parquet
â””â”€â”€ metadata/
    â”œâ”€â”€ partitions.json
    â””â”€â”€ last_update.json

çŠ¶æ€: âœ… å·²è½åœ°ï¼ˆ2026-01-20ï¼‰
- 30ä¸ªåŸå§‹Parquetæ–‡ä»¶ï¼ˆ18.52MBï¼‰
- 3ä¸ªèšåˆParquetæ–‡ä»¶
- å®šæ—¶ä»»åŠ¡æ¯å¤©02:00è‡ªåŠ¨åŒæ­¥
"""
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import Optional, Dict, List
import json


class ParquetSyncService:
    """Parquet æ•°æ®åŒæ­¥æœåŠ¡"""
    
    def __init__(self, data_dir: str = None):
        # é»˜è®¤æ•°æ®ç›®å½•
        if data_dir is None:
            project_root = Path(__file__).resolve().parent.parent.parent.parent
            data_dir = project_root / "data"
        
        self.data_dir = Path(data_dir)
        self.raw_dir = self.data_dir / "raw"
        self.agg_dir = self.data_dir / "aggregated"
        self.metadata_dir = self.data_dir / "metadata"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for d in [self.raw_dir, self.agg_dir, self.metadata_dir]:
            d.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“¦ ParquetåŒæ­¥æœåŠ¡å·²åˆå§‹åŒ–: {self.data_dir}")
    
    def sync_raw_data(self, target_date: date, df: pd.DataFrame) -> str:
        """
        åŒæ­¥åŸå§‹æ•°æ®åˆ° Parquetï¼ˆæŒ‰æ—¥æœŸåˆ†åŒºï¼‰
        
        Args:
            target_date: æ•°æ®æ—¥æœŸ
            df: è®¢å•æ•°æ® DataFrame
        
        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
        """
        if df.empty:
            print(f"âš ï¸ ç©ºæ•°æ®ï¼Œè·³è¿‡åŒæ­¥: {target_date}")
            return ""
        
        # æ„å»ºåˆ†åŒºè·¯å¾„
        partition_path = self.raw_dir / str(target_date.year) / f"{target_date.month:02d}"
        partition_path.mkdir(parents=True, exist_ok=True)
        
        # æ–‡ä»¶å
        filename = f"orders_{target_date.strftime('%Y%m%d')}.parquet"
        filepath = partition_path / filename
        
        # å†™å…¥ Parquetï¼ˆä½¿ç”¨ snappy å‹ç¼©ï¼‰
        df.to_parquet(
            filepath,
            engine='pyarrow',
            compression='snappy',
            index=False
        )
        
        # æ›´æ–°å…ƒæ•°æ®
        self._update_partition_metadata(target_date, len(df))
        
        print(f"âœ… åŸå§‹æ•°æ®å·²åŒæ­¥: {filepath} ({len(df)} è¡Œ)")
        return str(filepath)
    
    def generate_daily_aggregations(self, target_date: date) -> Dict[str, str]:
        """
        ç”Ÿæˆæ—¥èšåˆæ•°æ®
        
        Args:
            target_date: èšåˆæ—¥æœŸ
        
        Returns:
            ç”Ÿæˆçš„èšåˆæ–‡ä»¶è·¯å¾„å­—å…¸
        """
        # è¯»å–å½“æ—¥åŸå§‹æ•°æ®
        raw_file = self.raw_dir / str(target_date.year) / f"{target_date.month:02d}" / f"orders_{target_date.strftime('%Y%m%d')}.parquet"
        
        if not raw_file.exists():
            print(f"âš ï¸ åŸå§‹æ•°æ®ä¸å­˜åœ¨: {raw_file}")
            return {}
        
        df = pd.read_parquet(raw_file)
        results = {}
        
        # ç¡®ä¿dailyç›®å½•å­˜åœ¨
        daily_dir = self.agg_dir / "daily"
        daily_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. KPI æ—¥èšåˆ
        try:
            kpi_agg = self._aggregate_kpi(df, target_date)
            kpi_file = daily_dir / "kpi_daily.parquet"
            self._append_or_create(kpi_file, kpi_agg, ['æ—¥æœŸ', 'é—¨åº—åç§°'])
            results['kpi'] = str(kpi_file)
        except Exception as e:
            print(f"âš ï¸ KPIèšåˆå¤±è´¥: {e}")
        
        # 2. æ¸ é“æ—¥èšåˆ
        try:
            channel_agg = self._aggregate_channel(df, target_date)
            channel_file = daily_dir / "channel_daily.parquet"
            self._append_or_create(channel_file, channel_agg, ['æ—¥æœŸ', 'é—¨åº—åç§°', 'æ¸ é“'])
            results['channel'] = str(channel_file)
        except Exception as e:
            print(f"âš ï¸ æ¸ é“èšåˆå¤±è´¥: {e}")
        
        # 3. å“ç±»æ—¥èšåˆ
        try:
            category_agg = self._aggregate_category(df, target_date)
            category_file = daily_dir / "category_daily.parquet"
            self._append_or_create(category_file, category_agg, ['æ—¥æœŸ', 'é—¨åº—åç§°', 'ä¸€çº§åˆ†ç±»å'])
            results['category'] = str(category_file)
        except Exception as e:
            print(f"âš ï¸ å“ç±»èšåˆå¤±è´¥: {e}")
        
        # æ›´æ–°å…ƒæ•°æ®
        self._update_last_sync(target_date)
        
        print(f"âœ… æ—¥èšåˆæ•°æ®å·²ç”Ÿæˆ: {target_date}")
        return results
    
    def _aggregate_kpi(self, df: pd.DataFrame, target_date: date) -> pd.DataFrame:
        """KPI èšåˆé€»è¾‘"""
        if df.empty:
            return pd.DataFrame()
        
        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
        required_fields = ['è®¢å•ID', 'é—¨åº—åç§°', 'å®æ”¶ä»·æ ¼', 'åˆ©æ¶¦é¢']
        for field in required_fields:
            if field not in df.columns:
                df[field] = 0 if field != 'é—¨åº—åç§°' else 'unknown'
        
        # å¡«å……ç¼ºå¤±å­—æ®µ
        for field in ['å¹³å°æœåŠ¡è´¹', 'ç‰©æµé…é€è´¹', 'ä¼å®¢åè¿”']:
            if field not in df.columns:
                df[field] = 0
            else:
                df[field] = df[field].fillna(0)
        
        # è®¢å•çº§èšåˆ
        order_agg = df.groupby('è®¢å•ID').agg({
            'å®æ”¶ä»·æ ¼': 'sum',
            'åˆ©æ¶¦é¢': 'sum',
            'å¹³å°æœåŠ¡è´¹': 'sum',
            'ç‰©æµé…é€è´¹': 'first',
            'ä¼å®¢åè¿”': 'sum',
            'é—¨åº—åç§°': 'first',
        }).reset_index()
        
        # è®¡ç®—è®¢å•å®é™…åˆ©æ¶¦
        order_agg['è®¢å•å®é™…åˆ©æ¶¦'] = (
            order_agg['åˆ©æ¶¦é¢'] -
            order_agg['å¹³å°æœåŠ¡è´¹'] -
            order_agg['ç‰©æµé…é€è´¹'] +
            order_agg['ä¼å®¢åè¿”']
        )
        
        # æŒ‰é—¨åº—èšåˆ
        kpi = order_agg.groupby('é—¨åº—åç§°').agg({
            'è®¢å•ID': 'count',
            'å®æ”¶ä»·æ ¼': 'sum',
            'è®¢å•å®é™…åˆ©æ¶¦': 'sum',
        }).reset_index()
        
        kpi.columns = ['é—¨åº—åç§°', 'è®¢å•æ•°', 'å•†å“å®æ”¶é¢', 'æ€»åˆ©æ¶¦']
        # æ—¥æœŸè½¬ä¸ºå­—ç¬¦ä¸²é¿å…Parquetç±»å‹é—®é¢˜
        kpi['æ—¥æœŸ'] = str(target_date)
        kpi['å¹³å‡å®¢å•ä»·'] = kpi['å•†å“å®æ”¶é¢'] / kpi['è®¢å•æ•°'].replace(0, 1)
        kpi['åˆ©æ¶¦ç‡'] = kpi['æ€»åˆ©æ¶¦'] / kpi['å•†å“å®æ”¶é¢'].replace(0, 1) * 100
        
        # åŠ¨é”€å•†å“æ•°
        if 'å•†å“åç§°' in df.columns and 'æœˆå”®' in df.columns:
            active_products = df[df['æœˆå”®'] > 0].groupby('é—¨åº—åç§°')['å•†å“åç§°'].nunique().reset_index()
            active_products.columns = ['é—¨åº—åç§°', 'åŠ¨é”€å•†å“æ•°']
            kpi = kpi.merge(active_products, on='é—¨åº—åç§°', how='left')
            kpi['åŠ¨é”€å•†å“æ•°'] = kpi['åŠ¨é”€å•†å“æ•°'].fillna(0).astype(int)
        else:
            kpi['åŠ¨é”€å•†å“æ•°'] = 0
        
        return kpi
    
    def _aggregate_channel(self, df: pd.DataFrame, target_date: date) -> pd.DataFrame:
        """æ¸ é“èšåˆ"""
        if df.empty or 'æ¸ é“' not in df.columns:
            return pd.DataFrame()
        
        # å¡«å……ç¼ºå¤±å­—æ®µ
        for field in ['å¹³å°æœåŠ¡è´¹', 'ç‰©æµé…é€è´¹', 'ä¼å®¢åè¿”']:
            if field not in df.columns:
                df[field] = 0
            else:
                df[field] = df[field].fillna(0)
        
        # è®¢å•çº§èšåˆ
        order_agg = df.groupby('è®¢å•ID').agg({
            'å®æ”¶ä»·æ ¼': 'sum',
            'åˆ©æ¶¦é¢': 'sum',
            'å¹³å°æœåŠ¡è´¹': 'sum',
            'ç‰©æµé…é€è´¹': 'first',
            'ä¼å®¢åè¿”': 'sum',
            'é—¨åº—åç§°': 'first',
            'æ¸ é“': 'first',
        }).reset_index()
        
        order_agg['è®¢å•å®é™…åˆ©æ¶¦'] = (
            order_agg['åˆ©æ¶¦é¢'] -
            order_agg['å¹³å°æœåŠ¡è´¹'] -
            order_agg['ç‰©æµé…é€è´¹'] +
            order_agg['ä¼å®¢åè¿”']
        )
        
        # æŒ‰é—¨åº—+æ¸ é“èšåˆ
        channel_agg = order_agg.groupby(['é—¨åº—åç§°', 'æ¸ é“']).agg({
            'è®¢å•ID': 'count',
            'å®æ”¶ä»·æ ¼': 'sum',
            'è®¢å•å®é™…åˆ©æ¶¦': 'sum',
        }).reset_index()
        
        channel_agg.columns = ['é—¨åº—åç§°', 'æ¸ é“', 'è®¢å•æ•°', 'é”€å”®é¢', 'åˆ©æ¶¦']
        # æ—¥æœŸè½¬ä¸ºå­—ç¬¦ä¸²é¿å…Parquetç±»å‹é—®é¢˜
        channel_agg['æ—¥æœŸ'] = str(target_date)
        channel_agg['å®¢å•ä»·'] = channel_agg['é”€å”®é¢'] / channel_agg['è®¢å•æ•°'].replace(0, 1)
        channel_agg['åˆ©æ¶¦ç‡'] = channel_agg['åˆ©æ¶¦'] / channel_agg['é”€å”®é¢'].replace(0, 1) * 100
        
        return channel_agg
    
    def _aggregate_category(self, df: pd.DataFrame, target_date: date) -> pd.DataFrame:
        """å“ç±»èšåˆ"""
        if df.empty or 'ä¸€çº§åˆ†ç±»å' not in df.columns:
            return pd.DataFrame()
        
        # å¡«å……ç¼ºå¤±å­—æ®µ
        if 'æœˆå”®' not in df.columns:
            df['æœˆå”®'] = 1
        
        category_agg = df.groupby(['é—¨åº—åç§°', 'ä¸€çº§åˆ†ç±»å']).agg({
            'è®¢å•ID': 'nunique',
            'å®æ”¶ä»·æ ¼': 'sum',
            'åˆ©æ¶¦é¢': 'sum',
            'æœˆå”®': 'sum',
        }).reset_index()
        
        category_agg.columns = ['é—¨åº—åç§°', 'ä¸€çº§åˆ†ç±»å', 'è®¢å•æ•°', 'é”€å”®é¢', 'åˆ©æ¶¦', 'é”€é‡']
        # æ—¥æœŸè½¬ä¸ºå­—ç¬¦ä¸²é¿å…Parquetç±»å‹é—®é¢˜
        category_agg['æ—¥æœŸ'] = str(target_date)
        
        return category_agg
    
    def _append_or_create(self, filepath: Path, df: pd.DataFrame, dedup_keys: List[str]):
        """è¿½åŠ æˆ–åˆ›å»º Parquet æ–‡ä»¶"""
        if df.empty:
            return
        
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        if filepath.exists():
            # è¯»å–ç°æœ‰æ•°æ®
            existing = pd.read_parquet(filepath)
            # åˆå¹¶ï¼ˆå»é‡ï¼‰
            combined = pd.concat([existing, df], ignore_index=True)
            combined = combined.drop_duplicates(subset=dedup_keys, keep='last')
        else:
            combined = df
        
        combined.to_parquet(filepath, engine='pyarrow', compression='snappy', index=False)
    
    def _update_partition_metadata(self, target_date: date, record_count: int):
        """æ›´æ–°åˆ†åŒºå…ƒæ•°æ®"""
        metadata_file = self.metadata_dir / "partitions.json"
        
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        else:
            metadata = {"partitions": {}}
        
        partition_key = target_date.strftime('%Y-%m')
        if partition_key not in metadata["partitions"]:
            metadata["partitions"][partition_key] = {"dates": {}, "total_records": 0}
        
        metadata["partitions"][partition_key]["dates"][str(target_date)] = record_count
        metadata["partitions"][partition_key]["total_records"] = sum(
            metadata["partitions"][partition_key]["dates"].values()
        )
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    def _update_last_sync(self, target_date: date):
        """æ›´æ–°æœ€ååŒæ­¥æ—¶é—´"""
        metadata_file = self.metadata_dir / "last_update.json"
        metadata = {
            "last_sync_date": str(target_date),
            "last_sync_time": datetime.now().isoformat(),
        }
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    def get_status(self) -> Dict:
        """è·å–åŒæ­¥çŠ¶æ€"""
        # ç»Ÿè®¡Parquetæ–‡ä»¶
        raw_files = list(self.raw_dir.glob("**/*.parquet"))
        agg_files = list(self.agg_dir.glob("**/*.parquet"))
        
        # è¯»å–å…ƒæ•°æ®
        last_update_file = self.metadata_dir / "last_update.json"
        if last_update_file.exists():
            with open(last_update_file, 'r') as f:
                last_update = json.load(f)
        else:
            last_update = None
        
        return {
            "data_dir": str(self.data_dir),
            "raw_files_count": len(raw_files),
            "aggregated_files_count": len(agg_files),
            "last_update": last_update,
        }


# å…¨å±€å•ä¾‹
parquet_sync_service = ParquetSyncService()
