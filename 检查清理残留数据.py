# -*- coding: utf-8 -*-
"""
æ£€æŸ¥å’Œæ¸…ç†æ®‹ç•™æ•°æ®

æ£€æŸ¥å†…å®¹ï¼š
1. Redis ç¼“å­˜ä¸­çš„é—¨åº—æ•°æ®
2. é¢„èšåˆè¡¨ä¸­å·²åˆ é™¤é—¨åº—çš„æ®‹ç•™æ•°æ®
3. å¯¹æ¯” orders è¡¨å’Œé¢„èšåˆè¡¨çš„é—¨åº—åˆ—è¡¨

ä½¿ç”¨æ–¹å¼ï¼š
    python æ£€æŸ¥æ¸…ç†æ®‹ç•™æ•°æ®.py          # ä»…æ£€æŸ¥
    python æ£€æŸ¥æ¸…ç†æ®‹ç•™æ•°æ®.py --clean   # æ£€æŸ¥å¹¶æ¸…ç†
"""

import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from sqlalchemy import text
from database.connection import SessionLocal

# é¢„èšåˆè¡¨åˆ—è¡¨
AGGREGATION_TABLES = [
    "store_daily_summary",
    "store_hourly_summary",
    "category_daily_summary",
    "delivery_summary",
    "product_daily_summary"
]


def get_orders_stores():
    """è·å– orders è¡¨ä¸­çš„é—¨åº—åˆ—è¡¨"""
    session = SessionLocal()
    try:
        result = session.execute(text("""
            SELECT DISTINCT store_name FROM orders WHERE store_name IS NOT NULL
        """))
        stores = [row[0] for row in result.fetchall()]
        return set(stores)
    finally:
        session.close()


def get_aggregation_stores(table_name):
    """è·å–é¢„èšåˆè¡¨ä¸­çš„é—¨åº—åˆ—è¡¨"""
    session = SessionLocal()
    try:
        result = session.execute(text(f"""
            SELECT DISTINCT store_name FROM {table_name} WHERE store_name IS NOT NULL
        """))
        stores = [row[0] for row in result.fetchall()]
        return set(stores)
    except Exception as e:
        print(f"   âš ï¸ è¡¨ {table_name} ä¸å­˜åœ¨æˆ–æŸ¥è¯¢å¤±è´¥: {e}")
        return set()
    finally:
        session.close()


def check_redis_cache():
    """æ£€æŸ¥ Redis ç¼“å­˜"""
    print("\n" + "="*60)
    print("ğŸ“¦ æ£€æŸ¥ Redis ç¼“å­˜")
    print("="*60)
    
    try:
        from redis_cache_manager import RedisCacheManager
        cache = RedisCacheManager()
        
        if not cache.enabled:
            print("âš ï¸ Redis æœªå¯ç”¨æˆ–æœªè¿æ¥")
            return None, []
        
        # è·å–æ‰€æœ‰é”®
        redis_client = cache.redis_client
        all_keys = []
        
        # æ‰«ææ‰€æœ‰é”®
        cursor = 0
        while True:
            cursor, keys = redis_client.scan(cursor, match="*", count=1000)
            all_keys.extend([k.decode() if isinstance(k, bytes) else k for k in keys])
            if cursor == 0:
                break
        
        print(f"ğŸ“Š Redis ä¸­å…±æœ‰ {len(all_keys)} ä¸ªé”®")
        
        # åˆ†ç±»ç»Ÿè®¡
        store_keys = [k for k in all_keys if 'store' in k.lower() or 'order' in k.lower()]
        other_keys = [k for k in all_keys if k not in store_keys]
        
        print(f"   â€¢ é—¨åº—/è®¢å•ç›¸å…³: {len(store_keys)} ä¸ª")
        print(f"   â€¢ å…¶ä»–: {len(other_keys)} ä¸ª")
        
        # æ˜¾ç¤ºé—¨åº—ç›¸å…³çš„é”®
        if store_keys:
            print(f"\né—¨åº—/è®¢å•ç›¸å…³çš„ç¼“å­˜é”®:")
            for key in store_keys[:20]:  # æœ€å¤šæ˜¾ç¤º20ä¸ª
                try:
                    ttl = redis_client.ttl(key)
                    print(f"   â€¢ {key} (TTL: {ttl}s)")
                except:
                    print(f"   â€¢ {key}")
            if len(store_keys) > 20:
                print(f"   ... è¿˜æœ‰ {len(store_keys) - 20} ä¸ª")
        
        return cache, store_keys
        
    except ImportError:
        print("âš ï¸ redis_cache_manager æ¨¡å—æœªæ‰¾åˆ°")
        return None, []
    except Exception as e:
        print(f"âŒ Redis æ£€æŸ¥å¤±è´¥: {e}")
        return None, []


def check_aggregation_tables():
    """æ£€æŸ¥é¢„èšåˆè¡¨ä¸­çš„æ®‹ç•™æ•°æ®"""
    print("\n" + "="*60)
    print("ğŸ“Š æ£€æŸ¥é¢„èšåˆè¡¨æ®‹ç•™æ•°æ®")
    print("="*60)
    
    # è·å– orders è¡¨ä¸­çš„æœ‰æ•ˆé—¨åº—
    orders_stores = get_orders_stores()
    print(f"\nğŸ“¦ orders è¡¨ä¸­çš„é—¨åº—: {len(orders_stores)} ä¸ª")
    for store in sorted(orders_stores):
        print(f"   â€¢ {store}")
    
    # æ£€æŸ¥æ¯ä¸ªé¢„èšåˆè¡¨
    orphan_data = {}
    
    for table in AGGREGATION_TABLES:
        print(f"\nğŸ” æ£€æŸ¥ {table}...")
        agg_stores = get_aggregation_stores(table)
        
        if not agg_stores:
            print(f"   âœ… è¡¨ä¸ºç©ºæˆ–ä¸å­˜åœ¨")
            continue
        
        # æ‰¾å‡ºå­¤å„¿æ•°æ®ï¼ˆåœ¨é¢„èšåˆè¡¨ä¸­ä½†ä¸åœ¨ orders è¡¨ä¸­çš„é—¨åº—ï¼‰
        orphan_stores = agg_stores - orders_stores
        
        if orphan_stores:
            print(f"   âš ï¸ å‘ç° {len(orphan_stores)} ä¸ªå·²åˆ é™¤é—¨åº—çš„æ®‹ç•™æ•°æ®:")
            orphan_data[table] = orphan_stores
            for store in sorted(orphan_stores):
                # ç»Ÿè®¡æ®‹ç•™æ•°æ®é‡
                session = SessionLocal()
                try:
                    count = session.execute(text(f"""
                        SELECT COUNT(*) FROM {table} WHERE store_name = :store_name
                    """), {"store_name": store}).scalar()
                    print(f"      â€¢ {store}: {count} æ¡")
                finally:
                    session.close()
        else:
            print(f"   âœ… æ— æ®‹ç•™æ•°æ®")
    
    return orphan_data


def clean_redis_cache(cache, store_keys):
    """æ¸…ç† Redis ç¼“å­˜"""
    print("\n" + "="*60)
    print("ğŸ§¹ æ¸…ç† Redis ç¼“å­˜")
    print("="*60)
    
    if not cache or not cache.enabled:
        print("âš ï¸ Redis ä¸å¯ç”¨ï¼Œè·³è¿‡")
        return
    
    if not store_keys:
        print("âœ… æ²¡æœ‰éœ€è¦æ¸…ç†çš„ç¼“å­˜")
        return
    
    redis_client = cache.redis_client
    deleted = 0
    
    for key in store_keys:
        try:
            redis_client.delete(key)
            deleted += 1
        except Exception as e:
            print(f"   âš ï¸ åˆ é™¤ {key} å¤±è´¥: {e}")
    
    print(f"âœ… å·²æ¸…ç† {deleted} ä¸ªç¼“å­˜é”®")


def clean_aggregation_tables(orphan_data):
    """æ¸…ç†é¢„èšåˆè¡¨ä¸­çš„æ®‹ç•™æ•°æ®"""
    print("\n" + "="*60)
    print("ğŸ§¹ æ¸…ç†é¢„èšåˆè¡¨æ®‹ç•™æ•°æ®")
    print("="*60)
    
    if not orphan_data:
        print("âœ… æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ®‹ç•™æ•°æ®")
        return
    
    session = SessionLocal()
    total_deleted = 0
    
    try:
        for table, stores in orphan_data.items():
            print(f"\næ¸…ç† {table}...")
            for store in stores:
                try:
                    result = session.execute(text(f"""
                        DELETE FROM {table} WHERE store_name = :store_name
                    """), {"store_name": store})
                    count = result.rowcount
                    total_deleted += count
                    print(f"   ğŸ—‘ï¸ {store}: {count} æ¡")
                except Exception as e:
                    print(f"   âŒ {store}: {e}")
            
            session.commit()
        
        print(f"\nâœ… å…±æ¸…ç† {total_deleted} æ¡æ®‹ç•™æ•°æ®")
        
    except Exception as e:
        session.rollback()
        print(f"âŒ æ¸…ç†å¤±è´¥: {e}")
    finally:
        session.close()


def main():
    parser = argparse.ArgumentParser(description='æ£€æŸ¥å’Œæ¸…ç†æ®‹ç•™æ•°æ®')
    parser.add_argument('--clean', action='store_true', help='æ‰§è¡Œæ¸…ç†æ“ä½œ')
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("ğŸ” æ®‹ç•™æ•°æ®æ£€æŸ¥å·¥å…·")
    print("="*60)
    
    # 1. æ£€æŸ¥ Redis ç¼“å­˜
    cache, store_keys = check_redis_cache()
    
    # 2. æ£€æŸ¥é¢„èšåˆè¡¨
    orphan_data = check_aggregation_tables()
    
    # 3. æ±‡æ€»
    print("\n" + "="*60)
    print("ğŸ“‹ æ£€æŸ¥ç»“æœæ±‡æ€»")
    print("="*60)
    
    has_issues = False
    
    if store_keys:
        print(f"âš ï¸ Redis ç¼“å­˜: {len(store_keys)} ä¸ªé—¨åº—/è®¢å•ç›¸å…³çš„é”®")
        has_issues = True
    else:
        print("âœ… Redis ç¼“å­˜: æ— é—®é¢˜")
    
    if orphan_data:
        total_orphan = sum(len(stores) for stores in orphan_data.values())
        print(f"âš ï¸ é¢„èšåˆè¡¨: {total_orphan} ä¸ªå·²åˆ é™¤é—¨åº—çš„æ®‹ç•™æ•°æ®")
        has_issues = True
    else:
        print("âœ… é¢„èšåˆè¡¨: æ— æ®‹ç•™æ•°æ®")
    
    # 4. æ¸…ç†ï¼ˆå¦‚æœæŒ‡å®šäº† --cleanï¼‰
    if args.clean and has_issues:
        print("\n" + "="*60)
        print("ğŸ§¹ å¼€å§‹æ¸…ç†...")
        print("="*60)
        
        confirm = input("\nç¡®è®¤æ¸…ç†æ‰€æœ‰æ®‹ç•™æ•°æ®ï¼Ÿ(yes/no): ")
        if confirm.lower() == 'yes':
            clean_redis_cache(cache, store_keys)
            clean_aggregation_tables(orphan_data)
            print("\nâœ… æ¸…ç†å®Œæˆï¼")
        else:
            print("å·²å–æ¶ˆæ¸…ç†")
    elif has_issues and not args.clean:
        print("\nğŸ’¡ æç¤º: ä½¿ç”¨ --clean å‚æ•°æ‰§è¡Œæ¸…ç†")
        print("   python æ£€æŸ¥æ¸…ç†æ®‹ç•™æ•°æ®.py --clean")


if __name__ == "__main__":
    main()
