# V8.9.2 千万级数据保护

## 🎯 更新背景

**问题**: 同事反馈系统卡在"执行查询"步骤  
**原因**: 数据库有 1200万条 订单记录，`query.all()` 尝试一次性加载所有数据  
**影响**: 系统卡死，无法使用

---

## ✅ 本次更新

### 新增功能

#### 1. 数据量自动检查 ⭐⭐⭐
```python
# 查询前自动检查数据量
total_count = query.count()
print(f"[Database] 匹配记录: {total_count:,} 条")
```

#### 2. 数据量保护机制 ⭐⭐⭐
```python
MAX_ROWS = 500000  # 最大 50 万条
WARN_ROWS = 100000  # 警告阈值 10 万条

if total_count > MAX_ROWS:
    raise ValueError("数据量过大，请缩小查询范围")

if total_count > WARN_ROWS:
    print("⚠️ 数据量较大，查询可能需要 10-60 秒")
```

#### 3. 友好的错误提示 ⭐⭐
```
❌ 数据量过大 (1,200,000 条)，超过系统限制 (500,000 条)
💡 建议:
   1. 缩小时间范围（如最近 30-90 天）
   2. 选择单个门店查询
   3. 使用数据导出功能
   4. 联系管理员优化数据库
```

---

## 📝 修改的文件

### database/data_source_manager.py
**位置**: 第 223-245 行  
**修改内容**: 在 `query.all()` 前添加数据量检查

**修改前**:
```python
# 执行查询
print(f"[Database] 执行查询...")
results = query.all()
print(f"[Database] 查询到 {len(results)} 条记录")
```

**修改后**:
```python
# 🚨 V8.9.2: 添加数据量保护
# 先检查数据量
print(f"[Database] 检查数据量...")
total_count = query.count()
print(f"[Database] 匹配记录: {total_count:,} 条")

# 数据量保护
MAX_ROWS = 500000  # 最大 50 万条
WARN_ROWS = 100000  # 警告阈值 10 万条

if total_count > MAX_ROWS:
    error_msg = (
        f"❌ 数据量过大 ({total_count:,} 条)，超过系统限制 ({MAX_ROWS:,} 条)\n"
        f"💡 建议:\n"
        f"   1. 缩小时间范围（如最近 30-90 天）\n"
        f"   2. 选择单个门店查询\n"
        f"   3. 使用数据导出功能\n"
        f"   4. 联系管理员优化数据库"
    )
    print(error_msg)
    raise ValueError(error_msg)

if total_count > WARN_ROWS:
    print(f"⚠️ 数据量较大 ({total_count:,} 条)，查询可能需要 10-60 秒，请耐心等待...")

# 执行查询
print(f"[Database] 执行查询...")
results = query.all()
print(f"[Database] ✅ 查询完成，获取到 {len(results):,} 条记录")
```

---

## 🎯 保护规则

| 数据量 | 行为 | 提示 |
|--------|------|------|
| < 10 万 | ✅ 正常查询 | 无 |
| 10-50 万 | ⚠️ 显示警告 | "数据量较大，查询可能需要 10-60 秒" |
| > 50 万 | ❌ 拒绝查询 | "数据量过大，请缩小查询范围" |

---

## 📊 性能影响

### 新增的性能开销
- **数据量检查**: `query.count()` 约 0.1-1 秒
- **总体影响**: 可忽略（相比查询时间）

### 收益
- ✅ 防止系统卡死
- ✅ 提前发现问题
- ✅ 提供明确指引
- ✅ 改善用户体验

---

## 🧪 测试验证

### 测试脚本
```bash
python 测试数据量保护.py
```

### 测试场景

#### 场景1: 小数据量（< 10万）✅
```python
# 查询最近 7 天
start_date = datetime.now() - timedelta(days=7)
end_date = datetime.now()
```
**预期**: 正常查询，无警告

#### 场景2: 中等数据量（10-50万）⚠️
```python
# 查询最近 90 天
start_date = datetime.now() - timedelta(days=90)
end_date = datetime.now()
```
**预期**: 显示警告，但允许查询

#### 场景3: 大数据量（> 50万）❌
```python
# 查询全部数据
# 不设置日期范围
```
**预期**: 拒绝查询，显示错误提示

---

## 💡 使用建议

### 推荐的查询范围

#### 单个门店
- ✅ 最近 7 天: 1,000-5,000 条
- ✅ 最近 30 天: 5,000-20,000 条
- ✅ 最近 90 天: 15,000-60,000 条
- ⚠️ 最近 1 年: 60,000-240,000 条

#### 全部门店
- ✅ 最近 7 天: 10,000-50,000 条
- ⚠️ 最近 30 天: 50,000-200,000 条
- ❌ 最近 90 天: 150,000-600,000 条（可能被拦截）
- ❌ 最近 1 年: 600,000-2,400,000 条（必定被拦截）

### 如何查询大范围数据

#### 方法1: 分批查询
```
1. 查询 Q1 数据（1-3月）
2. 查询 Q2 数据（4-6月）
3. 查询 Q3 数据（7-9月）
4. 查询 Q4 数据（10-12月）
5. 在 Excel 中合并
```

#### 方法2: 逐个门店
```
1. 查询门店A全年数据
2. 查询门店B全年数据
3. 查询门店C全年数据
...
在 Excel 中合并
```

#### 方法3: 使用数据导出（未来功能）
```
1. 提交导出任务
2. 后台异步处理
3. 完成后下载文件
```

---

## 🔄 后续优化计划

### V8.10（下周）
- [ ] 实施流式查询（`yield_per`）
- [ ] 实施后端分页
- [ ] 添加查询进度条

### V9.0（下月）
- [ ] 数据分区（按月份）
- [ ] 物化视图（预聚合）
- [ ] 数据导出功能
- [ ] 查询优化器

---

## 📚 相关文档

- **千万级数据紧急处理方案.md** - 详细技术分析
- **给同事的紧急操作指南.md** - 用户操作指南
- **数据库查询卡顿问题诊断报告.md** - 问题诊断
- **测试数据量保护.py** - 自动化测试

---

## ✅ 更新清单

- [x] 添加数据量检查
- [x] 添加数据量保护
- [x] 添加友好错误提示
- [x] 创建测试脚本
- [x] 创建用户指南
- [x] 创建技术文档
- [ ] 用户验证（待同事测试）

---

**版本**: V8.9.2  
**发布日期**: 2025-12-11  
**优先级**: 🔴 紧急  
**类型**: Bug 修复 + 功能增强  
**影响**: 防止千万级数据卡死系统  
