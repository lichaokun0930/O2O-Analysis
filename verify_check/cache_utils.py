#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¼“å­˜å·¥å…·æ¨¡å—
ä¼˜åŒ–DataFrameå“ˆå¸Œè®¡ç®—å’Œç¼“å­˜ç®¡ç†
"""

import pandas as pd
import hashlib
import pickle
import gzip
from pathlib import Path
from datetime import datetime
import json


def calculate_data_hash_fast(df: pd.DataFrame) -> str:
    """
    å¿«é€Ÿè®¡ç®—DataFrameå“ˆå¸Œå€¼ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    
    ç›¸æ¯” df.to_json() + MD5ï¼š
    - é€Ÿåº¦æå‡ 10-100å€
    - å†…å­˜å ç”¨å‡å°‘ 50%+
    
    å‚æ•°:
        df: pandas DataFrame
    
    è¿”å›:
        str: MD5å“ˆå¸Œå€¼ï¼ˆ32å­—ç¬¦ï¼‰
    
    ç¤ºä¾‹:
        >>> df = pd.DataFrame({'A': [1, 2, 3]})
        >>> hash1 = calculate_data_hash_fast(df)
        >>> hash2 = calculate_data_hash_fast(df)
        >>> hash1 == hash2
        True
    """
    # æ–¹æ¡ˆï¼šä½¿ç”¨pandaså†…ç½®å“ˆå¸Œï¼ˆæœ€å¿«ï¼‰
    try:
        # è®¡ç®—æ¯è¡Œçš„å“ˆå¸Œå¹¶æ±‚å’Œ
        hash_sum = pd.util.hash_pandas_object(df, index=False).sum()
        
        # ç»“åˆæ•°æ®å½¢çŠ¶ç¡®ä¿å”¯ä¸€æ€§
        shape_str = f"{df.shape[0]}x{df.shape[1]}"
        
        # ç”Ÿæˆæœ€ç»ˆå“ˆå¸Œ
        combined = f"{hash_sum}_{shape_str}"
        return hashlib.md5(combined.encode()).hexdigest()
    
    except Exception as e:
        # é™çº§æ–¹æ¡ˆï¼šåŸºäºå…³é”®ç»Ÿè®¡ä¿¡æ¯
        print(f"âš ï¸ pandaså“ˆå¸Œå¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ: {e}")
        
        stats = {
            'shape': df.shape,
            'columns': list(df.columns),
            'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},
            'row_sum': df.select_dtypes(include=['number']).sum().sum() if len(df.select_dtypes(include=['number']).columns) > 0 else 0,
            'null_count': df.isnull().sum().sum()
        }
        
        stats_str = json.dumps(stats, sort_keys=True)
        return hashlib.md5(stats_str.encode()).hexdigest()


def calculate_data_hash_legacy(df: pd.DataFrame) -> str:
    """
    ä¼ ç»Ÿçš„DataFrameå“ˆå¸Œè®¡ç®—ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
    
    âš ï¸ æ€§èƒ½è¾ƒå·®ï¼Œå»ºè®®è¿ç§»åˆ° calculate_data_hash_fast()
    
    å‚æ•°:
        df: pandas DataFrame
    
    è¿”å›:
        str: MD5å“ˆå¸Œå€¼
    """
    import warnings
    warnings.warn(
        "calculate_data_hash_legacy() æ€§èƒ½è¾ƒå·®ï¼Œå»ºè®®ä½¿ç”¨ calculate_data_hash_fast()",
        DeprecationWarning,
        stacklevel=2
    )
    
    # åŸå§‹æ–¹æ¡ˆï¼što_json() + MD5
    json_str = df.to_json(orient='records', force_ascii=False)
    return hashlib.md5(json_str.encode('utf-8')).hexdigest()


def save_dataframe_compressed(df: pd.DataFrame, file_path: Path) -> int:
    """
    ä¿å­˜DataFrameåˆ°å‹ç¼©æ–‡ä»¶
    
    å‚æ•°:
        df: pandas DataFrame
        file_path: ä¿å­˜è·¯å¾„ï¼ˆå»ºè®®ä½¿ç”¨ .pkl.gz åç¼€ï¼‰
    
    è¿”å›:
        int: æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    
    ç¤ºä¾‹:
        >>> df = pd.DataFrame({'A': range(1000)})
        >>> size = save_dataframe_compressed(df, Path('data.pkl.gz'))
        >>> size > 0
        True
    """
    file_path = Path(file_path)
    file_path.parent.mkdir(parents=True, exist_ok=True)
    
    # ä½¿ç”¨pickleåºåˆ—åŒ–å¹¶gzipå‹ç¼©
    with gzip.open(file_path, 'wb', compresslevel=6) as f:
        pickle.dump(df, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    return file_path.stat().st_size


def load_dataframe_compressed(file_path: Path) -> pd.DataFrame:
    """
    ä»å‹ç¼©æ–‡ä»¶åŠ è½½DataFrame
    
    å‚æ•°:
        file_path: æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        pandas DataFrame
    
    ç¤ºä¾‹:
        >>> df_original = pd.DataFrame({'A': [1, 2, 3]})
        >>> save_dataframe_compressed(df_original, Path('test.pkl.gz'))
        >>> df_loaded = load_dataframe_compressed(Path('test.pkl.gz'))
        >>> df_original.equals(df_loaded)
        True
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    with gzip.open(file_path, 'rb') as f:
        return pickle.load(f)


def get_cache_metadata(file_path: Path) -> dict:
    """
    è·å–ç¼“å­˜æ–‡ä»¶å…ƒæ•°æ®
    
    å‚æ•°:
        file_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        dict: å…ƒæ•°æ®ï¼ˆåˆ›å»ºæ—¶é—´ã€å¤§å°ã€å“ˆå¸Œç­‰ï¼‰
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        return None
    
    stat = file_path.stat()
    
    return {
        'path': str(file_path),
        'size_bytes': stat.st_size,
        'size_mb': round(stat.st_size / 1024 / 1024, 2),
        'created_time': datetime.fromtimestamp(stat.st_ctime).isoformat(),
        'modified_time': datetime.fromtimestamp(stat.st_mtime).isoformat(),
        'age_hours': round((datetime.now().timestamp() - stat.st_mtime) / 3600, 1)
    }


def cleanup_old_caches(cache_dir: Path, max_age_hours: int = 72, keep_latest: int = 5):
    """
    æ¸…ç†è¿‡æœŸç¼“å­˜æ–‡ä»¶
    
    å‚æ•°:
        cache_dir: ç¼“å­˜ç›®å½•
        max_age_hours: æœ€å¤§ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
        keep_latest: è‡³å°‘ä¿ç•™æœ€æ–°çš„Nä¸ªæ–‡ä»¶
    
    è¿”å›:
        int: åˆ é™¤çš„æ–‡ä»¶æ•°é‡
    
    ç¤ºä¾‹:
        >>> count = cleanup_old_caches(Path('cache'), max_age_hours=24, keep_latest=3)
        >>> count >= 0
        True
    """
    cache_dir = Path(cache_dir)
    
    if not cache_dir.exists():
        return 0
    
    # è·å–æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
    cache_files = sorted(
        [f for f in cache_dir.glob('*.pkl.gz')],
        key=lambda f: f.stat().st_mtime,
        reverse=True  # æœ€æ–°çš„åœ¨å‰
    )
    
    if len(cache_files) <= keep_latest:
        return 0
    
    deleted_count = 0
    current_time = datetime.now().timestamp()
    max_age_seconds = max_age_hours * 3600
    
    # ä¿ç•™æœ€æ–°çš„keep_latestä¸ªæ–‡ä»¶
    for file_path in cache_files[keep_latest:]:
        file_age = current_time - file_path.stat().st_mtime
        
        if file_age > max_age_seconds:
            try:
                file_path.unlink()
                deleted_count += 1
                print(f"ğŸ—‘ï¸ åˆ é™¤è¿‡æœŸç¼“å­˜: {file_path.name} (å¹´é¾„: {file_age / 3600:.1f}å°æ—¶)")
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤å¤±è´¥ {file_path.name}: {e}")
    
    return deleted_count


# æ€§èƒ½åŸºå‡†æµ‹è¯•
def benchmark_hash_methods(df: pd.DataFrame):
    """
    å¯¹æ¯”ä¸åŒå“ˆå¸Œè®¡ç®—æ–¹æ³•çš„æ€§èƒ½
    
    å‚æ•°:
        df: æµ‹è¯•ç”¨DataFrame
    """
    import time
    
    print(f"\n{'='*60}")
    print(f"DataFrameå“ˆå¸Œæ€§èƒ½æµ‹è¯•")
    print(f"æ•°æ®è§„æ¨¡: {df.shape[0]:,} è¡Œ Ã— {df.shape[1]} åˆ—")
    print(f"{'='*60}\n")
    
    # æ–¹æ³•1ï¼šå¿«é€Ÿå“ˆå¸Œ
    start = time.time()
    hash1 = calculate_data_hash_fast(df)
    time1 = time.time() - start
    print(f"âœ… å¿«é€Ÿå“ˆå¸Œ: {time1:.4f}ç§’ â†’ {hash1}")
    
    # æ–¹æ³•2ï¼šä¼ ç»Ÿå“ˆå¸Œï¼ˆå¯èƒ½å¾ˆæ…¢ï¼Œé™åˆ¶å¤§å°ï¼‰
    if len(df) <= 10000:
        start = time.time()
        hash2 = calculate_data_hash_legacy(df)
        time2 = time.time() - start
        print(f"âš ï¸ ä¼ ç»Ÿå“ˆå¸Œ: {time2:.4f}ç§’ â†’ {hash2}")
        print(f"\nğŸ“Š æ€§èƒ½æå‡: {time2 / time1:.1f}å€")
    else:
        print(f"âš ï¸ ä¼ ç»Ÿå“ˆå¸Œ: è·³è¿‡ï¼ˆæ•°æ®é‡è¿‡å¤§ï¼‰")
    
    print(f"{'='*60}\n")


# å•å…ƒæµ‹è¯•
if __name__ == "__main__":
    import tempfile
    
    print("=" * 60)
    print("ç¼“å­˜å·¥å…·æ¨¡å—æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_df = pd.DataFrame({
        'A': range(1000),
        'B': ['item_' + str(i) for i in range(1000)],
        'C': [i * 0.5 for i in range(1000)]
    })
    
    print(f"\næµ‹è¯•æ•°æ®: {test_df.shape}")
    
    # æµ‹è¯•1ï¼šå“ˆå¸Œè®¡ç®—
    print("\n1ï¸âƒ£ æµ‹è¯•å“ˆå¸Œè®¡ç®—...")
    hash1 = calculate_data_hash_fast(test_df)
    hash2 = calculate_data_hash_fast(test_df)
    print(f"   å“ˆå¸Œå€¼1: {hash1}")
    print(f"   å“ˆå¸Œå€¼2: {hash2}")
    print(f"   ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if hash1 == hash2 else 'âŒ å¤±è´¥'}")
    
    # æµ‹è¯•2ï¼šå‹ç¼©ä¿å­˜/åŠ è½½
    print("\n2ï¸âƒ£ æµ‹è¯•å‹ç¼©ä¿å­˜/åŠ è½½...")
    with tempfile.TemporaryDirectory() as tmpdir:
        cache_path = Path(tmpdir) / 'test_cache.pkl.gz'
        
        size = save_dataframe_compressed(test_df, cache_path)
        print(f"   ä¿å­˜å¤§å°: {size / 1024:.2f} KB")
        
        df_loaded = load_dataframe_compressed(cache_path)
        equals = test_df.equals(df_loaded)
        print(f"   æ•°æ®ä¸€è‡´: {'âœ… é€šè¿‡' if equals else 'âŒ å¤±è´¥'}")
        
        # æµ‹è¯•3ï¼šå…ƒæ•°æ®
        print("\n3ï¸âƒ£ æµ‹è¯•å…ƒæ•°æ®è·å–...")
        metadata = get_cache_metadata(cache_path)
        print(f"   æ–‡ä»¶å¤§å°: {metadata['size_mb']} MB")
        print(f"   åˆ›å»ºæ—¶é—´: {metadata['created_time']}")
    
    # æµ‹è¯•4ï¼šæ€§èƒ½åŸºå‡†
    print("\n4ï¸âƒ£ æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    benchmark_hash_methods(test_df)
    
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼\n")
