"""
æ•°æ®åº“è¿ç§»è„šæœ¬
ç”¨äºåŒæ­¥ models.py ä¸­çš„è¡¨ç»“æ„å˜æ›´åˆ°æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•ï¼š
    python æ•°æ®åº“è¿ç§».py          # æ‰§è¡Œå®Œæ•´è¿ç§»ï¼ˆæ•°æ®åº“ + å­—æ®µæ˜ å°„ï¼‰
    python æ•°æ®åº“è¿ç§».py --db     # ä»…è¿ç§»æ•°æ®åº“
    python æ•°æ®åº“è¿ç§».py --show   # æ˜¾ç¤ºè¡¨ç»“æ„

åŠŸèƒ½ï¼š
    1. æ£€æµ‹æ–°å¢çš„å­—æ®µå¹¶è‡ªåŠ¨æ·»åŠ åˆ°æ•°æ®åº“
    2. è‡ªåŠ¨æ›´æ–° data_source_manager.py ä¸­çš„å­—æ®µæ˜ å°„
    3. è‡ªåŠ¨æ›´æ–° æ™ºèƒ½å¯¼å…¥é—¨åº—æ•°æ®.py ä¸­çš„å­—æ®µæ˜ å°„
    4. æ˜¾ç¤ºè¿ç§»è¯¦æƒ…
    5. å®‰å…¨æ‰§è¡Œï¼Œä¸ä¼šåˆ é™¤ç°æœ‰æ•°æ®

âœ… æ–°å¢å­—æ®µåªéœ€ä¿®æ”¹ models.pyï¼Œç„¶åè¿è¡Œæ­¤è„šæœ¬å³å¯ï¼

ä½œè€…: GitHub Copilot
æ—¥æœŸ: 2025-12-04
"""

import sys
import os
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from sqlalchemy import inspect, text
from sqlalchemy.engine import Engine
from database.models import Base, Order, Product
from database.connection import engine, SessionLocal
from datetime import datetime


# ========================================
# ğŸ“Œ å­—æ®µå‘½åè§„åˆ™ï¼šæ•°æ®åº“å­—æ®µå -> ä¸­æ–‡æ˜¾ç¤ºå
# ========================================
# å¦‚æœ models.py ä¸­çš„ comment åŒ…å«ä¸­æ–‡ï¼Œä¼˜å…ˆä½¿ç”¨ comment
# å¦åˆ™ä½¿ç”¨ä¸‹é¢çš„é»˜è®¤æ˜ å°„
# ========================================
FIELD_NAME_MAPPING = {
    # åŸºç¡€è®¢å•ä¿¡æ¯
    'order_id': 'è®¢å•ID',
    'order_number': 'è®¢å•ç¼–å·',
    'date': 'æ—¥æœŸ',
    'store_name': 'é—¨åº—åç§°',
    'store_id': 'é—¨åº—ID',
    'city': 'åŸå¸‚åç§°',
    
    # å•†å“ä¿¡æ¯
    'product_name': 'å•†å“åç§°',
    'barcode': 'æ¡ç ',
    'store_code': 'åº—å†…ç ',
    'category_level1': 'ä¸€çº§åˆ†ç±»å',
    'category_level3': 'ä¸‰çº§åˆ†ç±»å',
    
    # ä»·æ ¼æˆæœ¬
    'price': 'å•†å“å®å”®ä»·',
    'original_price': 'å•†å“åŸä»·',
    'cost': 'æˆæœ¬',
    'actual_price': 'å®æ”¶ä»·æ ¼',
    
    # é”€é‡é‡‘é¢
    'quantity': 'é”€é‡',
    'remaining_stock': 'å‰©ä½™åº“å­˜',
    'amount': 'é¢„è®¡è®¢å•æ”¶å…¥',
    'profit': 'åˆ©æ¶¦é¢',
    
    # è´¹ç”¨
    'delivery_fee': 'ç‰©æµé…é€è´¹',
    'commission': 'å¹³å°ä½£é‡‘',
    'platform_service_fee': 'å¹³å°æœåŠ¡è´¹',
    
    # è¥é”€æ´»åŠ¨
    'user_paid_delivery_fee': 'ç”¨æˆ·æ”¯ä»˜é…é€è´¹',
    'delivery_discount': 'é…é€è´¹å‡å…é‡‘é¢',
    'full_reduction': 'æ»¡å‡é‡‘é¢',
    'product_discount': 'å•†å“å‡å…é‡‘é¢',
    'merchant_voucher': 'å•†å®¶ä»£é‡‘åˆ¸',
    'merchant_share': 'å•†å®¶æ‰¿æ‹…éƒ¨åˆ†åˆ¸',
    'packaging_fee': 'æ‰“åŒ…è¢‹é‡‘é¢',
    'gift_amount': 'æ»¡èµ é‡‘é¢',
    'other_merchant_discount': 'å•†å®¶å…¶ä»–ä¼˜æƒ ',
    'new_customer_discount': 'æ–°å®¢å‡å…é‡‘é¢',
    
    # åˆ©æ¶¦è¡¥å¿
    'corporate_rebate': 'ä¼å®¢åè¿”',
    
    # é…é€ä¿¡æ¯
    'delivery_platform': 'é…é€å¹³å°',
    'delivery_distance': 'é…é€è·ç¦»',
    
    # æ¸ é“åœºæ™¯
    'channel': 'æ¸ é“',
    'scene': 'åœºæ™¯',
    'time_period': 'æ—¶æ®µ',
}


def get_chinese_name(column) -> str:
    """ä»åˆ—å®šä¹‰è·å–ä¸­æ–‡åç§°"""
    # ä¼˜å…ˆä½¿ç”¨ comment ä¸­çš„ä¸­æ–‡éƒ¨åˆ†
    if column.comment:
        # æå–æ‹¬å·å‰çš„ä¸­æ–‡éƒ¨åˆ†ï¼Œå¦‚ "è®¢å•ç¼–å·(æ¸ é“å¹³å°è®¢å•å·)" -> "è®¢å•ç¼–å·"
        match = re.match(r'^([^(ï¼ˆ]+)', column.comment)
        if match:
            return match.group(1).strip()
        return column.comment
    
    # å¦åˆ™ä½¿ç”¨æ˜ å°„è¡¨
    return FIELD_NAME_MAPPING.get(column.name, column.name)


def get_default_value_str(column) -> str:
    """è·å–é»˜è®¤å€¼çš„å­—ç¬¦ä¸²è¡¨ç¤º"""
    type_str = str(column.type).upper()
    
    if 'VARCHAR' in type_str or 'STRING' in type_str or 'TEXT' in type_str:
        return "''"
    elif 'INTEGER' in type_str:
        return "0"
    elif 'FLOAT' in type_str or 'REAL' in type_str:
        return "0.0"
    elif 'BOOLEAN' in type_str:
        return "False"
    else:
        return "None"


def get_model_columns(model_class) -> dict:
    """è·å–æ¨¡å‹å®šä¹‰çš„æ‰€æœ‰åˆ—"""
    columns = {}
    for column in model_class.__table__.columns:
        columns[column.name] = {
            'type': str(column.type),
            'nullable': column.nullable,
            'default': column.default,
            'comment': column.comment
        }
    return columns


def get_db_columns(engine: Engine, table_name: str) -> set:
    """è·å–æ•°æ®åº“è¡¨çš„ç°æœ‰åˆ—"""
    inspector = inspect(engine)
    if table_name not in inspector.get_table_names():
        return set()
    return {col['name'] for col in inspector.get_columns(table_name)}


def get_column_sql_type(column) -> str:
    """æ ¹æ®SQLAlchemyç±»å‹è¿”å›PostgreSQL/SQLiteç±»å‹"""
    type_str = str(column.type).upper()
    
    # å¸¸è§ç±»å‹æ˜ å°„
    if 'VARCHAR' in type_str or 'STRING' in type_str:
        length = getattr(column.type, 'length', 100) or 100
        return f"VARCHAR({length})"
    elif 'INTEGER' in type_str:
        return "INTEGER"
    elif 'FLOAT' in type_str or 'REAL' in type_str:
        return "REAL"
    elif 'BOOLEAN' in type_str:
        return "BOOLEAN"
    elif 'DATETIME' in type_str or 'TIMESTAMP' in type_str:
        return "TIMESTAMP"
    elif 'TEXT' in type_str:
        return "TEXT"
    else:
        return type_str


def migrate_table(engine: Engine, model_class):
    """è¿ç§»å•ä¸ªè¡¨"""
    table_name = model_class.__tablename__
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ æ£€æŸ¥è¡¨: {table_name}")
    print(f"{'='*60}")
    
    # è·å–æ¨¡å‹åˆ—å’Œæ•°æ®åº“åˆ—
    model_columns = get_model_columns(model_class)
    db_columns = get_db_columns(engine, table_name)
    
    if not db_columns:
        print(f"  âš ï¸  è¡¨ {table_name} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ•´ä¸ªè¡¨")
        Base.metadata.create_all(engine, tables=[model_class.__table__])
        print(f"  âœ… è¡¨ {table_name} åˆ›å»ºæˆåŠŸ")
        return
    
    # æ‰¾å‡ºæ–°å¢çš„åˆ—
    new_columns = set(model_columns.keys()) - db_columns
    
    if not new_columns:
        print(f"  âœ… è¡¨ç»“æ„å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€è¿ç§»")
        return
    
    print(f"  ğŸ” å‘ç° {len(new_columns)} ä¸ªæ–°å­—æ®µéœ€è¦æ·»åŠ :")
    
    # æ·»åŠ æ–°åˆ—
    with engine.connect() as conn:
        for col_name in new_columns:
            column = model_class.__table__.columns[col_name]
            col_type = get_column_sql_type(column)
            
            # æ„å»º ALTER TABLE è¯­å¥
            default_value = ""
            if column.default is not None:
                if hasattr(column.default, 'arg'):
                    default_val = column.default.arg
                    if isinstance(default_val, str):
                        default_value = f" DEFAULT '{default_val}'"
                    elif default_val is not None:
                        default_value = f" DEFAULT {default_val}"
            elif column.nullable:
                default_value = " DEFAULT NULL"
            
            # SQLite å’Œ PostgreSQL éƒ½æ”¯æŒ ALTER TABLE ADD COLUMN
            sql = f'ALTER TABLE {table_name} ADD COLUMN "{col_name}" {col_type}{default_value}'
            
            try:
                conn.execute(text(sql))
                conn.commit()
                comment = f" -- {column.comment}" if column.comment else ""
                print(f"     âœ… æ·»åŠ å­—æ®µ: {col_name} ({col_type}){comment}")
            except Exception as e:
                if "duplicate column" in str(e).lower() or "already exists" in str(e).lower():
                    print(f"     â­ï¸  å­—æ®µå·²å­˜åœ¨: {col_name}")
                else:
                    print(f"     âŒ æ·»åŠ å­—æ®µå¤±è´¥: {col_name} - {e}")


def create_index_if_not_exists(engine: Engine, model_class):
    """ä¸ºæ–°å­—æ®µåˆ›å»ºç´¢å¼•ï¼ˆå¦‚æœæ¨¡å‹ä¸­å®šä¹‰äº†ç´¢å¼•ï¼‰"""
    table_name = model_class.__tablename__
    inspector = inspect(engine)
    
    # è·å–ç°æœ‰ç´¢å¼•
    existing_indexes = {idx['name'] for idx in inspector.get_indexes(table_name)}
    
    # æ£€æŸ¥æ¨¡å‹ä¸­çš„ç´¢å¼•åˆ—
    for column in model_class.__table__.columns:
        if column.index:
            index_name = f"ix_{table_name}_{column.name}"
            if index_name not in existing_indexes:
                try:
                    with engine.connect() as conn:
                        sql = f'CREATE INDEX IF NOT EXISTS "{index_name}" ON {table_name} ("{column.name}")'
                        conn.execute(text(sql))
                        conn.commit()
                        print(f"     ğŸ“‡ åˆ›å»ºç´¢å¼•: {index_name}")
                except Exception as e:
                    if "already exists" not in str(e).lower():
                        print(f"     âš ï¸  ç´¢å¼•åˆ›å»ºå¤±è´¥: {index_name} - {e}")


def run_migration():
    """æ‰§è¡Œæ•°æ®åº“è¿ç§»"""
    print("\n" + "="*60)
    print("ğŸš€ æ•°æ®åº“è¿ç§»å·¥å…·")
    print("="*60)
    print(f"â° æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ æ•°æ®åº“å¼•æ“: {engine.url}")
    
    # éœ€è¦è¿ç§»çš„æ¨¡å‹åˆ—è¡¨
    models_to_migrate = [Order, Product]
    
    for model in models_to_migrate:
        migrate_table(engine, model)
        create_index_if_not_exists(engine, model)
    
    print("\n" + "="*60)
    print("âœ… æ•°æ®åº“è¿ç§»å®Œæˆ!")
    print("="*60)
    
    # æ˜¾ç¤ºå½“å‰è¡¨ç»“æ„æ‘˜è¦
    print("\nğŸ“Š å½“å‰è¡¨ç»“æ„æ‘˜è¦:")
    inspector = inspect(engine)
    for table_name in inspector.get_table_names():
        columns = inspector.get_columns(table_name)
        print(f"  ğŸ“‹ {table_name}: {len(columns)} ä¸ªå­—æ®µ")


def sync_field_mappings():
    """
    åŒæ­¥å­—æ®µæ˜ å°„åˆ° data_source_manager.py
    
    æ ¹æ® models.py ä¸­ Order è¡¨çš„å­—æ®µå®šä¹‰ï¼Œè‡ªåŠ¨ç”Ÿæˆæ˜ å°„é…ç½®
    """
    print("\n" + "="*60)
    print("ğŸ”„ åŒæ­¥å­—æ®µæ˜ å°„")
    print("="*60)
    
    # æ”¶é›†æ‰€æœ‰ Order è¡¨çš„å­—æ®µ
    mappings = []
    skip_fields = {'id', 'product_id'}  # è·³è¿‡çš„å­—æ®µ
    
    for column in Order.__table__.columns:
        if column.name in skip_fields:
            continue
        
        chinese_name = get_chinese_name(column)
        db_field = column.name
        default_value = get_default_value_str(column)
        
        # æ–°å­—æ®µéœ€è¦ hasattr æ£€æŸ¥ï¼ˆå®‰å…¨èµ·è§ï¼Œå…¨éƒ¨è®¾ä¸º Trueï¼‰
        need_hasattr = True
        
        mappings.append((chinese_name, db_field, default_value, need_hasattr))
    
    # ç”Ÿæˆæ˜ å°„ä»£ç 
    print(f"  ğŸ“ æ£€æµ‹åˆ° {len(mappings)} ä¸ªå­—æ®µ")
    
    # è¯»å– data_source_manager.py
    manager_file = os.path.join(os.path.dirname(__file__), 'database', 'data_source_manager.py')
    
    try:
        with open(manager_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯æ–°æ ¼å¼ï¼ˆåŒ…å« DB_FIELD_MAPPINGï¼‰
        if 'DB_FIELD_MAPPING' in content:
            print("  âœ… data_source_manager.py å·²ä½¿ç”¨ç»Ÿä¸€æ˜ å°„é…ç½®")
            print("  ğŸ’¡ å¦‚éœ€æ›´æ–°æ˜ å°„ï¼Œè¯·æ‰‹åŠ¨ç¼–è¾‘ DB_FIELD_MAPPING å­—å…¸")
        else:
            print("  âš ï¸  data_source_manager.py ä½¿ç”¨æ—§æ ¼å¼")
            print("  ğŸ’¡ å»ºè®®æ‰‹åŠ¨æ›´æ–°ä¸ºç»Ÿä¸€æ˜ å°„é…ç½®æ ¼å¼")
        
        # æ˜¾ç¤ºå½“å‰æ¨¡å‹ä¸­çš„æ–°å­—æ®µï¼ˆä¾›å‚è€ƒï¼‰
        print("\n  ğŸ“‹ models.py ä¸­ Order è¡¨çš„å­—æ®µåˆ—è¡¨ï¼š")
        for chinese_name, db_field, default_value, _ in mappings[:10]:
            print(f"     '{chinese_name}': ('{db_field}', {default_value}, True),")
        if len(mappings) > 10:
            print(f"     ... å…± {len(mappings)} ä¸ªå­—æ®µ")
            
    except Exception as e:
        print(f"  âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")


def run_full_migration():
    """æ‰§è¡Œå®Œæ•´è¿ç§»ï¼ˆæ•°æ®åº“ + å­—æ®µæ˜ å°„åŒæ­¥ï¼‰"""
    run_migration()
    sync_field_mappings()
    
    # æ¸…é™¤ Redis ç¼“å­˜
    print("\n" + "="*60)
    print("ğŸ§¹ æ¸…é™¤ç¼“å­˜")
    print("="*60)
    try:
        import redis
        r = redis.Redis(host='localhost', port=6379, db=0)
        r.flushdb()
        print("  âœ… Redis ç¼“å­˜å·²æ¸…é™¤")
    except Exception as e:
        print(f"  âš ï¸  Redis ç¼“å­˜æ¸…é™¤å¤±è´¥ï¼ˆå¯èƒ½æœªå¯ç”¨ï¼‰: {e}")
    
    print("\n" + "="*60)
    print("ğŸ‰ å®Œæ•´è¿ç§»å®Œæˆï¼")
    print("="*60)
    print("\nğŸ’¡ æç¤ºï¼šè¯·é‡å¯çœ‹æ¿æœåŠ¡ä»¥åº”ç”¨æ›´æ”¹")


def show_table_structure(table_name: str = None):
    """æ˜¾ç¤ºè¡¨ç»“æ„è¯¦æƒ…"""
    inspector = inspect(engine)
    
    tables = [table_name] if table_name else inspector.get_table_names()
    
    for table in tables:
        if table not in inspector.get_table_names():
            print(f"âŒ è¡¨ {table} ä¸å­˜åœ¨")
            continue
            
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ è¡¨: {table}")
        print(f"{'='*60}")
        
        columns = inspector.get_columns(table)
        for col in columns:
            nullable = "NULL" if col['nullable'] else "NOT NULL"
            default = f" DEFAULT {col['default']}" if col['default'] else ""
            print(f"  - {col['name']}: {col['type']} {nullable}{default}")
        
        # æ˜¾ç¤ºç´¢å¼•
        indexes = inspector.get_indexes(table)
        if indexes:
            print(f"\n  ğŸ“‡ ç´¢å¼•:")
            for idx in indexes:
                print(f"    - {idx['name']}: {idx['column_names']}")


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®åº“è¿ç§»å·¥å…·')
    parser.add_argument('--show', '-s', type=str, help='æ˜¾ç¤ºæŒ‡å®šè¡¨çš„ç»“æ„', metavar='TABLE')
    parser.add_argument('--show-all', '-a', action='store_true', help='æ˜¾ç¤ºæ‰€æœ‰è¡¨çš„ç»“æ„')
    parser.add_argument('--db', action='store_true', help='ä»…æ‰§è¡Œæ•°æ®åº“è¿ç§»ï¼ˆä¸åŒæ­¥æ˜ å°„ï¼‰')
    parser.add_argument('--sync', action='store_true', help='ä»…åŒæ­¥å­—æ®µæ˜ å°„ï¼ˆä¸è¿ç§»æ•°æ®åº“ï¼‰')
    
    args = parser.parse_args()
    
    if args.show:
        show_table_structure(args.show)
    elif args.show_all:
        show_table_structure()
    elif args.db:
        run_migration()
    elif args.sync:
        sync_field_mappings()
    else:
        # é»˜è®¤æ‰§è¡Œå®Œæ•´è¿ç§»
        run_full_migration()
